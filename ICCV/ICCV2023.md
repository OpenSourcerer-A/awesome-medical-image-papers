| 年份 | 题目 | 作者 | 摘要                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 中文摘要 | link |
| --- | --- | --- |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| --- | --- |
| 2023 | Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis | Yankai Jiang, Mingze Sun, Heng Guo, Xiaoyu Bai, Ke Yan, Le Lu, Minfeng Xu | Self-supervised learning (SSL) has recently achieved promising performance for 3D medical image analysis tasks. Most current methods follow existing SSL paradigm originally designed for photographic or natural images, which cannot explicitly and thoroughly exploit the intrinsic similar anatomical structures across varying medical images. This may in fact degrade the quality of learned deep representations by maximizing the similarity among features containing spatial misalignment information and different anatomical semantics. In this work, we propose a new self-supervised learning framework, namely Alice, that explicitly fulfills Anatomical invariance modeling and semantic alignment via elaborately combining discriminative and generative objectives. Alice introduces a new contrastive learning strategy which encourages the similarity between views that are diversely mined but with consistent high-level semantics, in order to learn invariant anatomical features. Moreover, we design a conditional anatomical feature alignment module to complement corrupted embeddings with globally matched semantics and inter-patch topology information, conditioned by the distribution of local image content, which permits to create better contrastive pairs. Our extensive quantitative experiments on three 3D medical image analysis tasks demonstrate and validate the performance superiority of Alice, surpassing the previous best SSL counterpart methods and showing promising ability for united representation learning. Codes are available at https://github.com/alibaba-damo-academy/alice.                                                                                                                                                                                                                                                                                                                                | 自监督学习（SSL）最近在3D医学图像分析任务中取得了令人期待的表现。大多数当前方法遵循最初设计用于摄影或自然图像的现有SSL范例，这不能明确和彻底地利用各种医学图像之间固有的相似解剖结构。事实上，这可能通过最大化包含空间错位信息和不同解剖语义的特征之间的相似性来降低学习到的深层表示的质量。在这项工作中，我们提出了一个新的自监督学习框架，即Alice，通过精心组合判别性和生成性目标，明确实现解剖不变建模和语义对齐。Alice引入了一种新的对比学习策略，鼓励不同挖掘但具有一致高级语义的视图之间的相似性，以学习不变的解剖特征。此外，我们设计了一个条件解剖特征对齐模块，通过局部图像内容的分布来补充受损的嵌入，具有全局匹配的语义和区域拓扑信息，从而创建更好的对比对。我们在三个3D医学图像分析任务上进行了大量定量实验，证明并验证了Alice的性能优越性，超过了先前最好的SSL对应方法，并展示了统一表示学习的有希望能力。代码可在https://github.com/alibaba-damo-academy/alice找到。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Anatomical_Invariance_Modeling_and_Semantic_Alignment_for_Self-supervised_Learning_in_ICCV_2023_paper.pdf) |
| 2023 | Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation | Arne Schmidt, Pablo Morales-Ãlvarez, Rafael Molina | Medical image segmentation is a challenging task, particularly due to inter- and intra-observer variability, even between medical experts. In this paper, we propose a novel model, called Probabilistic Inter-Observer and iNtra-Observer variation NetwOrk (Pionono). It captures the labeling behavior of each rater with a multidimensional probability distribution and integrates this information with the feature maps of the image to produce probabilistic segmentation predictions. The model is optimized by variational inference and can be trained end-to-end. It outperforms state-of-the-art models such as STAPLE, Probabilistic U-Net, and models based on confusion matrices. Additionally, Pionono predicts multiple coherent segmentation maps that mimic the rater's expert opinion, which provides additional valuable information for the diagnostic process. Experiments on real-world cancer segmentation datasets demonstrate the high accuracy and efficiency of Pionono, making it a powerful tool for medical image analysis.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 医学图像分割是一项具有挑战性的任务，特别是由于医学专家之间的观察者之间和观察者内的变异性。在本文中，我们提出了一种新颖的模型，称为Probabilistic Inter-Observer and iNtra-Observer variation NetwOrk（Pionono）。它通过多维概率分布捕获每个评分者的标记行为，并将此信息与图像的特征图集成在一起，以产生概率分割预测。该模型通过变分推断进行优化，并可以进行端到端的训练。它胜过了诸如STAPLE、Probabilistic U-Net以及基于混淆矩阵的模型等现有模型。此外，Pionono预测出多个连贯的分割图，模拟评分者的专家意见，为诊断过程提供了额外有价值的信息。对真实癌症分割数据集的实验表明，Pionono具有高准确性和高效率，使其成为医学图像分析的强大工具。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Schmidt_Probabilistic_Modeling_of_Inter-_and_Intra-observer_Variability_in_Medical_Image_ICCV_2023_paper.pdf) |
| 2023 | A Skeletonization Algorithm for Gradient-Based Optimization | Martin J. Menten, Johannes C. Paetzold, Veronika A. Zimmer, Suprosanna Shit, Ivan Ezhov, Robbie Holland, Monika Probst, Julia A. Schnabel, Daniel Rueckert | The skeleton of a digital image is a compact representation of its topology, geometry, and scale. It has utility in many computer vision applications, such as image description, segmentation, and registration. However, skeletonization has only seen limited use in contemporary deep learning solutions. Most existing skeletonization algorithms are not differentiable, making it impossible to integrate them with gradient-based optimization. Compatible algorithms based on morphological operations and neural networks have been proposed, but their results often deviate from the geometry and topology of the true medial axis. This work introduces the first three-dimensional skeletonization algorithm that is both compatible with gradient-based optimization and preserves an object's topology. Our method is exclusively based on matrix additions and multiplications, convolutional operations, basic non-linear functions, and sampling from a uniform probability distribution, allowing it to be easily implemented in any major deep learning library. In benchmarking experiments, we prove the advantages of our skeletonization algorithm compared to non-differentiable, morphological, and neural-network-based baselines. Finally, we demonstrate the utility of our algorithm by integrating it with two medical image processing applications that use gradient-based optimization: deep-learning-based blood vessel segmentation, and multimodal registration of the mandible in computed tomography and magnetic resonance images.                                                                                                                                                                                                                                                                                                                                                                                                         | 数字图像的骨架是其拓扑结构、几何形状和尺度的紧凑表示。它在许多计算机视觉应用中具有实用性，如图像描述、分割和配准。然而，在当代深度学习解决方案中，骨架化仅被有限使用。大多数现有的骨架化算法不可微分，这使得无法将它们与基于梯度的优化集成。基于形态学运算和神经网络的兼容算法已被提出，但它们的结果常常偏离真实中轴的几何形状和拓扑。本研究介绍了第一个与基于梯度的优化兼容并保留对象拓扑的三维骨架化算法。我们的方法仅基于矩阵加法和乘法、卷积操作、基本非线性函数和从均匀概率分布中采样，使其能够轻松实现在任何主要深度学习库中。在基准实验中，我们证明了我们的骨架化算法相对于不可微分、形态学和神经网络基准的优势。最后，我们通过将其与两个使用基于梯度的优化的医学图像处理应用集成来展示我们算法的实用性：基于深度学习的血管分割和计算机断层扫描和磁共振图像中下颌的多模式配准。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Menten_A_Skeletonization_Algorithm_for_Gradient-Based_Optimization_ICCV_2023_paper.pdf) |
| 2023 | Learning Cross-Representation Affinity Consistency for Sparsely Supervised Biomedical Instance Segmentation | Xiaoyu Liu, Wei Huang, Zhiwei Xiong, Shenglong Zhou, Yueyi Zhang, Xuejin Chen, Zheng-Jun Zha, Feng Wu | Sparse instance-level supervision has recently been explored to address insufficient annotation in biomedical instance segmentation, which is easier to annotate crowded instances and better preserves instance completeness for 3D volumetric datasets compared to common semi-supervision.In this paper, we propose a sparsely supervised biomedical instance segmentation framework via cross-representation affinity consistency regularization. Specifically, we adopt two individual networks to enforce the perturbation consistency between an explicit affinity map and an implicit affinity map to capture both feature-level instance discrimination and pixel-level instance boundary structure. We then select the highly confident region of each affinity map as the pseudo label to supervise the other one for affinity consistency learning. To obtain the highly confident region, we propose a pseudo-label noise filtering scheme by integrating two entropy-based decision strategies. Extensive experiments on four biomedical datasets with sparse instance annotations show the state-of-the-art performance of our proposed framework. For the first time, we demonstrate the superiority of sparse instance-level supervision on 3D volumetric datasets, compared to common semi-supervision under the same annotation cost.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 稀疏的实例级监督最近被探索用于解决生物医学实例分割中标注不足的问题，相比普通的半监督方法，稀疏的实例级监督更容易标注拥挤实例，并更好地保留3D体积数据集的实例完整性。本文提出了一种通过跨表示亲和力一致性正则化的稀疏监督生物医学实例分割框架。具体来说，我们采用两个独立的网络来强制明确亲和力图和隐式亲和力图之间的扰动一致性，以捕获特征级实例区分和像素级实例边界结构。然后，我们选择每个亲和力图的高置信度区域作为伪标签，监督另一个亲和力图进行亲和力一致性学习。为了获得高置信度区域，我们提出了一种伪标签噪声过滤方案，通过整合两种基于熵的决策策略。在四个具有稀疏实例注释的生物医学数据集上进行的大量实验证明了我们提出的框架的最先进性能。首次，我们证明了与相同标注成本下的普通半监督方法相比，稀疏实例级监督在3D体积数据集上的优越性。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Learning_Cross-Representation_Affinity_Consistency_for_Sparsely_Supervised_Biomedical_Instance_Segmentation_ICCV_2023_paper.pdf) |
| 2023 | TopoSeg: Topology-Aware Nuclear Instance Segmentation | Hongliang He, Jun Wang, Pengxu Wei, Fan Xu, Xiangyang Ji, Chang Liu, Jie Chen | Nuclear instance segmentation has been critical for pathology image analysis in medical science, e.g., cancer diagnosis. Current methods typically adopt pixel-wise optimization for nuclei boundary exploration, where rich structural information could be lost for subsequent quantitative morphology assessment. To address this issue, we develop a topology-aware segmentation approach, termed TopoSeg, which exploits topological structure information to keep the predictions rational, especially in common situations with densely touching and overlapping nucleus instances. Concretely, TopoSeg builds on a topology-aware module (TAM), which encodes dynamic changes of different topology structures within the three-class probability maps (inside, boundary, and background) of the nuclei to persistence barcodes and makes the topology-aware loss function. To efficiently focus on regions with high topological errors, we propose an adaptive topology-aware selection (ATS) strategy to enhance the topology-aware optimization procedure further. Experiments on three nuclear instance segmentation datasets justify the superiority of TopoSeg, which achieves state-of-the-art performance. The code is available at https://github.com/hhlisme/toposeg.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 核实例分割在医学科学中的病理图像分析中至关重要，例如癌症诊断。当前方法通常采用像素级优化来探索细胞核边界，这可能会导致丰富的结构信息在后续定量形态评估中丢失。为解决这一问题，我们开发了一种拓扑感知分割方法，称为TopoSeg，利用拓扑结构信息使预测保持合理，尤其是在密集接触和重叠核实例的常见情况下。具体来说，TopoSeg基于一个拓扑感知模块(TAM)，该模块将细胞核的三类概率图（内部、边界和背景）中不同拓扑结构的动态变化编码为持久条形码，并生成拓扑感知损失函数。为了有效聚焦于具有高拓扑错误的区域，我们提出了一种自适应拓扑感知选择（ATS）策略，进一步增强了拓扑感知优化过程。对三个核实例分割数据集的实验证实了TopoSeg的优越性，其性能达到了最先进水平。代码可在https://github.com/hhlisme/toposeg 上找到。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/He_TopoSeg_Topology-Aware_Nuclear_Instance_Segmentation_ICCV_2023_paper.pdf) |
| 2023 | ConSlide: Asynchronous Hierarchical Interaction Transformer with Breakup-Reorganize Rehearsal for Continual Whole Slide Image Analysis | Yanyan Huang, Weiqin Zhao, Shujun Wang, Yu Fu, Yuming Jiang, Lequan Yu | Whole slide image (WSI) analysis has become increasingly important in the medical imaging community, enabling automated and objective diagnosis, prognosis, and therapeutic-response prediction. However, in clinical practice, the continuous progress of evolving WSI acquisition technology, the diversity of scanners, and different imaging protocols hamper the utility of WSI analysis models. In this paper, we propose the FIRST continual learning framework for WSI analysis, named ConSlide, to tackle the challenges of enormous image size, utilization of hierarchical structure, and catastrophic forgetting by progressive model updating on multiple sequential datasets. Our framework contains three key components. The Hierarchical Interaction Transformer (HIT) is proposed to model and utilize the hierarchical structural knowledge of WSI. The BreakupReorganize (BuRo) rehearsal method is developed for WSI data replay with efficient region storing buffer and WSI reorganizing operation. The asynchronous updating mechanism is devised to encourage the network to learn generic and specific knowledge respectively during the replay stage, based on a nested cross-scale similarity learning (CSSL) module. We evaluated the proposed ConSlide on four public WSI datasets from TCGA projects. It performs best over other state-of-the-art methods with a fair WSI-based continual learning setting and achieves a better trade-off of the overall performance and forgetting on previous tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                             | 全幻灯图像（WSI）分析在医学影像领域变得越来越重要，实现了自动化和客观的诊断、预后和治疗反应预测。然而，在临床实践中，不断进步的WSI采集技术、扫描仪的多样性和不同的成像协议阻碍了WSI分析模型的实用性。在本文中，我们提出了一种名为ConSlide的WSI分析的FIRST持续学习框架，以应对巨大图像大小、利用层次结构和渐进模型更新多个连续数据集时的灾难性遗忘等挑战。我们的框架包含三个关键组件。提出了分层交互变换器（HIT）来建模和利用WSI的层次结构知识。开发了适用于WSI数据重放的BreakupReorganize（BuRo）排练方法，具有高效的区域存储缓冲区和WSI重新组织操作。设计了异步更新机制，鼓励网络在重放阶段分别学习通用和特定知识，基于嵌套跨尺度相似性学习（CSSL）模块。我们在来自TCGA项目的四个公共WSI数据集上评估了提出的ConSlide。它在公平的基于WSI的持续学习设置中表现最佳，并在整体性能和对先前任务的遗忘之间取得了更好的平衡。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_ConSlide_Asynchronous_Hierarchical_Interaction_Transformer_with_Breakup-Reorganize_Rehearsal_for_Continual_ICCV_2023_paper.pdf) |
| 2023 | PRIOR: Prototype Representation Joint Learning from Medical Images and Reports | Pujin Cheng, Li Lin, Junyan Lyu, Yijin Huang, Wenhan Luo, Xiaoying Tang | Contrastive learning based vision-language joint pre-training has emerged as a successful representation learning strategy. In this paper, we present a prototype representation learning framework incorporating both global and local alignment between medical images and reports. In contrast to standard global multi-modality alignment methods, we employ a local alignment module for fine-grained representation. Furthermore, a cross-modality conditional reconstruction module is designed to interchange information across modalities in the training phase by reconstructing masked images and reports. For reconstructing long reports, a sentence-wise prototype memory bank is constructed, enabling the network to focus on low-level localized visual and high-level clinical linguistic features. Additionally, a non-auto-regressive generation paradigm is proposed for reconstructing non-sequential reports. Experimental results on five downstream tasks, including supervised classification, zero-shot classification, image-to-text retrieval, semantic segmentation, and object detection, show the proposed method outperforms other state-of-the-art methods across multiple datasets and under different dataset size settings. The code is available at https://github.com/QtacierP/PRIOR.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 基于对比学习的视觉-语言联合预训练已成为成功的表示学习策略。本文介绍了一个原型表示学习框架，该框架结合了医学图像和报告之间的全局和局部对齐。与标准的全局多模态对齐方法相反，我们采用了一个局部对齐模块用于细粒度表示。此外，设计了一个跨模态条件重构模块，在训练阶段通过重构遮蔽的图像和报告来交换信息。为了重构长报告，构建了一个按句原型记忆库，使网络能够专注于低级局部视觉和高级临床语言特征。此外，提出了一个非自回归生成范式，用于重构非顺序报告。在包括监督分类、零样本分类、图像到文本检索、语义分割和目标检测在内的五项下游任务上的实验结果表明，所提出的方法在多个数据集和不同数据集大小设置下优于其他最先进的方法。代码可在https://github.com/QtacierP/PRIOR 上获得。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_PRIOR_Prototype_Representation_Joint_Learning_from_Medical_Images_and_Reports_ICCV_2023_paper.pdf) |
| 2023 | LIMITR: Leveraging Local Information for Medical Image-Text Representation | Gefen Dawidowicz, Elad Hirsch, Ayellet Tal | Medical imaging analysis plays a critical role in the diagnosis and treatment of various medical conditions. This paper focuses on chest X-ray images and their corresponding radiological reports. It presents a new model that learns a joint X-ray image & report representation. The model is based on a novel alignment scheme between the visual data and the text, which takes into account both local and global information. Furthermore, the model integrates domain-specific information of two types -- lateral images and the consistent visual structure of chest images. Our representation is shown to benefit three types of retrieval tasks: text-image retrieval, class-based retrieval, and phrase-grounding.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 医学影像分析在各种医学疾病的诊断和治疗中发挥着至关重要的作用。本文关注胸部X射线图像及其对应的放射学报告。它提出了一种新模型，学习联合X射线图像和报告表示。该模型基于一种新颖的对齐方案，考虑了本地和全局信息。此外，该模型整合了两种类型的领域特定信息 -- 侧面图像和胸部图像的一致视觉结构。我们的表示被证明有益于三种检索任务：文本-图像检索，基于类别的检索和短语定位。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Dawidowicz_LIMITR_Leveraging_Local_Information_for_Medical_Image-Text_Representation_ICCV_2023_paper.pdf) |
| 2023 | CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image Arbitrary-Scale Super Resolution | Zixuan Chen, Lingxiao Yang, Jian-Huang Lai, Xiaohua Xie | Medical image arbitrary-scale super-resolution (MIASSR) has recently gained widespread attention, aiming to supersample medical volumes at arbitrary scales via a single model. However, existing MIASSR methods face two major limitations: (i) reliance on high-resolution (HR) volumes and (ii) limited generalization ability, which restricts their applications in various scenarios. To overcome these limitations, we propose Cube-based Neural Radiance Field (CuNeRF), a zero-shot MIASSR framework that is able to yield medical images at arbitrary scales and free viewpoints in a continuous domain. Unlike existing MISR methods that only fit the mapping between low-resolution (LR) and HR volumes, CuNeRF focuses on building a continuous volumetric representation from each LR volume without the knowledge from the corresponding HR one. This is achieved by the proposed differentiable modules: cube-based sampling, isotropic volume rendering, and cube-based hierarchical rendering. Through extensive experiments on magnetic resource imaging (MRI) and computed tomography (CT) modalities, we demonstrate that CuNeRF can synthesize high-quality SR medical images, which outperforms state-of-the-art MISR methods, achieving better visual verisimilitude and fewer objectionable artifacts. Compared to existing MISR methods, our CuNeRF is more applicable in practice.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 最近，医学图像任意尺度超分辨率（MIASSR）引起了广泛关注，旨在通过单一模型在任意尺度上超采样医学体积。然而，现有的MIASSR方法面临两个主要限制：（i）依赖于高分辨率（HR）体积和（ii）有限的泛化能力，这限制了它们在各种场景中的应用。为了克服这些限制，我们提出了基于立方体神经辐射场（CuNeRF）的零样本MIASSR框架，能够在连续域中产生任意尺度和自由视角的医学图像。与现有的MISR方法只适应低分辨率（LR）和高分辨率（HR）体积之间的映射不同，CuNeRF专注于从每个LR体积构建连续的体积表示，而无需来自相应HR体积的知识。这是通过提出的可微模块实现的：基于立方体的采样、各向同性体渲染和基于立方体的分层渲染。通过对磁共振成像（MRI）和计算机断层扫描（CT）模态的广泛实验，我们证明CuNeRF能够合成高质量的SR医学图像，优于最先进的MISR方法，具有更好的视觉逼真度和更少的令人反感的伪影。与现有的MISR方法相比，我们的CuNeRF在实践中更具适用性。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CuNeRF_Cube-Based_Neural_Radiance_Field_for_Zero-Shot_Medical_Image_Arbitrary-Scale_ICCV_2023_paper.pdf) |
| 2023 | SimpleClick: Interactive Image Segmentation with Simple Vision Transformers | Qin Liu, Zhenlin Xu, Gedas Bertasius, Marc Niethammer | Click-based interactive image segmentation aims at extracting objects with a limited user clicking. A hierarchical backbone is the de-facto architecture for current methods. Recently, the plain, non-hierarchical Vision Transformer (ViT) has emerged as a competitive backbone for dense prediction tasks. This design allows the original ViT to be a foundation model that can be finetuned for downstream tasks without redesigning a hierarchical backbone for pretraining. Although this design is simple and has been proven effective, it has not yet been explored for interactive segmentation. To fill this gap, we propose SimpleClick, the first plain-backbone method for interactive segmentation. Other than the plain backbone, we also explore several variants of simple feature pyramid networks that only take as input the last feature representation of the backbone. With the plain backbone pretrained as a masked autoencoder (MAE), SimpleClick achieves state-of-the-art performance. Remarkably, our method achieves 4.15 NoC@90 on SBD, improving 21.8% over the previous best result. Extensive evaluation on medical images demonstrates the generalizability of our method. We further develop an extremely tiny ViT backbone for SimpleClick and provide a detailed computational analysis, highlighting its suitability as a practical annotation tool.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 点击交互式图像分割旨在利用有限的用户点击来提取对象。目前的方法通常采用分层骨干结构。最近，普通的非分层Vision Transformer（ViT）已经成为密集预测任务的竞争骨干。这种设计使得原始的ViT可以作为一个基础模型，可以在不重新设计分层骨干进行预训练的情况下微调用于下游任务。尽管这种设计简单且已被证明有效，但尚未被用于交互式分割。为了填补这一空白，我们提出了SimpleClick，这是第一个用于交互式分割的普通骨干方法。除了普通骨干外，我们还探索了几种简单的特征金字塔网络的变体，这些网络只将骨干的最后特征表示作为输入。通过将普通骨干预先训练为掩蔽自编码器（MAE），SimpleClick实现了最先进的性能。值得注意的是，我们的方法在SBD上实现了4.15的NoC@90，比以前最好的结果提高了21.8%。对医学图像的广泛评估证明了我们方法的泛化能力。我们进一步为SimpleClick开发了一个极小的ViT骨干，并提供了详细的计算分析，突显其作为实用标注工具的适用性。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_SimpleClick_Interactive_Image_Segmentation_with_Simple_Vision_Transformers_ICCV_2023_paper.pdf) |
| 2023 | Unsupervised Learning of Object-Centric Embeddings for Cell Instance Segmentation in Microscopy Images | Steffen Wolf, Manan Lalit, Katie McDole, Jan Funke | Segmentation of objects in microscopy images is required for many biomedical applications. We introduce object-centric embeddings (OCEs), which embed image patches such that the spatial offsets between patches cropped from the same object are preserved. Those learnt embeddings can be used to delineate individual objects and thus obtain instance segmentations. Here, we show theoretically that, under assumptions commonly found in microscopy images, OCEs can be learnt through a self-supervised task that predicts the spatial offset between image patches. Together, this forms an unsupervised cell instance segmentation method which we evaluate on nine diverse large-scale microscopy datasets. Segmentations obtained with our method lead to substantially improved results, compared to a state-of-the-art baseline on six out of nine datasets, and perform on par on the remaining three datasets. If ground-truth annotations are available, our method serves as an excellent starting point for supervised training, reducing the required amount of ground-truth needed by one order of magnitude, thus substantially increasing the practical applicability of our method. Source code is available at github.com/funkelab/cellulus.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 显微镜图像中对象的分割在许多生物医学应用中是必需的。我们引入了对象中心嵌入（OCEs），它们嵌入图像块，以便裁剪自同一对象的块之间的空间偏移保持不变。这些学习的嵌入可以用来描绘单个对象，从而获得实例分割。在这里，我们理论上展示了，在显微镜图像中常见的假设下，OCEs可以通过一个自监督任务学习，该任务预测图像块之间的空间偏移。总的来说，这构成了一种无监督的细胞实例分割方法，我们在九个不同的大规模显微镜数据集上进行了评估。与九个数据集中的六个数据集相比，我们的方法得到的分割结果显著提高，并在剩下的三个数据集上表现相当。如果有地面真实标注可用，我们的方法可以作为监督训练的绝佳起点，将所需的地面真实数量减少一个数量级，从而大大增加了我们方法的实际适用性。源代码可在github.com/funkelab/cellulus找到。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Wolf_Unsupervised_Learning_of_Object-Centric_Embeddings_for_Cell_Instance_Segmentation_in_ICCV_2023_paper.pdf) |
| 2023 | XNet: Wavelet-Based Low and High Frequency Fusion Networks for Fully- and Semi-Supervised Semantic Segmentation of Biomedical Images | Yanfeng Zhou, Jiaxing Huang, Chenlong Wang, Le Song, Ge Yang | Fully- and semi-supervised semantic segmentation of biomedical images have been advanced with the development of deep neural networks (DNNs). So far, however, DNN models are usually designed to support one of these two learning schemes, unified models that support both fully- and semi-supervised segmentation remain limited. Furthermore, few fully-supervised models focus on the intrinsic low frequency (LF) and high frequency (HF) information of images to improve performance. Perturbations in consistency-based semi-supervised models are often artificially designed. They may introduce negative learning bias that are not beneficial for training. In this study, we propose a wavelet-based LF and HF fusion model XNet, which supports both fully- and semi-supervised semantic segmentation and outperforms state-of-the-art models in both fields. It emphasizes extracting LF and HF information for consistency training to alleviate the learning bias caused by artificial perturbations. Extensive experiments on two 2D and two 3D datasets demonstrate the effectiveness of our model. Code is available at https://github.com/Yanfeng-Zhou/XNet.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 随着深度神经网络（DNN）的发展，生物医学图像的全监督和半监督语义分割得到了进一步发展。然而，迄今为止，DNN模型通常设计为支持这两种学习方案中的一种，支持全监督和半监督分割的统一模型仍然有限。此外，很少有全监督模型专注于图像的内在低频（LF）和高频（HF）信息以提高性能。基于一致性的半监督模型中的扰动通常是人为设计的。它们可能引入负面学习偏差，这对训练并不有益。在这项研究中，我们提出了基于小波的LF和HF融合模型XNet，支持全监督和半监督语义分割，并在这两个领域中超越了最先进的模型。它强调提取LF和HF信息以进行一致性训练，以减轻人为扰动引起的学习偏差。对两个2D和两个3D数据集的广泛实验表明了我们模型的有效性。代码可在https://github.com/Yanfeng-Zhou/XNet 上找到。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_XNet_Wavelet-Based_Low_and_High_Frequency_Fusion_Networks_for_Fully-_ICCV_2023_paper.pdf) |
| 2023 | CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans | Jieneng Chen, Yingda Xia, Jiawen Yao, Ke Yan, Jianpeng Zhang, Le Lu, Fakai Wang, Bo Zhou, Mingyan Qiu, Qihang Yu, Mingze Yuan, Wei Fang, Yuxing Tang, Minfeng Xu, Jian Zhou, Yuqian Zhao, Qifeng Wang, Xianghua Ye, Xiaoli Yin, Yu Shi, Xin Chen, Jingren Zhou, Alan Yuille, Zaiyi Liu, Ling Zhang | Human readers or radiologists routinely perform full-body multi-organ multi-disease detection and diagnosis in clinical practice, while most medical AI systems are built to focus on single organs with a narrow list of a few diseases. This might severely limit AI's clinical adoption. A certain number of AI models need to be assembled non-trivially to match the diagnostic process of a human reading a CT scan. In this paper, we construct a Unified Tumor Transformer (CancerUniT) model to jointly detect tumor existence & location and diagnose tumor characteristics for eight major cancers in CT scans. CancerUniT is a query-based Mask Transformer model with the output of multi-tumor prediction. We decouple the object queries into organ queries, tumor detection queries and tumor diagnosis queries, and further establish hierarchical relationships among the three groups. This clinically-inspired architecture effectively assists inter- and intra-organ representation learning of tumors and facilitates the resolution of these complex, anatomically related multi-organ cancer image reading tasks. CancerUniT is trained end-to-end using curated large-scale CT images of 10,042 patients including eight major types of cancers and occurring non-cancer tumors (all are pathology-confirmed with 3D tumor masks annotated by radiologists). On the test set of 631 patients, CancerUniT has demonstrated strong performance under a set of clinically relevant evaluation metrics, substantially outperforming both multi-disease methods and an assembly of eight single-organ expert models in tumor detection, segmentation, and diagnosis. This moves one step closer towards a universal high performance cancer screening tool.                                                                                                                                                                                                    | 人类读者或放射科医生在临床实践中经常进行全身多器官多疾病的检测和诊断，而大多数医疗人工智能系统则专注于单个器官和少数疾病的狭窄列表。这可能严重限制人工智能在临床中的应用。需要组装一定数量的人工智能模型，以匹配人类阅读CT扫描的诊断过程。在本文中，我们构建了一个统一的肿瘤变换器（CancerUniT）模型，用于联合检测CT扫描中八种主要癌症的肿瘤存在和位置，并对肿瘤特征进行诊断。CancerUniT是一个基于查询的Mask Transformer模型，输出多肿瘤预测。我们将物体查询分解为器官查询、肿瘤检测查询和肿瘤诊断查询，并进一步建立了三个组之间的层次关系。这种临床启发的架构有效地帮助肿瘤的器官内和器官间表示学习，并促进解决这些复杂、解剖相关的多器官癌症图像阅读任务。CancerUniT使用包括八种主要癌症和发生非癌性肿瘤的10,042名患者的筛选大规模CT图像进行端到端训练（所有这些都经过病理学家确认，放射科医生注释了3D肿瘤掩模）。在631名患者的测试集上，CancerUniT在一组临床相关的评估指标下表现出色，明显优于多疾病方法和八个单器官专家模型的组装，在肿瘤检测、分割和诊断方面表现出色。这将我们迈向更接近一个通用的高性能癌症筛查工具。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CancerUniT_Towards_a_Single_Unified_Model_for_Effective_Detection_Segmentation_ICCV_2023_paper.pdf) |
| 2023 | UniverSeg: Universal Medical Image Segmentation | Victor Ion Butoi, Jose Javier Gonzalez Ortiz, Tianyu Ma, Mert R. Sabuncu, John Guttag, Adrian V. Dalca | While deep learning models have become the predominant method for medical image segmentation, they are typically not capable of generalizing to unseen segmentation tasks involving new anatomies, image modalities, or labels. Given a new segmentation task, researchers generally have to train or fine-tune models. This is time-consuming and poses a substantial barrier for clinical researchers, who often lack the resources and expertise to train neural networks. We present UniverSeg, a method for solving unseen medical segmentation tasks without additional training. Given a query image and an example set of image-label pairs that define a new segmentation task, UniverSeg employs a new CrossBlock mechanism to produce accurate segmentation maps without additional training. To achieve generalization to new tasks, we have gathered and standardized a collection of 53 open-access medical segmentation datasets with over 22,000 scans, which we refer to as MegaMedical. We used this collection to train UniverSeg on a diverse set of anatomies and imaging modalities. We demonstrate that UniverSeg substantially outperforms several related methods on unseen tasks, and thoroughly analyze and draw insights about important aspects of the proposed system. The UniverSeg source code and model weights are freely available at https://universeg.csail.mit.edu.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 深度学习模型已经成为医学图像分割的主要方法，但通常不能推广到涉及新解剖结构、图像模态或标签的未见分割任务。对于新的分割任务，研究人员通常需要训练或微调模型。这是耗时的，对于临床研究人员构成了实质性障碍，他们通常缺乏训练神经网络的资源和专业知识。我们提出了UniverSeg，一种用于解决未见医学分割任务的方法，无需额外训练。给定一个查询图像和定义新分割任务的图像-标签对示例集，UniverSeg采用新的CrossBlock机制生成准确的分割图，无需额外训练。为了实现对新任务的泛化，我们收集并标准化了一个包含53个开放获取的医学分割数据集的集合，超过22,000个扫描，我们称之为MegaMedical。我们使用这个集合在各种解剖结构和成像模态上训练UniverSeg。我们证明UniverSeg在未见任务上明显优于几种相关方法，并对所提出系统的重要方面进行了全面分析和洞察。UniverSeg源代码和模型权重可免费获取：https://universeg.csail.mit.edu。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Butoi_UniverSeg_Universal_Medical_Image_Segmentation_ICCV_2023_paper.pdf) |
| 2023 | Towards Unifying Medical Vision-and-Language Pre-Training via Soft Prompts | Zhihong Chen, Shizhe Diao, Benyou Wang, Guanbin Li, Xiang Wan | Medical vision-and-language pre-training (Med-VLP) has shown promising improvements on many downstream medical tasks owing to its applicability to extracting generic representations from medical images and texts. Practically, there exist two typical types, i.e., the fusion-encoder type and the dual-encoder type, depending on whether a heavy fusion module is used. The former is superior at multi-modal tasks owing to the sufficient interaction between modalities; the latter is good at uni-modal and cross-modal tasks due to the single-modality encoding ability. To take advantage of these two types, we propose an effective yet straightforward scheme named PTUnifier to unify the two types. We first unify the input format by introducing visual and textual prompts, which serve as DETR-like queries that assist in extracting features when one of the modalities is missing. By doing so, a single model could serve as a foundation model that processes various tasks adopting different input formats (i.e., image-only, text-only, and image-text-pair). Furthermore, we construct a prompt pool (instead of static ones) to improve diversity and scalability, enabling queries conditioned on different input instances. Experimental results show that our approach achieves state-of-the-art results on a broad range of tasks, spanning uni-modal tasks (i.e., image/text classification and text summarization), cross-modal tasks (i.e., image-to-text generation and image-text/text-image retrieval), and multi-modal tasks (i.e., visual question answering), demonstrating the effectiveness of our approach. Note that the adoption of prompts is orthogonal to most existing Med-VLP approaches and could be a beneficial and complementary extension to these approaches. The source code is available at https://anonymous.4open.science/r/ICCV-2023-Submission-PTUnifier/ and will be released in the final version of this paper. | 医学视觉与语言预训练（Med-VLP）已经在许多下游医学任务中表现出有希望的改进，因为它适用于从医学图像和文本中提取通用表示。实际上，存在两种典型类型，即融合编码器类型和双编码器类型，具体取决于是否使用重型融合模块。前者在多模态任务中表现优异，因为不同模态之间有足够的交互作用；后者在单模态和跨模态任务中表现良好，因为具有单模态编码能力。为了充分利用这两种类型，我们提出了一种有效而简单的方案，命名为PTUnifier，以统一这两种类型。我们首先通过引入视觉和文本提示来统一输入格式，这些提示充当类似DETR的查询，有助于在其中一种模态缺失时提取特征。通过这样做，一个单一模型可以作为一个基础模型，处理采用不同输入格式的各种任务（即仅图像、仅文本和图像-文本对）。此外，我们构建了一个提示池（而不是静态提示）来提高多样性和可伸缩性，使得查询能够根据不同的输入实例进行条件化。实验结果表明，我们的方法在广泛的任务范围上取得了最先进的结果，包括单模态任务（即图像/文本分类和文本摘要）、跨模态任务（即图像到文本生成和图像-文本/文本-图像检索）以及多模态任务（即视觉问答），展示了我们方法的有效性。值得注意的是，提示的采用与大多数现有的Med-VLP方法是正交的，并且可能是这些方法的有益和互补扩展。源代码可在https://anonymous.4open.science/r/ICCV-2023-Submission-PTUnifier/ 上找到，并将在本文的最终版本中发布。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Towards_Unifying_Medical_Vision-and-Language_Pre-Training_via_Soft_Prompts_ICCV_2023_paper.pdf) |
| 2023 | ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data | Maya Varma, Jean-Benoit Delbrouck, Sarah Hooper, Akshay Chaudhari, Curtis Langlotz | Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained on datasets consisting of image-caption pairs obtained from the web. However, real-world multimodal datasets, such as healthcare data, are significantly more complex: each image (e.g. X-ray) is often paired with text (e.g. physician report) that describes many distinct attributes occurring in fine-grained regions of the image. We refer to these samples as exhibiting high pairwise complexity, since each image-text pair can be decomposed into a large number of region-attribute pairings. The extent to which VLMs can capture fine-grained relationships between image regions and textual attributes when trained on such data has not been previously evaluated. The first key contribution of this work is to demonstrate through systematic evaluations that as the pairwise complexity of the training dataset increases, standard VLMs struggle to learn region-attribute relationships, exhibiting performance degradations of up to 37% on retrieval tasks. In order to address this issue, we introduce ViLLA as our second key contribution. ViLLA, which is trained to capture fine-grained region-attribute relationships from complex datasets, involves two components: (a) a lightweight, self-supervised mapping model to decompose image-text samples into region-attribute pairs, and (b) a contrastive VLM to learn representations from generated region-attribute pairs. We demonstrate with experiments across four domains (synthetic, product, medical, and natural images) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks, such as zero-shot object detection (up to 3.6 AP50 points on COCO and 0.6 mAP points on LVIS) and retrieval (up to 14.2 R-Precision points).                                                                                                                                                              | 视觉语言模型（VLMs），如CLIP和ALIGN，通常在由从网络获取的图像-标题对组成的数据集上进行训练。然而，真实世界中的多模态数据集，如医疗保健数据，显著更加复杂：每个图像（例如X射线）通常与描述图像细粒度区域中许多不同属性的文本（例如医生报告）配对。我们将这些样本称为展示高对复杂性，因为每个图像-文本对可以分解为大量的区域-属性配对。以前尚未评估VLMs在训练此类数据时能够捕捉图像区域和文本属性之间细粒度关系的程度。本文的第一个关键贡献是通过系统评估表明，随着训练数据集的对复杂性增加，标准VLMs难以学习区域-属性关系，在检索任务中表现下降高达37％。为了解决这个问题，我们介绍ViLLA作为我们的第二个关键贡献。ViLLA是针对复杂数据集训练以捕捉细粒度区域-属性关系的，包括两个组件：（a）轻量级的自监督映射模型，将图像-文本样本分解为区域-属性对，以及（b）对比VLM，从生成的区域-属性对中学习表示。我们通过在四个领域（合成、产品、医疗和自然图像）上的实验表明，ViLLA在细粒度推理任务中表现优于可比的VLMs，例如零样本目标检测（在COCO上高达3.6 AP50点和在LVIS上0.6 mAP点）和检索（高达14.2 R-Precision点）。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Varma_ViLLA_Fine-Grained_Vision-Language_Representation_Learning_from_Real-World_Data_ICCV_2023_paper.pdf) |
| 2023 | LightDepth: Single-View Depth Self-Supervision from Illumination Decline | Javier RodrÃ­guez-Puigvert, VÃ­ctor M. Batlle, J.M.M. Montiel, Ruben Martinez-Cantin, Pascal Fua, Juan D. TardÃ³s, Javier Civera | Single-view depth estimation can be remarkably effective if there is enough ground-truth depth data for supervised training. However, there are scenarios, especially in medicine in the case of endoscopies, where such data cannot be obtained. In such cases, multi-view self-supervision and synthetic-to-real transfer serve as alternative approaches, however, with a considerable performance reduction in comparison to supervised case.Instead, we propose a single-view self-supervised method that achieves a performance similar to the supervised case. In some medical devices, such as endoscopes, the camera and light sources are co-located at a small distance from the target surfaces. Thus, we can exploit that, for any given albedo and surface orientation, pixel brightness is inversely proportional to the square of the distance to the surface, providing a strong single-view self-supervisory signal. In our experiments, our self-supervised models deliver accuracies comparable to those of fully supervised ones, while being applicable without depth ground-truth data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 单视图深度估计在有足够的地面实况深度数据用于监督训练时可以非常有效。然而，在某些情况下，特别是在内窥镜等医学场景中，无法获得这样的数据。在这种情况下，多视图自我监督和从合成到实际的转移被视为替代方法，然而与监督情况相比，性能会有相当大的降低。相反，我们提出了一种单视图自我监督方法，其性能与监督情况相似。在一些医疗设备中，如内窥镜，摄像头和光源位于距离目标表面很近的位置。因此，我们可以利用这一点，对于任何给定的反照率和表面方向，像素亮度与到表面的距离的平方成反比，提供了强大的单视图自我监督信号。在我们的实验中，我们的自我监督模型提供了与完全监督模型相当的准确性，同时无需深度实况数据。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Rodriguez-Puigvert_LightDepth_Single-View_Depth_Self-Supervision_from_Illumination_Decline_ICCV_2023_paper.pdf) |
| 2023 | Preserving Tumor Volumes for Unsupervised Medical Image Registration | Qihua Dong, Hao Du, Ying Song, Yan Xu, Jing Liao | Medical image registration is a critical task that estimates the spatial correspondence between pairs of images. However, current traditional and learning-based methods rely on similarity measures to generate a deforming field, which often results in disproportionate volume changes in dissimilar regions, especially in tumor regions. These changes can significantly alter the tumor size and underlying anatomy, which limits the practical use of image registration in clinical diagnosis. To address this issue, we have formulated image registration with tumors as a constraint problem that preserves tumor volumes while maximizing image similarity in other normal regions. Our proposed framework involves a two-stage process. In the first stage, we use similarity-based registration to identify potential tumor regions by their volume change, generating a soft tumor mask accordingly. In the second stage, we propose a volume-preserving registration with a novel adaptive volume-preserving loss that penalizes the change in size adaptively based on the masks calculated from the previous stage. Our approach balances image similarity and volume preservation in different regions, i.e., normal and tumor regions, by using soft tumor masks to adjust the imposition of volume-preserving loss on each one. This ensures that the tumor volume is preserved during the registration process. We have evaluated our framework on various datasets and network architectures, demonstrating that our method successfully preserves the tumor volume while achieving comparable registration results with state-of-the-art methods. Our code is at: https://dddraxxx.github.io/Volume-Preserving-Registration/.                                                                                                                                                                                                                              | 医学图像配准是一项关键任务，它估计了成对图像之间的空间对应关系。然而，目前的传统和基于学习的方法依赖于相似性度量来生成一个变形场，这经常导致不同区域的不成比例的体积变化，特别是在肿瘤区域。这些变化可以显著改变肿瘤的大小和潜在解剖结构，从而限制了图像配准在临床诊断中的实际应用。为了解决这个问题，我们将带有肿瘤的图像配准问题制定为一个约束问题，保留肿瘤体积的同时最大化其他正常区域的图像相似性。我们提出的框架包括一个两阶段过程。在第一阶段，我们使用基于相似性的配准来通过体积变化识别潜在的肿瘤区域，相应地生成一个软性肿瘤掩模。在第二阶段，我们提出了一个带有新颖自适应体积保持损失的保持体积的配准，根据从前一阶段计算得到的掩模自适应地惩罚大小的变化。我们的方法通过使用软性肿瘤掩模在不同区域（即正常和肿瘤区域）上调整体积保持损失的施加，平衡了图像相似性和体积保持。这确保了在配准过程中肿瘤体积的保持。我们在各种数据集和网络架构上评估了我们的框架，证明我们的方法成功地保持了肿瘤体积，同时实现了与最先进方法相媲美的配准结果。我们的代码位于:https://dddraxxx.github.io/Volume-Preserving-Registration/。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Preserving_Tumor_Volumes_for_Unsupervised_Medical_Image_Registration_ICCV_2023_paper.pdf) |
| 2023 | MRM: Masked Relation Modeling for Medical Image Pre-Training with Genetics | Qiushi Yang, Wuyang Li, Baopu Li, Yixuan Yuan | Modern deep learning techniques on automatic multimodal medical diagnosis rely on massive expert annotations, which is time-consuming and prohibitive. Recent masked image modeling (MIM)-based pre-training methods have witnessed impressive advances for learning meaningful representations from unlabeled data and transferring to downstream tasks. However, these methods focus on natural images and ignore the specific properties of medical data, yielding unsatisfying generalization performance on downstream medical diagnosis. In this paper, we aim to leverage genetics to boost image pre-training and present a masked relation modeling (MRM) framework. Instead of explicitly masking input data in previous MIM methods leading to loss of disease-related semantics, we design relation masking to mask out token-wise feature relation in both self- and cross-modality levels, which preserves intact semantics within the input and allows the model to learn rich disease-related information. Moreover, to enhance semantic relation modeling, we propose relation matching to align the sample-wise relation between the intact and masked features. The relation matching exploits inter-sample relation by encouraging global constraints in the feature space to render sufficient semantic relation for feature representation. Extensive experiments demonstrate that the proposed framework is simple yet powerful, achieving state-of-the-art transfer performance on various downstream diagnosis tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                      | 现代深度学习技术在自动多模态医学诊断上依赖于大量专家注释，这是耗时且禁止的。最近基于遮蔽图像建模（MIM）的预训练方法在从未标记数据中学习有意义的表示并转移到下游任务方面取得了令人印象深刻的进展。然而，这些方法侧重于自然图像，忽视了医学数据的特定属性，在下游医学诊断中得到了不令人满意的泛化性能。在本文中，我们旨在利用遗传学来提升图像预训练，并提出了一种遮蔽关系建模（MRM）框架。与先前的MIM方法中明确遮蔽输入数据导致丢失与疾病相关的语义不同，我们设计了关系遮蔽来遮蔽自身和跨模态级别的令牌特征关系，这保留了输入中的完整语义，并允许模型学习丰富的与疾病相关信息。此外，为了增强语义关系建模，我们提出了关系匹配，以使完整和遮蔽特征之间的样本级关系对齐。关系匹配通过鼓励特征空间中的全局约束来利用样本间关系，以为特征表示提供足够的语义关系。大量实验表明，所提出的框架简单而强大，实现了各种下游诊断任务的最先进的传输性能。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_MRM_Masked_Relation_Modeling_for_Medical_Image_Pre-Training_with_Genetics_ICCV_2023_paper.pdf) |
| 2023 | Gram-based Attentive Neural Ordinary Differential Equations Network for Video Nystagmography Classification | Xihe Qiu, Shaojie Shi, Xiaoyu Tan, Chao Qu, Zhijun Fang, Hailing Wang, Yongbin Gao, Peixia Wu, Huawei Li | Video nystagmography (VNG) is the diagnostic gold standard of benign paroxysmal positional vertigo (BPPV), which requires medical professionals to examine the direction, frequency, intensity, duration, and variation in the strength of nystagmus on a VNG video. This is a tedious process heavily influenced by the doctor's experience, which is error-prone. Recent automatic VNG classification methods approach this problem from the perspective of video analysis without considering medical prior knowledge, resulting in unsatisfactory accuracy and limited diagnostic capability for nystagmographic types, thereby preventing their clinical application. In this paper, we propose an end-to-end data-driven novel BPPV diagnosis framework (TC-BPPV) by considering this problem as an eye trajectory classification problem due to the disease's symptoms and experts' prior knowledge. In this framework, we utilize an eye movement tracking system to capture the eye trajectory and propose the Gram-based attentive neural ordinary differential equations network (Gram-AODE) to perform classification. We validate our framework using the VNG dataset provided by the collaborative university hospital and achieve state-of-the-art performance. We also evaluate Gram-AODE on multiple open-source benchmarks to demonstrate its effectiveness in trajectory classification. Code is available at https://github.com/XiheQiu/Gram-AODE.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 视频眼震图（VNG）是良性阵发性位置性眩晕（BPPV）的诊断黄金标准，需要医疗专业人员检查VNG视频中眼球震颤的方向、频率、强度、持续时间和强度变化。这是一个繁琐的过程，严重受到医生经验的影响，容易出错。最近的自动VNG分类方法从视频分析的角度解决了这个问题，但没有考虑医学先验知识，导致了不令人满意的准确性和对眼震图类型的诊断能力有限，从而阻碍了它们的临床应用。本文提出了一个端到端的数据驱动的新型BPPV诊断框架（TC-BPPV），将这个问题视为一种眼球轨迹分类问题，考虑了疾病症状和专家的先验知识。在这个框架中，我们利用眼动跟踪系统捕捉眼球轨迹，并提出了基于Gram的注意神经常微分方程网络（Gram-AODE）进行分类。我们使用协作大学医院提供的VNG数据集验证了我们的框架，并取得了最先进的性能。我们还在多个开源基准上评估了Gram-AODE，以展示其在轨迹分类中的有效性。代码可在https://github.com/XiheQiu/Gram-AODE获得。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Qiu_Gram-based_Attentive_Neural_Ordinary_Differential_Equations_Network_for_Video_Nystagmography_ICCV_2023_paper.pdf) |
| 2023 | Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation | Aishik Konwer, Xiaoling Hu, Joseph Bae, Xuan Xu, Chao Chen, Prateek Prasanna | In medical vision, different imaging modalities provide complementary information. However, in practice, not all modalities may be available during inference or even training. Previous approaches, e.g., knowledge distillation or image synthesis, often assume the availability of full modalities for all patients during training; this is unrealistic and impractical due to the variability in data collection across sites. We propose a novel approach to learn enhanced modality-agnostic representations by employing a meta-learning strategy in training, even when only limited full modality samples are available. Meta-learning enhances partial modality representations to full modality representations by meta-training on partial modality data and meta-testing on limited full modality samples. Additionally, we co-supervise this feature enrichment by introducing an auxiliary adversarial learning branch. More specifically, a missing modality detector is used as a discriminator to mimic the full modality setting. Our segmentation framework significantly outperforms state-of-the-art brain tumor segmentation techniques in missing modality scenarios.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 在医学视觉领域，不同的成像模态提供了互补信息。然而，在实践中，并非所有模态在推断甚至训练过程中都可用。先前的方法，如知识蒸馏或图像合成，通常假设在训练过程中所有患者都有完整模态的数据；由于数据收集在不同地点存在变化，这是不切实际和不实用的。我们提出了一种新颖的方法，通过在训练中采用元学习策略来学习增强的模态不可知表示，即使只有有限数量的完整模态样本可用。元学习通过在部分模态数据上进行元训练，并在有限数量的完整模态样本上进行元测试，将部分模态表示增强为完整模态表示。此外，我们通过引入辅助对抗学习分支来共同监督这种特征增强。更具体地说，我们使用缺失模态检测器作为鉴别器来模拟完整模态设置。我们的分割框架在缺失模态情景下显着优于最先进的脑肿瘤分割技术。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Konwer_Enhancing_Modality-Agnostic_Representations_via_Meta-Learning_for_Brain_Tumor_Segmentation_ICCV_2023_paper.pdf) |
| 2023 | Continual Segment: Towards a Single, Unified and Non-forgetting Continual Segmentation Model of 143 Whole-body Organs in CT Scans | Zhanghexuan Ji, Dazhou Guo, Puyang Wang, Ke Yan, Le Lu, Minfeng Xu, Qifeng Wang, Jia Ge, Mingchen Gao, Xianghua Ye, Dakai Jin | Deep learning empowers the mainstream medical image segmentation methods. Nevertheless, current deep segmentation approaches are not capable of efficiently and effectively adapting and updating the trained models when new segmentation classes are incrementally added. In the real clinical environment, it can be preferred that segmentation models could be dynamically extended to segment new organs/tumors without the (re-)access to previous training datasets due to obstacles of patient privacy and data storage. This process can be viewed as a continual semantic segmentation (CSS) problem, being understudied for multi-organ segmentation. In this work, we propose a new architectural CSS learning framework to learn a single deep segmentation model for segmenting a total of 143 whole-body organs. Using the encoder/decoder network structure, we demonstrate that a continually trained then frozen encoder coupled with incrementally-added decoders can extract sufficiently representative image features for new classes to be subsequently and validly segmented, while avoiding the catastrophic forgetting in CSS. To maintain a single network model complexity, each decoder is progressively pruned using neural architecture search and teacher-student based knowledge distillation. Finally, we propose a body-part and anomaly-aware output merging module to combine organ predictions originating from different decoders and incorporate both healthy and pathological organs appearing in different datasets. Trained and validated on 3D CT scans of 2500+ patients from four datasets, our single network can segment a total of 143 whole-body organs with very high accuracy, closely reaching the upper bound performance level by training four separate segmentation models (i.e., one model per dataset/task).                                                                                                            | 深度学习赋予主流医学图像分割方法更强大的能力。然而，当前的深度分割方法并不能有效地适应和更新已训练模型，当新的分割类别逐渐增加时。在真实的临床环境中，人们更倾向于分割模型能够动态扩展以分割新的器官/肿瘤，而无需重新访问以前的训练数据集，因为患者隐私和数据存储的障碍。这个过程可以被视为一个持续语义分割（CSS）问题，在多器官分割方面尚未得到充分研究。在这项工作中，我们提出了一个新的架构CSS学习框架，用于学习一个单一的深度分割模型，用于分割共计143个全身器官。通过使用编码器/解码器网络结构，我们证明了一个持续训练然后冻结编码器，再加上逐渐添加的解码器，可以提取足够代表性的图像特征，以便随后和有效地对新类别进行分割，同时避免CSS中的灾难性遗忘。为了保持单一网络模型的复杂性，每个解码器都会逐步经过神经架构搜索和基于师生的知识蒸馏进行剪枝。最后，我们提出了一个身体部位和异常感知的输出合并模块，以合并来自不同解码器的器官预测，并结合出现在不同数据集中的健康和病理器官。在来自四个数据集的2500多名患者的3D CT扫描上进行训练和验证后，我们的单一网络可以以非常高的准确性分割共计143个全身器官，几乎接近通过训练四个单独的分割模型（即每个数据集/任务一个模型）达到的性能上限水平。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_Continual_Segment_Towards_a_Single_Unified_and_Non-forgetting_Continual_Segmentation_ICCV_2023_paper.pdf) |
| 2023 | CauSSL: Causality-inspired Semi-supervised Learning for Medical Image Segmentation | Juzheng Miao, Cheng Chen, Furui Liu, Hao Wei, Pheng-Ann Heng | Semi-supervised learning (SSL) has recently demonstrated great success in medical image segmentation, significantly enhancing data efficiency with limited annotations. However, despite its empirical benefits, there are still concerns in the literature about the theoretical foundation and explanation of semi-supervised segmentation. To explore this problem, this study first proposes a novel causal diagram to provide a theoretical foundation for the mainstream semi-supervised segmentation methods. Our causal diagram takes two additional intermediate variables into account, which are neglected in previous work. Drawing from this proposed causal diagram, we then introduce a causality-inspired SSL approach on top of co-training frameworks called CauSSL, to improve SSL for medical image segmentation. Specifically, we first point out the importance of algorithmic independence between two networks or branches in SSL, which is often overlooked in the literature. We then propose a novel statistical quantification of the uncomputable algorithmic independence and further enhance the independence via a min-max optimization process. Our method can be flexibly incorporated into different existing SSL methods to improve their performance. Our method has been evaluated on three challenging medical image segmentation tasks using both 2D and 3D network architectures and has shown consistent improvements over state-of-the-art methods. Our code is publicly available at: https://github.com/JuzhengMiao/CauSSL.                                                                                                                                                                                                                                                                                                                                                                                                            | 半监督学习（SSL）最近在医学图像分割领域取得了巨大成功，显著提高了在有限注释情况下的数据效率。然而，尽管在实证方面取得了成功，文献中仍存在对半监督分割的理论基础和解释的担忧。为了探讨这个问题，本研究首先提出了一个新颖的因果图，为主流半监督分割方法提供了理论基础。我们的因果图考虑了两个被忽视的中间变量，这在先前的工作中被忽略了。基于这个提出的因果图，我们引入了一种基于因果关系的SSL方法，称为CauSSL，用于改进医学图像分割的SSL。具体来说，我们首先指出SSL中两个网络或分支之间的算法独立性的重要性，在文献中经常被忽视。然后，我们提出了一种新颖的统计量化不可计算的算法独立性，并通过最小-最大优化过程进一步增强了独立性。我们的方法可以灵活地整合到不同的现有SSL方法中，以提高它们的性能。我们的方法已经在三个具有挑战性的医学图像分割任务上进行了评估，使用了2D和3D网络架构，并且与最先进的方法相比表现出了一致的改进。我们的代码公开可用于：https://github.com/JuzhengMiao/CauSSL。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Miao_CauSSL_Causality-inspired_Semi-supervised_Learning_for_Medical_Image_Segmentation_ICCV_2023_paper.pdf) |
| 2023 | Segmentation of Tubular Structures Using Iterative Training with Tailored Samples | Wei Liao | We propose a minimal path method to simultaneously compute segmentation masks and extract centerlines of tubular structures with line-topology. Minimal path methods are commonly used for the segmentation of tubular structures in a wide variety of applications. Recent methods use features extracted by CNNs, and often outperform methods using hand-tuned features. However, for CNN-based methods, the samples used for training may be generated inappropriately, so that they can be very different from samples encountered during inference. We approach this discrepancy by introducing a novel iterative training scheme, which enables generating better training samples specifically tailored for the minimal path methods without changing existing annotations. In our method, segmentation masks and centerlines are not determined after one another by post-processing, but obtained using the same steps. Our method requires only very few annotated training images. Comparison with seven previous approaches on three public datasets, including satellite images and medical images, shows that our method achieves state-of-the-art results both for segmentation masks and centerlines.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 我们提出了一种最小路径方法，可以同时计算具有线拓扑结构的管状结构的分割掩模并提取中心线。最小路径方法通常用于在各种应用中分割管状结构。最近的方法使用由CNN提取的特征，并且通常优于使用手动调整特征的方法。然而，对于基于CNN的方法，用于训练的样本可能生成不当，因此它们可能与推理过程中遇到的样本非常不同。我们通过引入一种新颖的迭代训练方案来解决这种差异，该方案能够生成更好的专门针对最小路径方法的训练样本，而无需更改现有的注释。在我们的方法中，分割掩模和中心线不是通过后处理一个接一个地确定，而是使用相同的步骤获取。我们的方法仅需要非常少的带注释的训练图像。与三个公共数据集上的七种先前方法进行比较，包括卫星图像和医学图像，在分割掩模和中心线方面，我们的方法均取得了最先进的结果。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Liao_Segmentation_of_Tubular_Structures_Using_Iterative_Training_with_Tailored_Samples_ICCV_2023_paper.pdf) |
| 2023 | BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification | Yuanhong Chen, Fengbei Liu, Hu Wang, Chong Wang, Yuyuan Liu, Yu Tian, Gustavo Carneiro | Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-Ray (CXR) classifiers have been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current multi-class noisy-label learning methods cannot be easily adapted. In this paper, we propose a new method designed for noisy multi-label CXR learning, which detects and smoothly re-labels noisy samples from the dataset to be used in the training of common multi-label classifiers. The proposed method optimises a bag of multi-label descriptors (BoMD) to promote their similarity with the semantic descriptors produced by language models from multi-label image annotations. Our experiments on noisy multi-label training sets and clean testing sets show that our model has state-of-the-art accuracy and robustness in many CXR multi-label classification benchmarks, including a new benchmark that we propose to systematically assess noisy multi-label methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 深度学习方法在医学影像问题中显示出出色的分类准确性，这在很大程度上归因于手动注释了干净标签的大规模数据集的可用性。然而，鉴于这种手动注释的高成本，新的医学影像分类问题可能需要依赖从放射学报告中提取的机器生成的嘈杂标签。事实上，许多胸部X射线（CXR）分类器是从带有嘈杂标签的数据集建模的，但它们的训练过程通常对嘈杂标签样本不够稳健，导致次优模型。此外，CXR数据集大多是多标签的，因此目前的多类嘈杂标签学习方法不能轻松地进行调整。在本文中，我们提出了一种专为嘈杂多标签CXR学习设计的新方法，该方法检测并平滑地重新标记数据集中的嘈杂样本，以用于训练常见的多标签分类器。所提出的方法优化了一组多标签描述符（BoMD），以促进它们与由语言模型从多标签图像注释中产生的语义描述符的相似性。我们在嘈杂多标签训练集和干净测试集上的实验表明，我们的模型在许多CXR多标签分类基准测试中具有最先进的准确性和鲁棒性，包括我们提出的用于系统评估嘈杂多标签方法的新基准测试。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_BoMD_Bag_of_Multi-label_Descriptors_for_Noisy_Chest_X-ray_Classification_ICCV_2023_paper.pdf) |
| 2023 | Taxonomy Adaptive Cross-Domain Adaptation in Medical Imaging via Optimization Trajectory Distillation | Jianan Fan, Dongnan Liu, Hang Chang, Heng Huang, Mei Chen, Weidong Cai | The success of automated medical image analysis depends on large-scale and expert-annotated training sets. Unsupervised domain adaptation (UDA) has been raised as a promising approach to alleviate the burden of labeled data collection. However, they generally operate under the closed-set adaptation setting assuming an identical label set between the source and target domains, which is over-restrictive in clinical practice where new classes commonly exist across datasets due to taxonomic inconsistency. While several methods have been presented to tackle both domain shifts and incoherent label sets, none of them take into account the common characteristics of the two issues and consider the learning dynamics along network training. In this work, we propose optimization trajectory distillation, a unified approach to address the two technical challenges from a new perspective. It exploits the low-rank nature of gradient space and devises a dual-stream distillation algorithm to regularize the learning dynamics of insufficiently annotated domain and classes with the external guidance obtained from reliable sources. Our approach resolves the issue of inadequate navigation along network optimization, which is the major obstacle in the taxonomy adaptive cross-domain adaptation scenario. We evaluate the proposed method extensively on several tasks towards various endpoints with clinical significance. The results demonstrate its effectiveness and improvements over previous methods.                                                                                                                                                                                                                                                                                                                                                                                                                             | 自动化医学图像分析的成功取决于大规模和专家注释的训练集。无监督域适应（UDA）被提出作为一种有前途的方法，以减轻标记数据收集的负担。然而，它们通常在封闭集适应设置下运行，假设源域和目标域之间具有相同的标签集，这在临床实践中是过于严格的，因为由于分类不一致，新类通常存在于数据集中。虽然已经提出了几种方法来解决域漂移和不一致的标签集的问题，但没有一种方法考虑到这两个问题的共同特征，并考虑网络训练过程中的学习动态。在这项工作中，我们提出了优化轨迹蒸馏，这是一种从新的角度解决这两个技术挑战的统一方法。它利用梯度空间的低秩特性，并设计了一个双流蒸馏算法，以规范不足注释的域和类别的学习动态，并从可靠来源获取外部指导。我们的方法解决了沿着网络优化不足导航的问题，这是在分类自适应跨域适应情景中的主要障碍。我们在几项具有临床意义的任务上对所提出的方法进行了广泛评估。结果表明其有效性和改进优于先前的方法。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_Taxonomy_Adaptive_Cross-Domain_Adaptation_in_Medical_Imaging_via_Optimization_Trajectory_ICCV_2023_paper.pdf) |
| 2023 | CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection | Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, Zongwei Zhou | An increasing number of public datasets have shown a marked impact on automated organ segmentation and tumor detection. However, due to the small size and partially labeled problem of each dataset, as well as a limited investigation of diverse types of tumors, the resulting models are often limited to segmenting specific organs/tumors and ignore the semantics of anatomical structures, nor can they be extended to novel domains To address these issues, we propose the CLIP-Driven Universal Model, which incorporates text embedding learned from Contrastive Language-Image Pre-training (CLIP) to segmentation models. This CLIP-based label encoding captures anatomical relationships, enabling the model to learn a structured feature embedding and segment 25 organs and 6 types of tumors. The proposed model is developed from an assembly of 14 datasets, using a total of 3,410 CT scans for training and then evaluated on 6,162 external CT scans from 3 additional datasets. We rank first on the Medical Segmentation Decathlon (MSD) public leaderboard and achieve state-of-the-art results on Beyond The Cranial Vault (BTCV). Additionally, the Universal Model is computationally more efficient (6xfaster) compared with dataset-specific models, generalized better to CT scans from varying sites, and shows stronger transfer learning performance on novel tasks.| 随着越来越多的公共数据集对自动器官分割和肿瘤检测产生了显著影响。然而，由于每个数据集的规模较小且部分标记问题，以及对不同类型肿瘤的研究有限，因此所得到的模型通常局限于分割特定器官/肿瘤，并且忽视了解剖结构的语义，也无法扩展到新领域。为了解决这些问题，我们提出了基于CLIP（对比语言-图像预训练）学习的文本嵌入的CLIP驱动通用模型，将其纳入到分割模型中。这种基于CLIP的标签编码捕获了解剖关系，使模型能够学习结构化特征嵌入并分割25个器官和6种类型的肿瘤。所提出的模型从14个数据集的组合中开发而来，使用共计3,410个CT扫描进行训练，然后在来自3个额外数据集的6,162个外部CT扫描上进行评估。我们在医学分割十项全能赛（MSD）公共排行榜上排名第一，并在Beyond The Cranial Vault（BTCV）上取得了最先进的结果。此外，通用模型在计算上更高效（快6倍），相比于特定数据集的模型，更好地适用于来自不同站点的CT扫描，并在新任务上表现出更强的迁移学习性能。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_CLIP-Driven_Universal_Model_for_Organ_Segmentation_and_Tumor_Detection_ICCV_2023_paper.pdf) |
| 2023 | MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training for X-ray Diagnosis | Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie |    In this paper, we consider enhancing medical visual-language pre-training (VLP) with domain-specific knowledge, by exploiting the paired image-text reports from the radiological daily practice. In particular, we make the following contributions: First, unlike existing works that directly process the raw reports, we adopt a novel triplet extraction module to extract the medical-related information, avoiding unnecessary complexity from language grammar and enhancing the supervision signals; Second, we propose a novel triplet encoding module with entity translationby querying a knowledge base, to exploit the rich domain knowledge in medical field, and implicitly build relationships between medical entities in the language embedding space; Third, we propose to use a Transformer-based fusion model for spatially aligning the entity description with visual signals at the image patch level, enabling the ability for medical diagnosis; Fourth, we conduct thorough experiments to validate the effectiveness of our architecture, and benchmark on numerous public benchmarks e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax, COVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning settings, our model has demonstrated strong performance compared with the former methods on disease classification and grounding.| 在本文中，我们考虑利用放射学日常实践中成对的图像-文本报告，通过加入领域特定知识来增强医学视觉语言预训练（VLP）。具体来说，我们做出了以下贡献：首先，与直接处理原始报告的现有作品不同，我们采用了一种新颖的三元组提取模块，以提取医学相关信息，避免语言语法带来的不必要复杂性，并增强了监督信号；其次，我们提出了一种通过查询知识库进行实体翻译的新颖三元组编码模块，以利用医学领域丰富的领域知识，并在语言嵌入空间中隐含地建立医学实体之间的关系；第三，我们提出使用基于Transformer的融合模型来在图像补丁级别上空间对齐实体描述与视觉信号，实现医学诊断的能力；第四，我们进行了彻底的实验证实我们架构的有效性，并在许多公共基准上进行了基准测试，例如ChestX-ray14、RSNA Pneumonia、SIIM-ACR Pneumothorax、COVIDx CXR-2、COVID Rural和EdemaSeverity。在零-shot和微调设置中，我们的模型在疾病分类和定位方面表现出与以前方法相比的强大性能。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_MedKLIP_Medical_Knowledge_Enhanced_Language-Image_Pre-Training_for_X-ray_Diagnosis_ICCV_2023_paper.pdf) |
| 2023 | MAGI: Multi-Annotated Explanation-Guided Learning | Yifei Zhang, Siyi Gu, Yuyang Gao, Bo Pan, Xiaofeng Yang, Liang Zhao | Explanation supervision is a technique in which the model is guided by human-generated explanations during training. This technique aims to improve the predictability of the model by incorporating human understanding of the prediction process into the training phase. This is a challenging task since it relies on the accuracy of human annotation labels. To obtain high-quality explanation annotations, using multiple annotations to do explanation supervision is a reasonable method. However, how to use multiple annotations to improve accuracy is particularly challenging due to the following: 1) The noisiness of annotations from different annotators; 2) The lack of pre-given information about the corresponding relationship between annotations and annotators; 3) Missing annotations since some images are not labeled by all annotators. To solve these challenges, we propose a Multi-annotated explanation-guided learning (MAGI) framework to do explanation supervision with comprehensive and high-quality generated annotations. We first propose a novel generative model to generate annotations from all annotators and infer them using a newly proposed variational inference-based technique by learning the characteristics of each annotator. We also incorporate an alignment mechanism into the generative model to infer the correspondence between annotations and annotators in the training process. Extensive experiments on two datasets from the medical imaging domain demonstrate the effectiveness of our proposed framework in handling noisy annotations while obtaining superior prediction performance compared with previous SOTA. | 解释监督是一种技术，在这种技术中，在训练过程中，模型受到人类生成的解释的指导。该技术旨在通过将人类对预测过程的理解纳入训练阶段来提高模型的可预测性。这是一项具有挑战性的任务，因为它依赖于人类注释标签的准确性。为了获得高质量的解释标注，使用多个注释来进行解释监督是一种合理的方法。然而，如何利用多个注释来提高准确性是特别具有挑战性的，原因如下：1）不同注释者的注释噪声；2）关于注释和注释者之间对应关系的预先给定信息的缺乏；3）由于某些图像未被所有注释者标记而导致的缺失注释。为了解决这些挑战，我们提出了一个名为Multi-annotated explanation-guided learning（MAGI）的框架，用于使用全面和高质量的生成注释进行解释监督。我们首先提出了一种新颖的生成模型，从所有注释者那里生成注释，并通过学习每个注释者的特征，使用一种新提出的基于变分推断的技术来推断这些注释。我们还将一种对齐机制整合到生成模型中，在训练过程中推断注释与注释者之间的对应关系。对医学影像领域的两个数据集进行的大量实验证明了我们提出的框架在处理嘈杂的注释时的有效性，同时获得了优于先前最先进技术的预测性能。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MAGI_Multi-Annotated_Explanation-Guided_Learning_ICCV_2023_paper.pdf) |
| 2023 | Multi-granularity Interaction Simulation for Unsupervised Interactive Segmentation | Kehan Li, Yian Zhao, Zhennan Wang, Zesen Cheng, Peng Jin, Xiangyang Ji, Li Yuan, Chang Liu, Jie Chen | Interactive segmentation enables users to segment as needed by providing cues of objects, which introduces human-computer interaction for many fields, such as image editing and medical image analysis. Typically, massive and expansive pixel-level annotations are spent to train deep models by object-oriented interactions with manually labeled object masks. In this work, we reveal that informative interactions can be made by simulation with semantic-consistent yet diverse region exploration in an unsupervised paradigm. Concretely, we introduce a Multi-granularity Interaction Simulation (MIS) approach to open up a promising direction for unsupervised interactive segmentation. Drawing on the high-quality dense features produced by recent self-supervised models, we propose to gradually merge patches or regions with similar features to form more extensive regions and thus, every merged region serves as a semantic-meaningful multi-granularity proposal. By randomly sampling these proposals and simulating possible interactions based on them, we provide meaningful interaction at multiple granularities to teach the model to understand interactions. Our MIS significantly outperforms non-deep learning unsupervised methods and is even comparable with some previous deep-supervised methods without any annotation. | 交互式分割通过提供对象线索使用户能够根据需要进行分割，从而为诸如图像编辑和医学图像分析等许多领域引入了人机交互。通常，通过与手动标记的对象掩膜进行面向对象的交互，需要大量和广泛的像素级注释来训练深度模型。在这项工作中，我们揭示了在无监督范例中可以通过模拟进行语义一致且多样化的区域探索来进行信息交互。具体而言，我们引入了一种多粒度交互模拟（MIS）方法，为无监督交互式分割开辟了一个有前途的方向。借鉴最近自监督模型产生的高质量密集特征，我们提出逐渐合并具有相似特征的补丁或区域以形成更广泛的区域，因此，每个合并的区域都作为一个语义有意义的多粒度提案。通过随机采样这些提案并根据它们进行可能的交互模拟，我们在多个粒度上提供有意义的交互，以教导模型理解交互。我们的MIS明显优于非深度学习的无监督方法，并且甚至与一些以前的深度监督方法相媲美，而不需要任何注释。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Multi-granularity_Interaction_Simulation_for_Unsupervised_Interactive_Segmentation_ICCV_2023_paper.pdf) |
| 2023 | FRAug: Tackling Federated Learning with Non-IID Features via Representation Augmentation | Haokun Chen, Ahmed Frikha, Denis Krompass, Jindong Gu, Volker Tresp | Federated Learning (FL) is a decentralized machine learning paradigm, in which multiple clients collaboratively train neural networks without centralizing their local data, and hence preserve data privacy. However, real-world FL applications usually encounter challenges arising from distribution shifts across the local datasets of individual clients. These shifts may drift the global model aggregation or result in convergence to deflected local optimum. While existing efforts have addressed distribution shifts in the label space, an equally important challenge remains relatively unexplored. This challenge involves situations where the local data of different clients indicate identical label distributions but exhibit divergent feature distributions. This issue can significantly impact the global model performance in the FL framework. In this work, we propose Federated Representation Augmentation (FRAug) to resolve this practical and challenging problem. FRAug optimizes a shared embedding generator to capture client consensus. Its output synthetic embeddings are transformed into client-specific by a locally optimized RTNet to augment the training space of each client. Our empirical evaluation on three public benchmarks and a real-world medical dataset demonstrates the effectiveness of the proposed method, which substantially outperforms the current state-of-the-art FL methods for feature distribution shifts, including PartialFed and FedBN. | 联邦学习（FL）是一种分散的机器学习范式，在这种范式中，多个客户端协作训练神经网络，而不集中他们的本地数据，从而保护数据隐私。然而，现实世界中的FL应用通常会遇到因个体客户端的本地数据集之间的分布偏移而产生的挑战。这些偏移可能会导致全局模型聚合漂移或收敛到偏移的局部最优解。尽管现有的研究已经解决了标签空间中的分布偏移问题，但同样重要的挑战仍未得到充分探讨。这一挑战涉及到不同客户端的本地数据显示相同标签分布但表现出不同特征分布的情况。这个问题可以显著影响FL框架中全局模型的性能。在本文中，我们提出了联邦表示增强（FRAug）来解决这个实际且具有挑战性的问题。FRAug优化了一个共享嵌入生成器来捕捉客户端的共识。其输出的合成嵌入通过局部优化的RTNet转换为每个客户端的特定嵌入，以增强每个客户端的训练空间。我们在三个公共基准测试和一个真实世界的医学数据集上的实证评估显示了所提出方法的有效性，该方法在处理特征分布偏移方面明显优于当前的最先进FL方法，包括PartialFed和FedBN。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_FRAug_Tackling_Federated_Learning_with_Non-IID_Features_via_Representation_Augmentation_ICCV_2023_paper.pdf) |
| 2023 | Bayesian Optimization Meets Self-Distillation | HyunJae Lee, Heon Song, Hyeonsoo Lee, Gi-hyeon Lee, Suyeong Park, Donggeun Yoo | Bayesian optimization (BO) has contributed greatly to improving model performance by suggesting promising hyperparameter configurations iteratively based on observations from multiple training trials. However, only partial knowledge (i.e., the measured performances of trained models and their hyperparameter configurations) from previous trials is transferred. On the other hand, Self-Distillation (SD) only transfers partial knowledge learned by the task model itself. To fully leverage the various knowledge gained from all training trials, we propose the BOSS framework, which combines BO and SD. BOSS suggests promising hyperparameter configurations through BO and carefully selects pre-trained models from previous trials for SD, which are otherwise abandoned in the conventional BO process. BOSS achieves significantly better performance than both BO and SD in a wide range of tasks including general image classification, learning with noisy labels, semi-supervised learning, and medical image analysis tasks. Our code is available at https://github.com/sooperset/boss. | 贝叶斯优化（BO）通过根据多次训练试验的观察建议有前途的超参数配置，大大提高了模型性能。然而，仅从先前试验中转移了部分知识（即训练模型的测量性能及其超参数配置）。另一方面，自我蒸馏（SD）仅传递任务模型本身学到的部分知识。为了充分利用从所有训练试验中获得的各种知识，我们提出了BOSS框架，结合了BO和SD。BOSS通过BO建议有前途的超参数配置，并仔细选择先前试验中的预训练模型进行SD，否则这些模型在传统的BO过程中会被放弃。在包括一般图像分类、带有噪声标签的学习、半监督学习和医学图像分析任务在内的各种任务中，BOSS的性能明显优于BO和SD。我们的代码可在https://github.com/sooperset/boss获得。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Bayesian_Optimization_Meets_Self-Distillation_ICCV_2023_paper.pdf) |
| 2023 | Foreground-Background Separation through Concept Distillation from Generative Image Foundation Models | Mischa Dombrowski, Hadrien Reynaud, Matthew Baugh, Bernhard Kainz | Curating datasets for object segmentation is a difficult task. With the advent of large-scale pre-trained generative models, conditional image generation has been given a significant boost in result quality and ease of use. In this paper, we present a novel method that enables the generation of general foreground-background segmentation models from simple textual descriptions, without requiring segmentation labels. We leverage and explore pre-trained latent diffusion models, to automatically generate weak segmentation masks for concepts and objects. The masks are then used to fine-tune the diffusion model on an inpainting task, which enables fine-grained removal of the object, while at the same time providing a synthetic foreground and background dataset. We demonstrate that using this method beats previous methods in both discriminative and generative performance and closes the gap with fully supervised training while requiring no pixel-wise object labels. We show results on the task of segmenting four different objects (humans, dogs, cars, birds) and a use case scenario in medical image analysis. The code is available at https://github.com/MischaD/fobadiffusion. | 为对象分割筛选数据集是一项困难的任务。随着大规模预训练生成模型的出现，有条件的图像生成在结果质量和易用性方面得到了显著提升。本文提出了一种新颖的方法，可以从简单的文本描述中生成通用的前景-背景分割模型，而无需分割标签。我们利用和探索预训练的潜在扩散模型，自动生成概念和对象的弱分割蒙版。然后使用这些蒙版来微调扩散模型进行修补任务，从而实现对对象的细粒度移除，同时提供合成的前景和背景数据集。我们证明，使用这种方法在辨别和生成性能方面均优于先前的方法，并在不需要像素级对象标签的情况下缩小了与完全监督训练之间的差距。我们展示了在分割四种不同对象（人类、狗、汽车、鸟类）以及医学图像分析中的使用案例场景的结果。代码可在https://github.com/MischaD/fobadiffusion 上找到。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Dombrowski_Foreground-Background_Separation_through_Concept_Distillation_from_Generative_Image_Foundation_Models_ICCV_2023_paper.pdf) |
| 2023 | Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction | Jinhong Wang, Yi Cheng, Jintai Chen, TingTing Chen, Danny Chen, Jian Wu | Ordinal regression refers to classifying object instances into ordinal categories. It has been widely studied in many scenarios, such as medical disease grading and movie rating. Known methods focused only on learning inter-class ordinal relationships, but still incur limitations in distinguishing adjacent categories thus far. In this paper, we propose a simple sequence prediction framework for ordinal regression called Ord2Seq, which, for the first time, transforms each ordinal category label into a special label sequence and thus regards an ordinal regression task as a sequence prediction process. In this way, we decompose an ordinal regression task into a series of recursive binary classification steps, so as to subtly distinguish adjacent categories. Comprehensive experiments show the effectiveness of distinguishing adjacent categories for performance improvement and our new approach exceeds state-of-the-art performances in four different scenarios. Codes are available at https://github.com/wjh892521292/Ord2Seq. | 序数回归是将对象实例分类为序数类别的过程。它在许多场景中得到了广泛研究，如医学疾病分级和电影评分。已知的方法仅关注学习类间序数关系，但到目前为止仍存在无法区分相邻类别的限制。本文提出了一种简单的序列预测框架，用于序数回归，称为Ord2Seq，这是第一次将每个序数类别标签转换为特殊标签序列，从而将序数回归任务视为序列预测过程。通过这种方式，我们将序数回归任务分解为一系列递归二元分类步骤，以巧妙区分相邻类别。综合实验证明了区分相邻类别对性能改进的有效性，我们的新方法在四种不同场景中均超过了最先进的性能。代码可在https://github.com/wjh892521292/Ord2Seq 上找到。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Ord2Seq_Regarding_Ordinal_Regression_as_Label_Sequence_Prediction_ICCV_2023_paper.pdf) |
| 2023 | Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis | Minho Park, Jooyeol Yun, Seunghwan Choi, Jaegul Choo | Existing text-to-image generation approaches have set high standards for photorealism and text-image correspondence, largely benefiting from web-scale text-image datasets, which can include up to 5 billion pairs. However, text-to-image generation models trained on domain-specific datasets, such as urban scenes, medical images, and faces, still suffer from low text-image correspondence due to the lack of text-image pairs. Additionally, collecting billions of text-image pairs for a specific domain can be time-consuming and costly. Thus, ensuring high text-image correspondence without relying on web-scale text-image datasets remains a challenging task. In this paper, we present a novel approach for enhancing text-image correspondence by leveraging available semantic layouts. Specifically, we propose a Gaussian-categorical diffusion process that simultaneously generates both images and corresponding layout pairs. Our experiments reveal that we can guide text-to-image generation models to be aware of the semantics of different image regions, by training the model to generate semantic labels for each pixel. We demonstrate that our approach achieves higher text-image correspondence compared to existing text-to-image generation approaches in the Multi-Modal CelebA-HQ and the Cityscapes dataset, where text-image pairs are scarce. | 现有的文本到图像生成方法已经为逼真度和文本图像对应性设定了很高的标准，这在很大程度上得益于规模庞大的文本图像数据集，其中可以包含高达50亿对。然而，针对特定领域数据集（如城市场景、医学图像和人脸）进行训练的文本到图像生成模型，由于缺乏文本图像对，仍然存在低文本图像对应性的问题。此外，为特定领域收集数十亿个文本图像对可能耗时且昂贵。因此，在不依赖规模庞大的文本图像数据集的情况下确保高文本图像对应性仍然是一个具有挑战性的任务。在本文中，我们提出了一种通过利用可用的语义布局来增强文本图像对应性的新方法。具体来说，我们提出了一种高斯-分类扩散过程，同时生成图像和相应的布局对。我们的实验表明，通过训练模型为每个像素生成语义标签，我们可以引导文本到图像生成模型意识到不同图像区域的语义。我们证明，与现有的文本到图像生成方法相比，我们的方法在Multi-Modal CelebA-HQ和Cityscapes数据集中实现了更高的文本图像对应性，其中文本图像对稀缺。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Park_Learning_to_Generate_Semantic_Layouts_for_Higher_Text-Image_Correspondence_in_ICCV_2023_paper.pdf) |
| 2023 | Stochastic Segmentation with Conditional Categorical Diffusion Models | Lukas Zbinden, Lars Doorenbos, Theodoros Pissas, Adrian Thomas Huber, Raphael Sznitman, Pablo MÃ¡rquez-Neila | Semantic segmentation has made significant progress in recent years thanks to deep neural networks, but the common objective of generating a single segmentation output that accurately matches the image's content may not be suitable for safety-critical domains such as medical diagnostics and autonomous driving. Instead, multiple possible correct segmentation maps may be required to reflect the true distribution of annotation maps. In this context, stochastic semantic segmentation methods must learn to predict conditional distributions of labels given the image, but this is challenging due to the typically multimodal distributions, high-dimensional output spaces, and limited annotation data. To address these challenges, we propose a conditional categorical diffusion model (CCDM) for semantic segmentation based on Denoising Diffusion Probabilistic Models. Our model is conditioned to the input image, enabling it to generate multiple segmentation label maps that account for the aleatoric uncertainty arising from divergent ground truth annotations. Our experimental results show that CCDM achieves state-of-the-art performance on LIDC, a stochastic semantic segmentation dataset, and outperforms established baselines on the classical segmentation dataset Cityscapes. | 近年来，由于深度神经网络的发展，语义分割取得了显著进展，但生成一个准确匹配图像内容的单一分割输出的普遍目标可能不适用于医学诊断和自动驾驶等安全关键领域。相反，可能需要多个可能的正确分割地图来反映注释地图的真实分布。在这种情况下，随机语义分割方法必须学会预测在给定图像的条件下标签的条件分布，但由于通常是多模式分布、高维输出空间和有限的注释数据，这是具有挑战性的。为了解决这些挑战，我们提出了一种基于去噪扩散概率模型的条件分类扩散模型（CCDM）用于语义分割。我们的模型针对输入图像进行了条件化，使其能够生成多个分割标签地图，考虑到由于不同的基准真值注释而产生的随机不确定性。我们的实验结果表明，CCDM在LIDC上取得了最先进的性能，这是一个随机语义分割数据集，并且在经典分割数据集Cityscapes上优于已建立的基线。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Zbinden_Stochastic_Segmentation_with_Conditional_Categorical_Diffusion_Models_ICCV_2023_paper.pdf) |
| 2023 | Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models | Suhyeon Lee, Hyungjin Chung, Minyoung Park, Jonghyuk Park, Wi-Sun Ryu, Jong Chul Ye | Diffusion models have become a popular approach for image generation and reconstruction due to their numerous advantages. However, most diffusion-based inverse problem-solving methods only deal with 2D images, and even recently published 3D methods do not fully exploit the 3D distribution prior. To address this, we propose a novel approach using two perpendicular pre-trained 2D diffusion models to solve the 3D inverse problem. By modeling the 3D data distribution as a product of 2D distributions sliced in different directions, our method effectively addresses the curse of dimensionality. Our experimental results demonstrate that our method is highly effective for 3D medical image reconstruction tasks, including MRI Z-axis super-resolution, compressed sensing MRI, and sparse-view CT. Our method can generate high-quality voxel volumes suitable for medical applications. | 扩散模型已成为图像生成和重建的流行方法，因为其具有众多优势。然而，大多数基于扩散的逆问题求解方法仅处理2D图像，即使最近发布的3D方法也未充分利用3D分布先验。为解决这一问题，我们提出了一种新颖的方法，利用两个垂直预训练的2D扩散模型来解决3D逆问题。通过将3D数据分布建模为在不同方向切片的2D分布的乘积，我们的方法有效地解决了维度灾难。我们的实验结果表明，我们的方法对于3D医学图像重建任务非常有效，包括MRI Z轴超分辨率、压缩感知MRI和稀疏视图CT。我们的方法可以生成适用于医学应用的高质量体素体积。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Improving_3D_Imaging_with_Pre-Trained_Perpendicular_2D_Diffusion_Models_ICCV_2023_paper.pdf) |
| 2023 | Chasing Clouds: Differentiable Volumetric Rasterisation of Point Clouds as a Highly Efficient and Accurate Loss for Large-Scale Deformable 3D Registration | Mattias P. Heinrich, Alexander Bigalke, Christoph GroÃbrÃ¶hmer, Lasse Hansen | Learning-based registration for large-scale 3D point clouds has been shown to improve robustness and accuracy compared to classical methods and can be trained without supervision for locally rigid problems. However, for tasks with highly deformable structures, such as alignment of pulmonary vascular trees for medical diagnostics, previous approaches of self-supervision with regularisation and point distance losses have failed to succeed, leading to the need for complex synthetic augmentation strategies to obtain reliably strong supervision. In this work, we introduce a novel Differentiable Volumetric Rasterisation of point Clouds (DiVRoC) that overcomes those limitations and offers a highly efficient and accurate loss for large-scale deformable 3D registration. DiVRoC drastically reduces the computational complexity for measuring point cloud distances for high-resolution data with over 100k 3D points and can also be employed to extrapolate and regularise sparse motion fields, as loss in a self-training setting and as objective function in instance optimisation. DiVRoC can be successfully embedded into geometric registration networks, including PointPWC-Net and other graph CNNs. Our approach yields new state-of-the-art accuracy on the challenging PVT dataset in three different settings without training with manual ground truth: 1) unsupervised metric-based learning 2) self-supervised learning with pseudo labels generated by self-training and 3) optimisation based alignment without learning. https://github.com/mattiaspaul/ChasingClouds | 学习为大规模3D点云注册的研究表明，与传统方法相比，可以提高鲁棒性和准确性，并且可以在没有监督的情况下针对局部刚性问题进行训练。然而，对于高度可变结构的任务，例如医学诊断中肺血管树的对准，先前的自我监督方法与正则化和点距离损失相结合未能取得成功，导致需要复杂的合成增强策略来获得可靠的强监督。在这项工作中，我们引入了一种新颖的点云可微体积光栅化（DiVRoC），克服了这些限制，并为大规模可变形3D注册提供了高效准确的损失。DiVRoC极大地降低了测量高分辨率数据（超过100k个3D点）点云距离的计算复杂性，还可用于外推和规范稀疏运动场，作为自我训练设置中的损失以及实例优化中的目标函数。DiVRoC可以成功嵌入到几何注册网络中，包括PointPWC-Net和其他图卷积神经网络。我们的方法在具有挑战性的PVT数据集上以三种不同设置取得了新的最先进的准确性，而无需使用手动地面真值进行训练：1）无监督的基于度量的学习 2）通过自我训练生成的伪标签的自我监督学习和3）基于优化的对准而无需学习。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Heinrich_Chasing_Clouds_Differentiable_Volumetric_Rasterisation_of_Point_Clouds_as_a_ICCV_2023_paper.pdf) |
| 2023 | DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion | Zixiang Zhao, Haowen Bai, Yuanzhi Zhu, Jiangshe Zhang, Shuang Xu, Yulun Zhang, Kai Zhang, Deyu Meng, Radu Timofte, Luc Van Gool | Multi-modality image fusion aims to combine different modalities to produce fused images that retain the complementary features of each modality, such as functional highlights and texture details. To leverage strong generative priors and address challenges such as unstable training and lack of interpretability for GAN-based generative methods, we propose a novel fusion algorithm based on the denoising diffusion probabilistic model (DDPM). The fusion task is formulated as a conditional generation problem under the DDPM sampling framework, which is further divided into an unconditional generation subproblem and a maximum likelihood subproblem. The latter is modeled in a hierarchical Bayesian manner with latent variables and inferred by the expectation-maximization (EM) algorithm. By integrating the inference solution into the diffusion sampling iteration, our method can generate high-quality fused images with natural image generative priors and cross-modality information from source images. Note that all we required is an unconditional pre-trained generative model, and no fine-tuning is needed. Our extensive experiments indicate that our approach yields promising fusion results in infrared-visible image fusion and medical image fusion. The code is available at https://github.com/Zhaozixiang1228/MMIF-DDFM. | 多模态图像融合旨在将不同模态结合起来，以产生融合图像，保留每种模态的互补特征，如功能亮点和纹理细节。为了利用强大的生成先验并解决基于GAN的生成方法存在的训练不稳定和缺乏可解释性等挑战，我们提出了一种基于去噪扩散概率模型（DDPM）的新型融合算法。融合任务被构建为DDPM采样框架下的条件生成问题，进一步分为无条件生成子问题和最大似然子问题。后者以潜在变量的层次贝叶斯方式建模，并通过期望最大化（EM）算法推断。通过将推断解决方案集成到扩散采样迭代中，我们的方法可以生成具有自然图像生成先验和源图像的跨模态信息的高质量融合图像。值得注意的是，我们所需的只是一个无条件预训练的生成模型，不需要微调。我们广泛的实验证明，我们的方法在红外-可见图像融合和医学图像融合中产生了有希望的融合结果。代码可在https://github.com/Zhaozixiang1228/MMIF-DDFM获得。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_DDFM_Denoising_Diffusion_Model_for_Multi-Modality_Image_Fusion_ICCV_2023_paper.pdf) |
| 2023 | Towards Saner Deep Image Registration | Bin Duan, Ming Zhong, Yan Yan | With recent advances in computing hardware and surges of deep-learning architectures, learning-based deep image registration methods have surpassed their traditional counterparts, in terms of metric performance and inference time. However, these methods focus on improving performance measurements such as Dice, resulting in less attention given to model behaviors that are equally desirable for registrations, especially for medical imaging. This paper investigates these behaviors for popular learning-based deep registrations under a sanity-checking microscope. We find that most existing registrations suffer from low inverse consistency and nondiscrimination of identical pairs due to overly optimized image similarities. To rectify these behaviors, we propose a novel regularization-based sanity-enforcer method that imposes two sanity checks on the deep model to reduce its inverse consistency errors and increase its discriminative power simultaneously. Moreover, we derive a set of theoretical guarantees for our sanity-checked image registration method, with experimental results supporting our theoretical findings and their effectiveness in increasing the sanity of models without sacrificing any performance. | 随着计算硬件的最新进展和深度学习架构的激增，基于学习的深度图像配准方法在指标性能和推断时间方面已经超越了传统方法。然而，这些方法侧重于改善Dice等性能指标，导致对同样适用于配准的模型行为给予的关注较少，特别是对于医学成像。本文通过对流行的基于学习的深度配准方法进行理智检查微观研究了这些行为。我们发现大多数现有的配准方法由于过度优化的图像相似性而导致逆一致性较低和相同对的非鉴别性。为了纠正这些行为，我们提出了一种基于正则化的理智强化器方法，该方法对深度模型施加了两个理智检查，以同时减少其逆一致性错误并增加其鉴别能力。此外，我们为我们的理智检查图像配准方法提供了一套理论保证，实验证据支持我们的理论发现及其在提高模型的理智性而不牺牲任何性能方面的有效性。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Duan_Towards_Saner_Deep_Image_Registration_ICCV_2023_paper.pdf) |
| 2023 | Adversarial Bayesian Augmentation for Single-Source Domain Generalization | Sheng Cheng, Tejas Gokhale, Yezhou Yang | Generalizing to unseen image domains is a challenging problem primarily due to the lack of diverse training data, inaccessible target data, and the large domain shift that may exist in many real-world settings. As such data augmentation is a critical component of domain generalization methods that seek to address this problem. We present Adversarial Bayesian Augmentation (ABA), a novel algorithm that learns to generate image augmentations in the challenging single-source domain generalization setting. ABA draws on the strengths of adversarial learning and Bayesian neural networks to guide the generation of diverse data augmentations - these synthesized image domains aid the classifier in generalizing to unseen domains. We demonstrate the strength of ABA on several types of domain shift including style shift, subpopulation shift, and shift in the medical imaging setting. ABA outperforms all previous state-of-the-art methods, including pre-specified augmentations, pixel-based and convolutional-based augmentations. Code: https://github.com/shengcheng/ABA. | 在看不见的图像领域进行泛化是一个具有挑战性的问题，主要是因为缺乏多样化的训练数据、无法访问的目标数据以及在许多实际情况下可能存在的大量领域转移。因此，数据增强是旨在解决这一问题的领域泛化方法中的一个关键组成部分。我们提出了Adversarial Bayesian Augmentation（ABA），这是一种新颖的算法，它学习在具有挑战性的单源领域泛化设置中生成图像增强。ABA利用对抗学习和贝叶斯神经网络的优势来引导生成多样化的数据增强-这些合成图像领域有助于分类器在泛化到看不见的领域时表现良好。我们展示了ABA在几种类型的领域转移中的强大能力，包括风格转移、子群体转移以及医学图像设置中的转移。ABA优于所有先前的最先进方法，包括预先指定的增强、基于像素和基于卷积的增强。代码：https://github.com/shengcheng/ABA。 | [link](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Adversarial_Bayesian_Augmentation_for_Single-Source_Domain_Generalization_ICCV_2023_paper.pdf) |
