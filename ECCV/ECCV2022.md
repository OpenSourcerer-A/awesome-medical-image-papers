| 年份 | 题目 | 作者 | 摘要 | 中文摘要 | link |
| --- | --- | --- | --- | --- | --- |
| 2022 | TALISMAN: Targeted Active Learning for Object Detection with Rare Classes and Slices Using Submodular Mutual Information | Suraj Kothawade, Saikat Ghosh, Sumit Shekhar, Yu Xiang, Rishabh Iyer | "Deep neural networks based object detectors have shown great success in a variety of domains like autonomous vehicles, biomedical imaging, etc. It is known that their success depends on a large amount of data from the domain of interest. While deep models often perform well in terms of overall accuracy, they often struggle in performance on rare yet critical data slices. For example, data slices like ""motorcycle at night"" or ""bicycle at night"" are often rare but very critical slices for self-driving applications and false negatives on such rare slices could result in ill-fated failures and accidents. Active learning (AL) is a well-known paradigm to incrementally and adaptively build training datasets with a human in the loop. However, current AL based acquisition functions are not well-equipped to tackle real-world datasets with rare slices, since they are based on uncertainty scores or global descriptors of the image. We propose TALISMAN, a novel framework for Targeted Active Learning or object detectIon with rare slices using Submodular MutuAl iNformation. Our method uses the submodular mutual information functions instantiated using features of the region of interest (RoI) to efficiently target and acquire data points with rare slices. We evaluate our framework on the standard PASCAL VOC07+12 and BDD100K, a real-world self-driving dataset. We observe that TALISMAN outperforms other methods by in terms of average precision on rare slices, and in terms of mAP." | 基于深度神经网络的目标检测器在自动驾驶汽车、生物医学成像等领域取得了巨大成功。众所周知，它们的成功取决于来自感兴趣领域的大量数据。虽然深度模型在整体准确性方面通常表现良好，但在罕见但关键的数据片段上通常表现出困难。例如，“夜间摩托车”或“夜间自行车”等数据片段通常是自动驾驶应用中罕见但非常关键的片段，对这些罕见片段的误报可能导致不幸失败和事故。主动学习（AL）是一种众所周知的范式，用于在人类参与的情况下逐步和自适应地构建训练数据集。然而，当前基于AL的采集函数并不适合处理具有罕见片段的真实世界数据集，因为它们基于图像的不确定性分数或全局描述符。我们提出了TALISMAN，一种使用子模互信息进行目标主动学习或检测罕见片段的新框架。我们的方法利用感兴趣区域（RoI）的特征实例化的子模互信息函数，以有效地定位和获取具有罕见片段的数据点。我们在标准的PASCAL VOC07+12和BDD100K上评估了我们的框架，这是一个真实世界的自动驾驶数据集。我们观察到，TALISMAN在罕见片段的平均精度和mAP方面优于其他方法。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136980001.pdf) |
| 2022 | Uncertainty-Aware Multi-modal Learning via Cross-Modal Random Network Prediction | Hu Wang, Jianpeng Zhang, Yuanhong Chen, Congbo Ma, Jodie Avery, Louise Hull, Gustavo Carneiro | "Multi-modal learning focuses on training models by equally combining multiple input data modalities during the prediction process. However, this equal combination can be detrimental to the prediction accuracy because different modalities are usually accompanied by varying levels of uncertainty. Using such uncertainty to combine modalities has been studied by a couple of approaches, but with limited success because these approaches are either designed to deal with specific classification or segmentation problems and cannot be easily translated into other tasks, or suffer from numerical instabilities. In this paper, we propose a new Uncertainty-aware Multi-modal Learner that estimates uncertainty by measuring feature density via Cross-modal Random Network Prediction (CRNP). CRNP is designed to require little adaptation to translate between different prediction tasks, while having a stable training process. From a technical point of view, CRNP is the first approach to explore random network prediction to estimate uncertainty and to combine multi-modal data. Experiments on two 3D multi-modal medical image segmentation tasks and three 2D multi-modal computer vision classification tasks show the effectiveness, adaptability and robustness of CRNP. Also, we provide an extensive discussion on different fusion functions and visualization to validate the proposed model." | 多模态学习专注于在预测过程中通过平等组合多个输入数据模态来训练模型。然而，这种平等组合可能对预测准确性产生不利影响，因为不同的模态通常伴随着不同程度的不确定性。利用这种不确定性来组合模态已经被一些方法研究过，但由于这些方法要么设计用于处理特定的分类或分割问题，不能轻易转化为其他任务，要么受到数值不稳定性的影响，因此取得了有限的成功。在本文中，我们提出了一种新的基于不确定性的多模态学习器，通过通过跨模态随机网络预测（CRNP）来测量特征密度来估计不确定性。CRNP被设计为需要很少的调整就能在不同的预测任务之间转换，同时具有稳定的训练过程。从技术角度来看，CRNP是第一种探索随机网络预测来估计不确定性和组合多模态数据的方法。在两个3D多模态医学图像分割任务和三个2D多模态计算机视觉分类任务上的实验显示了CRNP的有效性、适应性和稳健性。此外，我们提供了对不同融合函数和可视化的广泛讨论，以验证所提出的模型。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136970195.pdf) |
| 2022 | Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing | Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, Hoifung Poon, Ozan Oktay | "Multi-modal data abounds in biomedicine, such as radiology images and reports. Interpreting this data at scale is essential for improving clinical care and accelerating clinical research. Biomedical text with its complex semantics poses additional challenges in vision-language modelling compared to the general domain, and previous work has used insufficiently adapted models that lack domain-specific language understanding. In this paper, we show that principled textual semantic modelling can substantially improve contrastive learning in self-supervised vision-language processing. We release a language model that achieves state-of-the-art results in radiology natural-language inference through its improved vocabulary and novel language pretraining objective leveraging semantics and discourse characteristics in radiology reports. Further, we propose a self-supervised joint vision-language approach with a focus on better text modelling. It establishes new state of the art results on a wide range of publicly available benchmarks, in part by leveraging our new domain-specific language model. We release a new dataset with locally-aligned phrase grounding annotations by radiologists to facilitate the study of complex semantic modelling in biomedical vision-language processing. A broad evaluation, including on this new dataset, shows that our contrastive learning approach, aided by textual-semantic modelling, outperforms prior methods in segmentation tasks, despite only using a global-alignment objective." | 生物医学领域存在大量的多模态数据，如放射学图像和报告。在规模上解释这些数据对于改善临床护理和加速临床研究至关重要。与一般领域相比，生物医学文本具有复杂的语义，对视觉语言建模提出了额外的挑战，先前的研究使用了缺乏领域特定语言理解的模型。在本文中，我们展示了基于原则的文本语义建模可以显著改善自监督视觉语言处理中的对比学习。我们发布了一个语言模型，通过改进的词汇表和利用放射学报告中的语义和话语特征的新颖语言预训练目标，实现了在放射学自然语言推理方面的最新结果。此外，我们提出了一种自监督联合视觉语言方法，重点放在更好的文本建模上。通过利用我们的新领域特定语言模型，在一系列公开可用的基准测试中取得了最新的成果。我们发布了一个新的数据集，其中包含放射科医生提供的本地对齐短语定位注释，以促进对生物医学视觉语言处理中复杂语义建模的研究。广泛的评估，包括对这个新数据集的评估，显示我们的对比学习方法，在分割任务中表现优于先前的方法，尽管只使用了一个全局对齐目标。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960001.pdf) |
| 2022 | Generalized Brain Image Synthesis with Transferable Convolutional Sparse Coding Networks | Yawen Huang, Feng Zheng, Xu Sun, Yuexiang Li, Ling Shao, Yefeng Zheng | "High inter-equipment variability and expensive examination costs of brain imaging remain key challenges in leveraging the heterogeneous scans effectively. Despite rapid growth in image-to-image translation with deep learning models, the target brain data may not always be achievable due to the specific attributes of brain imaging. In this paper, we present a novel generalized brain image synthesis method, powered by our transferable convolutional sparse coding networks, to address the lack of interpretable cross-modal medical image representation learning. The proposed approach masters the ability to imitate the machine-like anatomically meaningful imaging by translating features directly under a series of mathematical processings, leading to the reduced domain discrepancy while enhancing model transferability. Specifically, we first embed the globally normalized features into a domain discrepancy metric to learn the domain-invariant representations, then optimally preserve domain-specific geometrical property to reflect the intrinsic graph structures, and further penalize their subspace mismatching to reduce the generalization error. The overall framework is cast in a minimax setting, and the extensive experiments show that the proposed method yields state-of-the-art results on multiple datasets." | 脑部成像的高设备间变异性和昂贵的检查成本仍然是有效利用异质扫描的关键挑战。尽管深度学习模型在图像之间的转换中迅速增长，但由于脑部成像的特定属性，目标脑部数据并不总是可实现的。在本文中，我们提出了一种新颖的通用脑部图像合成方法，由我们可转移的卷积稀疏编码网络驱动，以解决缺乏可解释的跨模态医学图像表示学习的问题。所提出的方法掌握了通过直接在一系列数学处理下转换特征来模仿机器式解剖学意义成像的能力，从而减少域差异性，同时增强模型的可转移性。具体而言，我们首先将全局归一化特征嵌入到域差异度度量中，学习域不变表示，然后最佳地保留域特定的几何属性以反映内在的图形结构，并进一步惩罚它们的子空间不匹配以减少泛化误差。整体框架设定在极小极大设置中，广泛的实验表明，所提出的方法在多个数据集上取得了最先进的结果。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136940178.pdf) |
| 2022 | Natural Synthetic Anomalies for Self-Supervised Anomaly Detection and Localization | Hannah M. SchlÃ¼ter, Jeremy Tan, Benjamin Hou, Bernhard Kainz | "We introduce a simple and intuitive self-supervision task, Natural Synthetic Anomalies (NSA), for training an end-to-end model for anomaly detection and localization using only normal training data. NSA integrates Poisson image editing to seamlessly blend scaled patches of various sizes from separate images. This creates a wide range of synthetic anomalies which are more similar to natural sub-image irregularities than previous data-augmentation strategies for self-supervised anomaly detection. We evaluate the proposed method using natural and medical images. Our experiments with the MVTec AD dataset show that a model trained to localize NSA anomalies generalizes well to detecting real-world a priori unknown types of manufacturing defects. Our method achieves an overall detection AUROC of 97.2 outperforming all previous methods that learn without the use of additional datasets. Code available at https://github.com/hmsch/natural-synthetic-anomalies." | 我们引入了一种简单直观的自监督任务，即自然合成异常（NSA），用于仅使用正常训练数据训练端到端模型进行异常检测和定位。NSA集成了泊松图像编辑，无缝地融合来自不同图像的各种大小的缩放补丁。这创建了一系列更类似于自然子图像不规则性的合成异常，而不是以前用于自监督异常检测的数据增强策略。我们使用自然和医学图像评估了所提出的方法。我们在MVTec AD数据集上的实验表明，训练定位NSA异常的模型很好地泛化到检测真实世界先验未知类型的制造缺陷。我们的方法实现了97.2的总体检测AUROC，优于所有以前在没有使用额外数据集的情况下学习的方法。代码可在https://github.com/hmsch/natural-synthetic-anomalies找到。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910459.pdf) |
| 2022 | DiffuseMorph: Unsupervised Deformable Image Registration Using Diffusion Model | Boah Kim, Inhwa Han, Jong Chul Ye | "Deformable image registration is one of the fundamental tasks in medical imaging. Classical registration algorithms usually require a high computational cost for iterative optimizations. Although deep-learning-based methods have been developed for fast image registration, it is still challenging to obtain realistic continuous deformations from a moving image to a fixed image with less topological folding problem. To address this, here we present a novel diffusion-model-based image registration method, called DiffuseMorph. DiffuseMorph not only generates synthetic deformed images through reverse diffusion but also allows image registration by deformation fields. Specifically, the deformation fields are generated by the conditional score function of the deformation between the moving and fixed images, so that the registration can be performed from continuous deformation by simply scaling the latent feature of the score. Experimental results on 2D facial and 3D medical image registration tasks demonstrate that our method provides flexible deformations with topology preservation capability." | 可变形图像配准是医学影像中的基本任务之一。传统的配准算法通常需要高计算成本进行迭代优化。尽管已经开发了基于深度学习的快速图像配准方法，但仍然具有从移动图像到固定图像获取真实连续变形并减少拓扑折叠问题的挑战。为了解决这个问题，我们提出了一种基于扩散模型的图像配准方法，称为DiffuseMorph。DiffuseMorph不仅通过反向扩散生成合成变形图像，还允许通过变形场进行图像配准。具体来说，变形场是由移动图像和固定图像之间变形的条件评分函数生成的，因此可以通过简单缩放评分的潜在特征来执行连续变形的配准。在2D面部和3D医学图像配准任务上的实验结果表明，我们的方法提供具有拓扑保留能力的灵活变形。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910336.pdf) |
| 2022 | Learning Topological Interactions for Multi-Class Medical Image Segmentation | Saumya Gupta, Xiaoling Hu, James Kaan, Michael Jin, Mutshipay Mpoy, Katherine Chung, Gagandeep Singh, Mary Saltz, Tahsin Kurc, Joel Saltz, Apostolos Tassiopoulos, Prateek Prasanna, Chao Chen | "Deep learning methods have achieved impressive performance for multi-class medical image segmentation. However, they are limited in their ability to encode topological interactions among different classes (e.g., containment and exclusion). These constraints naturally arise in biomedical images and can be crucial in improving segmentation quality. In this paper, we introduce a novel topological interaction module to encode the topological interactions into a deep neural network. The implementation is completely convolution-based and thus can be very efficient. This empowers us to incorporate the constraints into end-to-end training and enrich the feature representation of neural networks. The efficacy of the proposed method is validated on different types of interactions. We also demonstrate the generalizability of the method on both proprietary and public challenge datasets, in both 2D and 3D settings, as well as across different modalities such as CT and Ultrasound. Code is available at: https://github.com/TopoXLab/TopoInteraction" | 深度学习方法在多类医学图像分割方面取得了令人印象深刻的性能。然而，它们在编码不同类别之间的拓扑相互作用（例如包含和排斥）方面存在局限性。这些约束在生物医学图像中自然产生，并且可以对改善分割质量至关重要。本文介绍了一种新颖的拓扑相互作用模块，用于将拓扑相互作用编码到深度神经网络中。实现完全基于卷积，因此非常高效。这使我们能够将约束集成到端到端的训练中，并丰富神经网络的特征表示。所提出方法的有效性已在不同类型的相互作用上得到验证。我们还展示了该方法在专有和公共挑战数据集上的泛化能力，无论是在2D还是3D设置中，以及跨不同模态，如CT和超声波。代码可在以下链接获得：https://github.com/TopoXLab/TopoInteraction。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136890691.pdf) |
| 2022 | Joint Learning of Localized Representations from Medical Images and Reports | Philip MÃ¼ller, Georgios Kaissis, Congyu Zou, Daniel Rueckert | "Contrastive learning has proven effective for pre-training image models on unlabeled data with promising results for tasks such as medical image classification. Using paired text (like radiological reports) during pre-training improves the results even further. Still, most existing methods target image classification downstream tasks and may not be optimal for localized tasks like semantic segmentation or object detection. We therefore propose Localized representation learning from Vision and Text (LoVT), a text-supervised pre-training method that explicitly targets localized medical imaging tasks. Our method combines instance-level image-report contrastive learning with local contrastive learning on image region and report sentence representations. We evaluate LoVT and commonly used pre-training methods on an evaluation framework of 18 localized tasks on chest X-rays from five public datasets. LoVT performs best on 10 of the 18 studied tasks making it the preferred method of choice for localized tasks." | 对比学习已被证明对于在未标记数据上预训练图像模型非常有效，对于诸如医学图像分类之类的任务表现出有希望的结果。在预训练过程中使用成对文本（如放射学报告）可以进一步改善结果。然而，大多数现有方法都针对图像分类下游任务，对于像语义分割或目标检测这样的定位任务可能不是最佳选择。因此，我们提出了一种名为来自视觉和文本的局部表示学习（LoVT）的文本监督预训练方法，明确针对定位的医学成像任务。我们的方法结合了实例级图像-报告对比学习和对图像区域和报告句子表示的局部对比学习。我们在来自五个公共数据集的胸部X射线图像上的18个定位任务的评估框架上评估了LoVT和常用的预训练方法。LoVT在18项研究任务中的10项中表现最佳，使其成为定位任务的首选方法。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860670.pdf) |
| 2022 | Visual Knowledge Tracing | Neehar Kondapaneni, Pietro Perona, Oisin Mac Aodha | "Each year, thousands of people learn new visual categorization tasks - radiologists learn to recognize tumors, birdwatchers learn to distinguish similar species, and crowd workers learn how to annotate valuable data for applications like autonomous driving. As humans learn, their brain updates the visual features it extracts and attend to, which ultimately informs their final classification decisions. In this work, we propose a novel task of tracing the evolving classification behavior of human learners as they engage in challenging visual classification tasks. We propose models that jointly extract the visual features used by learners as well as predicting the classification functions they utilize. We collect three challenging new datasets from real human learners in order to evaluate the performance of different visual knowledge tracing methods. Our results show that our recurrent models are able to predict the classification behavior of human learners on three challenging medical image and species identification tasks." | 每年，成千上万的人学习新的视觉分类任务 - 放射科医生学习识别肿瘤，鸟类观察者学习区分相似物种，群体工作者学习如何为自动驾驶等应用程序注释有价值的数据。随着人类学习，他们的大脑更新提取和关注的视觉特征，最终影响其最终的分类决策。在这项工作中，我们提出了一个新颖的任务，追踪人类学习者在参与具有挑战性的视觉分类任务时演变的分类行为。我们提出了模型，既提取学习者使用的视觉特征，又预测他们使用的分类函数。我们收集了三个具有挑战性的新数据集，以评估不同视觉知识追踪方法的性能。我们的结果表明，我们的循环模型能够预测人类学习者在三个具有挑战性的医学图像和物种识别任务上的分类行为。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850410.pdf) |
| 2022 | RadioTransformer: A Cascaded Global-Focal Transformer for Visual Attention-Guided Disease Classification | Moinak Bhattacharya, Shubham Jain, Prateek Prasanna | "In this work, we present RadioTransformer, a novel visual attention-driven transformer framework, that leverages radiologistsâ gaze patterns and models their visuo-cognitive behavior for disease diagnosis on chest radiographs. Domain experts, such as radiologists, rely on visual information for medical image interpretation. On the other hand, deep neural networks have demonstrated significant promise in similar tasks even where visual interpretation is challenging. Eye-gaze tracking has been used to capture the viewing behavior of domain experts, lending insights into the complexity of visual search. However, deep learning frameworks, even those that rely on attention mechanisms, do not leverage this rich domain information. RadioTransformer fills this critical gap by learning from radiologistsâ visual search patterns, encoded as âhuman visual attention regionsâ in a cascaded global-focal transformer framework. The overall âglobalâ image characteristics and the more detailed âlocalâ features are captured by the proposed global and focal modules, respectively. We experimentally validate the efficacy of our student-teacher approach for 8 datasets involving different disease classification tasks where eye-gaze data is not available during the inference phase. Code: https://github.com/bmi-imaginelab/radiotransformer." | 在这项工作中，我们提出了RadioTransformer，这是一个新颖的视觉注意力驱动的变压器框架，利用放射科医生的凝视模式并模拟他们的视觉认知行为，用于胸部X射线疾病诊断。领域专家，如放射科医生，依赖于视觉信息进行医学图像解释。另一方面，深度神经网络在类似任务中展现出了显著的潜力，即使在视觉解释具有挑战性的情况下也是如此。眼球注视跟踪已被用于捕捉领域专家的观看行为，提供了对视觉搜索复杂性的见解。然而，即使依赖于注意力机制的深度学习框架也没有利用这些丰富的领域信息。RadioTransformer通过在级联全局-焦点变压器框架中学习放射科医生的视觉搜索模式（编码为“人类视觉注意力区域”）来填补这一关键差距。所提出的全局和焦点模块分别捕获整体“全局”图像特征和更详细的“局部”特征。我们通过实验证实了我们的学生-教师方法在涉及不同疾病分类任务的8个数据集上的有效性，其中在推断阶段不提供眼球注视数据。代码：https://github.com/bmi-imaginelab/radiotransformer。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810669.pdf) |
| 2022 | Accurate Detection of Proteins in Cryo-Electron Tomograms from Sparse Labels | Qinwen Huang, Ye Zhou, Hsuan-Fu Liu, Alberto Bartesaghi | "Cryo-electron tomography (CET) combined with sub-volume averaging (SVA), is currently the only imaging technique capable of determining the structure of proteins imaged inside cells at molecular resolution. To obtain high-resolution reconstructions, sub-volumes containing randomly distributed copies of the protein of interest need be identified, extracted and subjected to SVA, making accurate particle detection a critical step in the CET processing pipeline. Classical template-based methods have high false-positive rates due to the very low signal-to-noise ratios (SNR) typical of CET volumes, while more recent neural-network based detection algorithms require extensive labeling, are very slow to train and can take days to run. To address these issues, we propose a novel particle detection framework that uses positive-unlabeled learning and exploits the unique properties of 3D tomograms to improve detection performance. Our end-to-end framework is able to identify particles within minutes when trained using a single partially labeled tomogram. We conducted extensive validation experiments on two challenging CET datasets representing different experimental conditions, and observed more than 10% improvement in mAP and F1 scores compared to existing particle picking methods used in CET. Ultimately, the proposed framework will facilitate the structural analysis of challenging biomedical targets imaged within the native environment of cells." | 冷冻电子断层扫描（Cryo-electron tomography，CET）结合亚体积平均（sub-volume averaging，SVA）是目前唯一能够在细胞内分子分辨率下确定蛋白质结构的成像技术。为了获得高分辨率重建，需要识别、提取和经过SVA处理包含感兴趣蛋白质随机分布拷贝的亚体积，因此准确的颗粒检测是CET处理流程中的关键步骤。传统的基于模板的方法由于CET体积的信噪比非常低，具有较高的误报率，而最近基于神经网络的检测算法需要大量标记，训练速度很慢，可能需要几天才能运行。为了解决这些问题，我们提出了一种新颖的颗粒检测框架，利用正负样本学习，并利用3D断层扫描的独特特性来提高检测性能。我们的端到端框架能够在使用单个部分标记的断层扫描进行训练时在几分钟内识别颗粒。我们在两个具有挑战性的CET数据集上进行了大量验证实验，这些数据集代表不同的实验条件，与CET中使用的现有颗粒拾取方法相比，观察到mAP和F1得分提高了超过10%。最终，该框架将促进在细胞的自然环境中成像的具有挑战性的生物医学目标的结构分析。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810636.pdf) |
| 2022 | K-SALSA: K-Anonymous Synthetic Averaging of Retinal Images via Local Style Alignment | Minkyu Jeon, Hyeonjin Park, Hyunwoo J. Kim, Michael Morley, Hyunghoon Cho | "The application of modern machine learning to retinal image analyses offers valuable insights into a broad range of human health conditions beyond ophthalmic diseases. Additionally, data sharing is key to fully realizing the potential of machine learning models by providing a rich and diverse collection of training data. However, the personally-identifying nature of retinal images, encompassing the unique vascular structure of each individual, often prevents this data from being shared openly. While prior works have explored image de-identification strategies based on synthetic averaging of images in other domains (e.g. facial images), existing techniques face difficulty in preserving both privacy and clinical utility in retinal images, as we demonstrate in our work. We therefore introduce k-SALSA, a generative adversarial network (GAN)-based framework for synthesizing retinal fundus images that summarize a given private dataset while satisfying the privacy notion of k-anonymity. k-SALSA brings together state-of-the-art techniques for training and inverting GANs to achieve practical performance on retinal images. Furthermore, k-SALSA leverages a new technique, called local style alignment, to generate a synthetic average that maximizes the retention of fine-grain visual patterns in the source images, thus improving the clinical utility of the generated images. On two benchmark datasets of diabetic retinopathy (EyePACS and APTOS), we demonstrate our improvement upon existing methods with respect to image fidelity, classification performance, and mitigation of membership inference attacks. Our work represents a step toward broader sharing of retinal images for scientific collaboration. Keywords : Medical image privacy, k-anonymity, generative adversarial networks, fundus imaging, synthetic data generation, style transfer" | 现代机器学习在视网膜图像分析中的应用为人类健康状况提供了宝贵的见解，超越了眼科疾病范围。此外，数据共享是充分发挥机器学习模型潜力的关键，通过提供丰富多样的训练数据。然而，视网膜图像具有个人身份特征，包括每个个体的独特血管结构，这经常阻止数据被公开共享。尽管先前的研究已经探索了基于合成图像平均化的图像去识别策略（例如面部图像），但现有技术在保护隐私和临床效用方面在视网膜图像上面临困难，正如我们在工作中所展示的。因此，我们引入了k-SALSA，这是一个基于生成对抗网络（GAN）的框架，用于合成概括给定私有数据集的视网膜底部图像，同时满足k-匿名的隐私概念。k-SALSA结合了用于训练和反演GAN的最新技术，以在视网膜图像上实现实际性能。此外，k-SALSA利用一种新技术，称为局部风格对齐，生成一个合成平均值，最大限度地保留源图像中的细粒度视觉模式，从而提高生成图像的临床效用。在两个糖尿病视网膜病变基准数据集（EyePACS和APTOS）上，我们展示了相对于现有方法在图像保真度、分类性能和减轻成员推断攻击方面的改进。我们的工作代表了在科学合作中更广泛共享视网膜图像的一步。关键词：医学图像隐私、k-匿名、生成对抗网络、底部成像、合成数据生成、风格转移。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810652.pdf) |
| 2022 | UniMiSS: Universal Medical Self-Supervised Learning via Breaking Dimensionality Barrier | Yutong Xie, Jianpeng Zhang, Yong Xia, Qi Wu | "Self-supervised learning (SSL) opens up huge opportunities for medical image analysis that is well known for its lack of annotations. However, aggregating massive (unlabeled) 3D medical images like computerized tomography (CT) remains challenging due to its high imaging cost and privacy restrictions. In this paper, we advocate bringing a wealth of 2D images like chest X-rays as compensation for the lack of 3D data, aiming to build a universal medical self-supervised representation learning framework, called UniMiSS. The following problem is how to break the dimensionality barrier, i.e., making it possible to perform SSL with both 2D and 3D images? To achieve this, we design a pyramid U-like medical Transformer (MiT). It is composed of the switchable patch embedding (SPE) module and Transformers. The SPE module adaptively switches to either 2D or 3D patch embedding, depending on the input dimension. The embedded patches are converted into a sequence regardless of their original dimensions. The Transformers model the long-term dependencies in a sequence-to-sequence manner, thus enabling UniMiSS to learn representations from both 2D and 3D images. With the MiT as the backbone, we perform the UniMiSS in a self-distillation manner. We conduct expensive experiments on six 3D/2D medical image analysis tasks, including segmentation and classification. The results show that the proposed UniMiSS achieves promising performance on various downstream tasks, outperforming the ImageNet pre-training and other advanced SSL counterparts substantially. Code is available at https://github.com/YtongXie/UniMiSS-code." | 自监督学习（SSL）为医学图像分析开辟了巨大机遇，因其缺乏标注而闻名。然而，聚合大量（未标记的）三维医学图像，如计算机断层扫描（CT），由于其高成像成本和隐私限制，仍然具有挑战性。在本文中，我们主张利用大量的二维图像，如胸部X射线，作为缺乏三维数据的补偿，旨在构建一个通用的医学自监督表示学习框架，称为UniMiSS。接下来的问题是如何打破维度壁垒，即如何使得同时使用二维和三维图像进行SSL成为可能？为了实现这一目标，我们设计了金字塔状U形医学Transformer（MiT）。它由可切换的补丁嵌入（SPE）模块和Transformer组成。SPE模块根据输入维度自适应地切换到二维或三维补丁嵌入。嵌入的补丁被转换为一个序列，无论其原始维度如何。Transformer以序列到序列方式建模序列中的长期依赖性，从而使UniMiSS能够从二维和三维图像中学习表示。以MiT作为骨干，我们以自蒸馏的方式进行UniMiSS。我们对包括分割和分类在内的六个三维/二维医学图像分析任务进行了昂贵的实验。结果显示，所提出的UniMiSS在各种下游任务上取得了令人期待的性能，显著优于ImageNet预训练和其他先进的SSL对手。代码可在https://github.com/YtongXie/UniMiSS-code找到。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810551.pdf) |
| 2022 | Med-DANet: Dynamic Architecture Network for Efficient Medical Volumetric Segmentation | Wenxuan Wang, Chen Chen, Jing Wang, Sen Zha, Yan Zhang, Jiangyun Li | "For 3D medical image (e.g. CT and MRI) segmentation, the difficulty of segmenting each slice in a clinical case varies greatly. Previous research on volumetric medical image segmentation in a slice-by-slice manner conventionally use the identical 2D deep neural network to segment all the slices of the same case, ignoring the data heterogeneity among image slices. In this paper, we focus on multi-modal 3D MRI brain tumor segmentation and propose a dynamic architecture network named Med-DANet based on adaptive model selection to achieve effective accuracy and efficiency trade-off. For each slice of the input 3D MRI volume, our proposed method learns a slice-specific decision by the Decision Network to dynamically select a suitable model from the predefined Model Bank for the subsequent 2D segmentation task. Extensive experimental results on both BraTS 2019 and 2020 datasets show that our proposed method achieves comparable or better results than previous state-of-the-art methods for 3D MRI brain tumor segmentation with much less model complexity. Compared with the state-of-the-art 3D method TransBTS, the proposed framework improves the model efficiency by up to 3.5x without sacrificing the accuracy. Our code will be publicly available at https://github.com/Wenxuan-1119/Med-DANet." | 对于3D医学图像（例如CT和MRI）分割，临床案例中每个切片的分割难度差异很大。先前关于体积医学图像分割的研究通常采用相同的2D深度神经网络逐片分割同一案例的所有切片，忽略了图像切片之间的数据异质性。本文专注于多模态3D MRI脑肿瘤分割，并提出了一种基于自适应模型选择的动态架构网络Med-DANet，以实现准确性和效率之间的有效权衡。对于输入的3D MRI体积的每个切片，我们提出的方法通过决策网络学习切片特定的决策，动态选择适合的模型来执行后续的2D分割任务。在BraTS 2019和2020数据集上进行的大量实验结果显示，我们提出的方法在3D MRI脑肿瘤分割方面的效果与先前最先进的方法相当或更好，且模型复杂度大大降低。与最先进的3D方法TransBTS相比，所提出的框架提高了模型效率最多达到3.5倍，而不会牺牲准确性。我们的代码将公开在https://github.com/Wenxuan-1119/Med-DANet。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810499.pdf) |
| 2022 | One-Shot Medical Landmark Localization by Edge-Guided Transform and Noisy Landmark Refinement | Zihao Yin, Ping Gong, Chunyu Wang, Yizhou Yu, Yizhou Wang | "As an important upstream task for many medical applications, supervised landmark localization still requires non-negligible annotation costs to achieve desirable performance. Besides, due to cumbersome collection procedures, the limited size of medical landmark datasets impacts the effectiveness of large-scale self-supervised pre-training methods. To address these challenges, we propose a two-stage framework for one-shot medical landmark localization, which first infers landmarks by unsupervised registration from the labeled exemplar to unlabeled targets, and then utilizes these noisy pseudo labels to train robust detectors. To handle the significant structure variations, we learn an end-to-end cascade of global alignment and local deformations, under the guidance of novel loss functions which incorporate edge information. In stage \uppercase\expandafter{\romannumeral2}, we explore self-consistency for selecting reliable pseudo labels and cross-consistency for semi-supervised learning. Our method achieves state-of-the-art performances on public datasets of different body parts, which demonstrates its general applicability. Code will be publicly available." | 作为许多医疗应用程序中的重要上游任务，监督式地标定位仍然需要不可忽视的注释成本才能实现理想的性能。此外，由于繁琐的收集程序，医学地标数据集的有限规模影响了大规模自监督预训练方法的有效性。为了解决这些挑战，我们提出了一个用于一次性医学地标定位的两阶段框架，首先通过从标记的样本到未标记目标的无监督注册推断地标，然后利用这些嘈杂的伪标签训练稳健的检测器。为了处理显著的结构变化，我们学习端到端的全局对齐和局部变形级联，在新颖的损失函数的指导下，这些损失函数融合了边缘信息。在第二阶段，我们探索自一致性以选择可靠的伪标签和交叉一致性以进行半监督学习。我们的方法在不同身体部位的公共数据集上实现了最先进的性能，证明了其普适性。代码将公开提供。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810466.pdf) |
| 2022 | Personalizing Federated Medical Image Segmentation via Local Calibration | Jiacheng Wang, Yueming Jin, Liansheng Wang | "Medical image segmentation under federated learning (FL) is a promising direction by allowing multiple clinical sites to collaboratively learn a global model without centralizing datasets. However, using a single model to adapt to various data distributions from different sites is extremely challenging. Personalized FL tackles this issue by only utilizing partial model parameters shared from global server, while keeping the rest to adapt to its own data distribution in the local training of each site. However, most existing methods concentrate on the partial parameter splitting, while do not consider the inter-site in-consistencies during the local training, which in fact can facilitate the knowledge communication over sites to benefit the model learning for improving the local accuracy. In this paper, we propose a personalized federated framework with Local Calibration (LC-Fed), to leverage the inter-site in-consistencies in both feature- and prediction- levels to boost the segmentation. Concretely, as each local site has its alternative attention on the various features, we first design the contrastive site embedding coupled with channel selection operation to calibrate the encoded features. Moreover, we propose to exploit the knowledge of prediction-level in-consistency to guide the personalized modeling on the ambiguous regions, e.g., anatomical boundaries. It is achieved by computing a disagreement-aware map to calibrate the prediction. Effectiveness of our method has been verified on three medical image segmentation tasks with different modalities, where our method consistently shows superior performance to the state-of-the-art personalized FL methods. Code is available at https://github.com/jcwang123/FedLC." | 在联邦学习（FL）下的医学图像分割是一个有前途的方向，它允许多个临床站点协作学习全局模型，而无需将数据集集中化。然而，使用单一模型来适应来自不同站点的各种数据分布是极具挑战性的。个性化FL通过仅利用从全局服务器共享的部分模型参数来解决这个问题，同时保留其余部分以适应本地训练中的数据分布。然而，大多数现有方法集中在部分参数分割上，而不考虑本地训练过程中站点间的不一致性，而实际上这种不一致性可以促进站点间的知识传播，有利于提高模型学习以改善本地准确性。在本文中，我们提出了一个具有本地校准的个性化联邦框架（LC-Fed），以利用特征和预测水平上的站点间不一致性来促进分割。具体来说，由于每个本地站点对各种特征具有其替代注意力，我们首先设计了对比站点嵌入，结合通道选择操作来校准编码特征。此外，我们提出利用预测水平上的不一致性知识来指导对模糊区域（例如解剖边界）的个性化建模。通过计算一个不一致感知地图来校准预测。我们的方法在三个具有不同模态的医学图像分割任务上的有效性得到验证，其中我们的方法始终表现出优于最先进的个性化FL方法的性能。代码可在https://github.com/jcwang123/FedLC上找到。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810449.pdf) |
| 2022 | Auto-FedRL: Federated Hyperparameter Optimization for Multi-Institutional Medical Image Segmentation | Pengfei Guo, Dong Yang, Ali Hatamizadeh, An Xu, Ziyue Xu, Wenqi Li, Can Zhao, Daguang Xu, Stephanie Harmon, Evrim Turkbey, Baris Turkbey, Bradford Wood, Francesca Patella, Elvira Stellato, Gianpaolo Carrafiello, Vishal M. Patel, Holger R. Roth | "Federated learning (FL) is a distributed machine learning technique that enables collaborative model training while avoiding explicit data sharing. The inherent privacy-preserving property of FL algorithms makes them especially attractive to the medical field. However, in case of heterogeneous client data distributions, standard FL methods are unstable and require intensive hyperparameter tuning to achieve optimal performance. Conventional hyperparameter optimization algorithms are impractical in real-world FL applications as they involve numerous training trials, which are often not affordable with limited compute budgets. In this work, we propose an efficient reinforcement learning (RL)-based federated hyperparameter optimization algorithm, termed Auto-FedRL, in which an online RL agent can dynamically adjust hyperparameters of each client based on the current training progress. Extensive experiments are conducted to investigate different search strategies and RL agents. The effectiveness of the proposed method is validated on a heterogeneous data split of the CIFAR-10 dataset as well as two real-world medical image segmentation datasets for COVID-19 lesion segmentation in chest CT and pancreas segmentation in abdominal CT.." | 联邦学习（FL）是一种分布式机器学习技术，可以实现协作模型训练，同时避免显式数据共享。FL算法固有的隐私保护属性使其在医疗领域尤为吸引人。然而，在客户数据分布异构的情况下，标准FL方法不稳定，需要进行密集的超参数调整才能实现最佳性能。传统的超参数优化算法在现实世界的FL应用中是不切实际的，因为涉及大量的训练试验，这在有限的计算预算下通常无法承受。在这项工作中，我们提出了一种高效的基于强化学习（RL）的联邦超参数优化算法，称为Auto-FedRL，在这种算法中，一个在线RL代理可以根据当前的训练进展动态调整每个客户端的超参数。进行了大量实验来研究不同的搜索策略和RL代理。所提出的方法在CIFAR-10数据集的异构数据拆分以及两个真实世界医学图像分割数据集（胸部CT中COVID-19病变分割和腹部CT中胰腺分割）上的有效性得到了验证。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810431.pdf) |
| 2022 | Generalizable Medical Image Segmentation via Random Amplitude Mixup and Domain-Specific Image Restoration | Ziqi Zhou, Lei Qi, Yinghuan Shi | "For medical image analysis, segmentation models trained on one or several domains lack generalization ability to unseen domains due to discrepancies between different data acquisition policies. We argue that the degeneration in segmentation performance is mainly attributed to overfitting to source domains and domain shift. To this end, we present a novel generalizable medical image segmentation method. To be specific, we design our approach as a multi-task paradigm by combining the segmentation model with a self-supervision domain-specific image restoration (DSIR) module for model regularization. We also design a random amplitude mixup (RAM) module, which incorporates low-level frequency information of different domain images to synthesize new images. To guide our model be resistant to domain shift, we introduce a semantic consistency loss. We demonstrate the performance of our method on two public generalizable segmentation benchmarks in medical images, which validates our method could achieve the state-of-the-art performance." | 针对医学图像分析，由于不同数据采集政策之间存在差异，训练在一个或多个领域的分割模型缺乏对未知领域的泛化能力。我们认为，分割性能的退化主要归因于对源领域的过拟合和领域转移。为此，我们提出了一种新颖的通用医学图像分割方法。具体而言，我们将我们的方法设计为一个多任务范式，通过将分割模型与自监督领域特定图像恢复（DSIR）模块相结合，实现模型正则化。我们还设计了一个随机幅度混合（RAM）模块，该模块结合了不同领域图像的低级频率信息，用于合成新图像。为了指导我们的模型抵抗领域转移，我们引入了语义一致性损失。我们在医学图像的两个公共通用分割基准上展示了我们方法的性能，验证了我们的方法可以实现最先进的性能。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810415.pdf) |
| 2022 | Dual Contrastive Learning with Anatomical Auxiliary Supervision for Few-Shot Medical Image Segmentation | Huisi Wu, Fangyan Xiao, Chongxin Liang | "Few-shot semantic segmentation is a promising solution for scarce data scenarios, especially for medical imaging challenges with limited training data. However, most of the existing few-shot segmentation methods tend to over rely on the images containing target classes, which may hinder its utilization of medical imaging data. In this paper, we present a few-shot segmentation model that employs anatomical auxiliary information from medical images without target classes for dual contrastive learning. The dual contrastive learning module performs comparison among vectors from the perspectives of prototypes and contexts, to enhance the discriminability of learned features and the data utilization. Besides, to distinguish foreground features from background features more friendly, a constrained iterative prediction module is designed to optimize the segmentation of the query image. Experiments on two medical image datasets show that the proposed method achieves performance comparable to state-of-the-art methods." | 少样本语义分割是稀缺数据场景的一种有前途的解决方案，特别是对于训练数据有限的医学图像挑战。然而，大多数现有的少样本分割方法往往过度依赖包含目标类的图像，这可能会阻碍其对医学图像数据的利用。本文提出了一种少样本分割模型，利用医学图像中不包含目标类的解剖辅助信息进行双对比学习。双对比学习模块从原型和上下文的角度对向量进行比较，以增强学习特征的可区分性和数据利用率。此外，为了更友好地区分前景特征和背景特征，设计了一个受限迭代预测模块，优化查询图像的分割。对两个医学图像数据集的实验证明，所提出的方法实现了与最先进方法相媲美的性能。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136800406.pdf) |
| 2022 | Towards Metrical Reconstruction of Human Faces | Wojciech Zielonka, Timo Bolkart, Justus Thies | "Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call \modellong, outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively)." | 人脸重建和跟踪是增强现实/虚拟现实、人机交互以及医疗应用中众多应用的基础。大多数这些应用都依赖于对形状的度量正确预测，特别是当重建的对象放入度量背景中时（即存在已知尺寸的参考对象时）。度量重建也是任何测量对象距离和尺寸的应用所需的（例如，虚拟配戴眼镜架）。目前，从单一图像进行人脸重建的最先进方法是通过自监督方式训练在大规模2D图像数据集上。然而，由于透视投影的性质，它们无法重建实际人脸尺寸，甚至在度量意义上，预测平均人脸的方法有时表现更好。为了学习人脸的实际形状，我们提倡采用监督训练方案。由于目前没有针对这一任务的大规模3D数据集，我们对小规模和中等规模数据库进行了注释和统一。由此产生的统一数据集仍然是一个中等规模数据集，包含超过2k个身份，如果仅在此数据集上进行训练将导致过拟合。因此，我们利用一个在大规模2D图像数据集上预训练的人脸识别网络，该网络为不同的人脸提供独特特征，并且对表情、光照和摄像机变化具有鲁棒性。利用这些特征，我们以监督方式训练我们的人脸形状估计器，继承了人脸识别网络的鲁棒性和泛化性。我们的方法，即\modellong，在当前非度量基准和我们的度量基准上明显优于最先进的重建方法（在NoW基准上分别平均误差降低了15%和24%）。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730249.pdf) |
| 2022 | BayesCap: Bayesian Identity Cap for Calibrated Uncertainty in Frozen Neural Networks | Uddeshya Upadhyay, Shyamgopal Karthik, Yanbei Chen, Massimiliano Mancini, Zeynep Akata | "High-quality calibrated uncertainty estimates are crucial for numerous real-world applications, especially for deep learning-based deployed ML systems. While Bayesian deep learning techniques allow uncertainty estimation, training them with large-scale datasets is an expensive process that does not always yield models competitive with non-Bayesian counterparts. Moreover, many of the high-performing deep learning models that are already trained and deployed are non-Bayesian in nature, and do not provide uncertainty estimates. To address these issues, we propose BayesCap that learns a Bayesian identity mapping for the frozen model, allowing uncertainty estimation. BayesCap is a memory-efficient method that can be trained on a small fraction of the original dataset, enhancing pretrained non-Bayesian computer vision models by providing calibrated uncertainty estimates for the predictions without (i) hampering the performance of the model and (ii) the need for expensive retraining the model from scratch. The proposed method is agnostic to various architectures and tasks. We show the efficacy of our method on a wide variety of tasks with a diverse set of architectures, including image super-resolution, deblurring, inpainting, and crucial application such as medical image translation. Moreover, we apply the derived uncertainty estimates to detect out-of-distribution samples in critical scenarios like depth estimation in autonomous driving." | 高质量的校准不确定性估计对许多实际应用至关重要，特别是对于基于深度学习的部署的机器学习系统。虽然贝叶斯深度学习技术允许进行不确定性估计，但在大规模数据集上训练它们是一个昂贵的过程，而且并不总是能够产生与非贝叶斯对手竞争的模型。此外，许多已经训练和部署的性能优越的深度学习模型本质上是非贝叶斯的，不提供不确定性估计。为了解决这些问题，我们提出了BayesCap，它为冻结模型学习了一个贝叶斯身份映射，从而实现不确定性估计。BayesCap是一种内存高效的方法，可以在原始数据集的一小部分上进行训练，通过为预训练的非贝叶斯计算机视觉模型提供校准的不确定性估计，而不会( i )影响模型的性能，也不需要昂贵的从头开始重新训练模型。该方法对各种体系结构和任务都是不可知的。我们展示了我们的方法在各种任务上的有效性，涵盖了各种架构，包括图像超分辨率、去模糊、修复和重要应用，如医学图像转换。此外，我们将推导的不确定性估计应用于在关键场景中检测超出分布范围的样本，比如在自动驾驶中的深度估计。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136720295.pdf) |
| 2022 | Learning Deep Non-Blind Image Deconvolution without Ground Truths | Yuhui Quan, Zhuojie Chen, Huan Zheng, Hui Ji | "Non-blind image deconvolution (NBID) is about restoring a latent sharp image from a blurred one, given an associated blur kernel. Most existing deep neural networks for NBID are trained over many ground truth (GT) images, which limits their applicability in practical applications such as microscopic imaging and medical imaging. This paper proposes an unsupervised deep learning approach for NBID which avoids accessing GT images. The challenge raised from the absence of GT images is tackled by a self-supervised reconstruction loss that approximates its supervised counterpart well. The possible errors of blur kernels are addressed by a self-supervised prediction loss based on intermediate samples as well as an ensemble inference scheme based on kernel perturbation. The experiments show that the proposed approach provides very competitive performance to existing supervised learning-based methods, no matter under accurate kernels or erroneous kernels." | 非盲图像去卷积（NBID）是指在给定相关模糊核的情况下，从模糊图像中恢复一幅潜在清晰图像的过程。大多数现有的用于NBID的深度神经网络是在许多地面实况（GT）图像上进行训练的，这限制了它们在微观成像和医学成像等实际应用中的适用性。本文提出了一种无监督深度学习方法，用于NBID，避免访问GT图像。由于缺少GT图像而引发的挑战是通过一个自监督重构损失来解决的，该损失很好地近似了其监督对应部分。针对模糊核的可能错误，采用了基于中间样本的自监督预测损失以及基于核扰动的集成推断方案。实验表明，所提出的方法在精确核或错误核下，与现有的基于监督学习的方法具有非常有竞争力的性能。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660631.pdf) |
| 2022 | Conditional-Flow NeRF: Accurate 3D Modelling with Reliable Uncertainty Quantification | Jianxiong Shen, Antonio Agudo, Francesc Moreno-Noguer, Adria Ruiz | "A critical limitation of current methods based on Neural Radiance Fields (NeRF) is that they are unable to quantify the uncertainty associated with the learned appearance and geometry of the scene. This information is paramount in real applications such as medical diagnosis or autonomous driving where, to reduce potentially catastrophic failures, the confidence on the model outputs must be included into the decision-making process. In this context, we introduce Conditional-Flow NeRF (CF-NeRF), a novel probabilistic framework to incorporate uncertainty quantification into NeRF-based approaches. For this purpose, our method learns a distribution over all possible radiance fields modelling the scene which is used to quantify the uncertainty associated with the modelled scene. In contrast to previous approaches enforcing strong constraints over the radiance field distribution, CF-NeRF learns it in a flexible and fully data-driven manner by coupling Latent Variable Modelling and Conditional Normalizing Flows. This strategy allows to obtain reliable uncertainty estimation while preserving model expressivity. Compared to previous state-of-the-art methods proposed for uncertainty quantification in NeRF, our experiments show that the proposed method achieves significantly lower prediction errors and more reliable uncertainty values for synthetic novel view and depth-map estimation." | 目前基于神经辐射场（NeRF）的当前方法的一个关键局限是它们无法量化与学习的场景外观和几何形状相关的不确定性。在真实应用中，如医学诊断或自动驾驶中，这些信息至关重要，为了减少潜在的灾难性故障，模型输出的置信度必须包含在决策过程中。在这种情况下，我们介绍了条件流NeRF（CF-NeRF），这是一种新颖的概率框架，用于将不确定性量化纳入基于NeRF的方法中。为此，我们的方法学习了一个分布，涵盖了模拟场景的所有可能的辐射场，用于量化模拟场景相关的不确定性。与之前通过对辐射场分布施加强约束的方法相比，CF-NeRF通过耦合潜变量建模和条件归一化流以灵活和完全数据驱动的方式学习，从而实现了可靠的不确定性估计并保持了模型的表达能力。与先前用于NeRF中不确定性量化的最先进方法相比，我们的实验表明，所提出的方法在合成新视角和深度图估计方面实现了显著更低的预测误差和更可靠的不确定性值。 | [link](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630531.pdf) |
