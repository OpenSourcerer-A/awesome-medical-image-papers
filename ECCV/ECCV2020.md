| 年份 | 题目 | 作者 | 摘要 | 中文摘要 | link |
| --- | --- | --- | --- | --- | --- |
| 2020 | An Asymmetric Modeling for Action Assessment  | Jibin Gao, Wei-Shi Zheng, Jia-Hui Pan, Chengying Gao, Yaowei Wang, Wei Zeng, Jianhuang Lai  | Action assessment is a task of assessing the performance of an action. It is widely applicable to many real-world scenarios such as medical treatment and sporting events. However, existing methods for action assessment are mostly limited to individual actions, especially lacking modeling of the asymmetric relations among agents (e.g., between persons and objects); and this limitation undermines their ability to assess actions containing asymmetrically interactive motion patterns, since there always exists subordination between agents in many interactive actions. In this work, we model the asymmetric interactions among agents for action assessment. In particular, we propose an asymmetric interaction module (AIM), to explicitly model asymmetric interactions between intelligent agents within an action, where we group these agents into a primary one (e.g., human) and secondary ones (e.g., objects). We perform experiments on JIGSAWS dataset containing surgical actions, and additionally collect a new dataset, TASD-2, for interactive sporting actions. The experimental results on two interactive action datasets show the effectiveness of our model, and our method achieves state-of-the-art performance. The extended experiment on AQA-7 dataset also demonstrates the generalization capability of our framework to conventional action assessment." | 行动评估是评估一项行动表现的任务。它广泛适用于许多现实场景，如医疗治疗和体育赛事。然而，现有的行动评估方法大多局限于个体行动，特别是缺乏对智能体之间的不对称关系（例如，人与物体之间）建模；这种局限影响了它们评估包含不对称交互运动模式的行动的能力，因为在许多互动行动中通常存在智能体之间的从属关系。在这项工作中，我们对智能体之间的不对称交互进行建模进行行动评估。特别是，我们提出了一个不对称交互模块（AIM），来明确模拟行动中智能体之间的不对称交互，我们将这些智能体分为一个主要智能体（例如，人类）和次要智能体（例如，物体）。我们在包含外科行动的JIGSAWS数据集上进行实验，并额外收集了一个新数据集TASD-2，用于互动体育行动。在两个互动行动数据集上的实验结果显示了我们模型的有效性，我们的方法实现了最先进的性能。对AQA-7数据集的扩展实验也展示了我们的框架对传统行动评估的泛化能力。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750222.pdf) |
| 2020 | Self-supervision with Superpixels: Training Few-shot Medical Image Segmentation without Annotation  | Cheng Ouyang, Carlo Biffi, Chen Chen, Turkay Kart, Huaqi Qiu, Daniel Rueckert  | Few-shot semantic segmentation (FSS) has great potential for medical imaging applications. Most of the existing FSS techniques require abundant annotated semantic classes for training. However, these methods may not be applicable for medical images due to the lack of annotations. To address this problem we make several contributions: (1) A novel self-supervised FSS framework for medical images in order to eliminate the requirement for annotations during training. Additionally, superpixel-based pseudo-labels are generated to provide supervision; (2) An adaptive local prototype pooling module plugged into prototypical networks, to solve the common challenging foreground-background imbalance problem in medical image segmentation; (3) We demonstrate the general applicability of the proposed approach for medical images using three different tasks: abdominal organ segmentation for CT and MRI, as well as cardiac segmentation for MRI. Our results show that, for medical image segmentation, the proposed method outperforms conventional FSS methods which require manual annotations for training." | 少样本语义分割(FSS)在医学图像应用中具有巨大潜力。大多数现有的FSS技术需要丰富的注释语义类别进行训练。然而，由于缺乏注释，这些方法可能不适用于医学图像。为了解决这个问题，我们做出了几点贡献：(1)提出了一种新颖的自监督FSS框架，用于医学图像，以消除训练过程中对注释的需求。此外，基于超像素的伪标签被生成以提供监督；(2)将自适应局部原型池模块插入到原型网络中，以解决医学图像分割中常见的前景-背景不平衡问题；(3)我们展示了所提出的方法在医学图像中的普适性，包括CT和MRI的腹部器官分割，以及MRI的心脏分割。我们的结果表明，对于医学图像分割，所提出的方法优于传统的FSS方法，这些方法需要手动注释进行训练。". | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123740749.pdf) |
| 2020 | Learning Permutation Invariant Representations using Memory Networks  | Shivam Kalra, Mohammed Adnan, Graham Taylor, H.R. Tizhoosh  | Many real-world tasks such as classification of digital histopathological images and 3D object detection involve learning from a set of instances. In these cases, only a group of instances or a set, collectively, contains meaningful information and therefore only the sets have labels, and not individual data instances. In this work, we present a permutation invariant neural network called Memory-based Exchangeable Model (MEM) for learning universal set functions. The MEM model consists of memory units that embed an input sequence to high-level features enabling it to learn inter-dependencies among instances through a self-attention mechanism. We evaluated the learning ability of MEM on various toy datasets, point cloud classification, and classification of whole slide images (WSIs) into two subtypes of the lung cancer--Lung Adenocarcinoma, and Lung Squamous Cell Carcinoma. We systematically extracted patches from WSIs of the lung, downloaded from The Cancer Genome Atlas (TCGA) dataset, the largest public repository of WSIs, achieving a competitive accuracy of 84.84% for classification of two sub-types of lung cancer. The results on other datasets are promising as well, and demonstrate the efficacy of our model. \keywords{Permutation Invariant Models, Multi Instance Learning, Whole Slide Image Classification, Medical Images}" | 许多现实世界的任务，如数字组织病理学图像的分类和3D物体检测，涉及从一组实例中学习。在这些情况下，只有一组实例或一个集合共同包含有意义的信息，因此只有集合有标签，而不是单个数据实例。在这项工作中，我们提出了一个称为Memory-based Exchangeable Model（MEM）的置换不变神经网络，用于学习通用集合函数。MEM模型由内存单元组成，将输入序列嵌入高级特征，使其能够通过自我注意机制学习实例之间的相互依赖关系。我们评估了MEM在各种玩具数据集、点云分类和将整张幻灯片图像（WSIs）分类为肺癌的两个亚型--肺腺癌和肺鳞状细胞癌。我们系统地从来自癌症基因组图谱（TCGA）数据集的肺WSIs中提取片段，实现了对两种肺癌亚型进行分类的竞争准确率达到84.84%。其他数据集的结果也是令人鼓舞的，并证明了我们模型的有效性。 \keywords{置换不变模型，多实例学习，整张幻灯片图像分类，医学图像}。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123740664.pdf) |
| 2020 | Adversarial Data Augmentation via Deformation Statistics  | Sahin Olut, Zhengyang Shen, Zhenlin Xu, Samuel Gerber, Marc Niethammer  | Deep learning models have been successful in computer vision and medical image analysis. However, training these models frequently requires large labeled image sets whose creation is often very time and labor intensive, for example, in the context of 3D segmentations. Approaches capable of training deep segmentation networks with a limited number of labeled samples are therefore highly desirable. Data augmentation or semi-supervised approaches are commonly used to cope with limited labeled training data. However, the augmentation strategies for many existing approaches are either hand-engineered or require computationally demanding searches. To that end, we explore an augmentation strategy which builds statistical deformation models from unlabeled data via principal component analysis and uses the resulting statistical deformation space to augment the labeled training samples. Specifically, we obtain transformations via deep registration models. This allows for an intuitive control over plausible deformation magnitudes via the statistical model and, if combined with an appropriate deformation model, yields spatially regular transformations. To optimally augment a dataset we use an adversarial strategy integrated into our statistical deformation model. We demonstrate the effectiveness of our approach for the segmentation of knee cartilage from 3D magnetic resonance images. We show favorable performance to state-of-the-art augmentation approaches." | 深度学习模型在计算机视觉和医学图像分析领域取得了成功。然而，训练这些模型通常需要大量标记的图像数据集，其创建往往非常耗时和劳动密集，例如，在3D分割的背景下。因此，能够通过有限数量的标记样本训练深度分割网络的方法是非常可取的。数据增强或半监督方法通常用于应对有限的标记训练数据。然而，许多现有方法的增强策略要么是手工设计的，要么需要计算密集型的搜索。因此，我们探索了一种增强策略，通过主成分分析从未标记数据构建统计变形模型，并利用该结果的统计变形空间来增强标记的训练样本。具体而言，我们通过深度注册模型获得变换。这使得通过统计模型直观地控制合理的变形幅度成为可能，并且，如果与适当的变形模型结合使用，将产生空间规则的变换。为了最佳地增强数据集，我们在我们的统计变形模型中整合了对抗策略。我们展示了我们的方法在从3D磁共振图像中分割膝关节软骨方面的有效性。我们展示了与现有增强方法相比有利的性能。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123740630.pdf) |
| 2020 | Deep Decomposition Learning for Inverse Imaging Problems  | Dongdong Chen, Mike E. Davies  | Deep learning is emerging as a new paradigm for solving inverse imaging problems. However, the deep learning methods often lack the assurance of traditional physics-based methods due to the lack of physical information considerations in neural network training and deploying. The appropriate supervision and explicit calibration by the information of the physic model can enhance the neural network learning and its practical performance. In this paper, inspired by the geometry that data can be decomposed by two components from the null-space of the forward operator and the range space of its pseudo-inverse, we train neural networks to learn the two components and therefore learn the decomposition, i.e. we explicitly reformulate the neural network layers as learning range-nullspace decomposition functions with reference to the layer inputs, instead of learning unreferenced functions. We empirically show that the proposed framework demonstrates superior performance over recent deep residual learning, unrolled learning and nullspace learning on tasks including compressive sensing medical imaging and natural image super-resolution. Our code is available at https://github.com/edongdongchen/DDN." | 深度学习作为解决逆向成像问题的新范式正在兴起。然而，由于神经网络训练和部署中缺乏物理信息考虑，深度学习方法常常缺乏传统基于物理的方法的保证。通过物理模型的适当监督和明确校准，可以增强神经网络的学习能力和实际性能。在本文中，受到数据可以通过正演算子的零空间和其伪逆的值域空间的两个分量进行分解的几何性启发，我们训练神经网络学习这两个分量，从而学习分解，即我们明确地重新构造神经网络层，将其作为学习范围-零空间分解函数，参考层输入，而不是学习无参考函数。我们在压缩感知医学成像和自然图像超分辨率等任务上经验性地展示了所提出的框架相对于最近的深度残差学习、展开学习和零空间学习具有卓越的性能。我们的代码可在https://github.com/edongdongchen/DDN上找到。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730511.pdf) |
| 2020 | A Broader Study of Cross-Domain Few-Shot Learning  | Yunhui Guo, Noel C. Codella, Leonid Karlinsky, James V. Codella, John R. Smith, Kate Saenko, Tajana Rosing, Rogerio Feris  | Recent progress on few-shot learning largely relies on annotated data for meta-learning: base classes sampled from the same domain as the novel classes. However, in many applications, collecting data for meta-learning is infeasible or impossible. This leads to the cross-domain few-shot learning problem, where there is a large shift between base and novel class domains. While investigations of the cross-domain few-shot scenario exist, these works are limited to natural images that still contain a high degree of visual similarity. No work yet exists that examines few-shot learning across different imaging methods seen in real world scenarios, such as aerial and medical imaging. In this paper, we propose the Broader Study of Cross-Domain Few-Shot Learning (BSCD-FSL) benchmark, consisting of image data from a diverse assortment of image acquisition methods. This includes natural images, such as crop disease images, but additionally those that present with an increasing dissimilarity to natural images, such as satellite images, dermatology images, and radiology images. Extensive experiments on the proposed benchmark are performed to evaluate state-of-art meta-learning approaches, transfer learning approaches, and newer methods for cross-domain few-shot learning. The results demonstrate that state-of-art meta-learning methods are surprisingly outperformed by earlier meta-learning approaches, and all meta-learning methods underperform in relation to simple fine-tuning by 12.8% average accuracy. In some cases, meta-learning even underperforms networks with random weights. Performance gains previously observed with methods specialized for cross-domain few-shot learning vanish in this more challenging benchmark. Finally, accuracy of all methods tend to correlate with dataset similarity to natural images, verifying the value of the benchmark to better represent the diversity of data seen in practice and guiding future research. Code for all experiments in this work will be made available on GitHub." | 最近关于少样本学习的进展主要依赖于用于元学习的带标注数据：从与新颖类别相同域中抽取的基础类别。然而，在许多应用中，收集用于元学习的数据是不可行或不可能的。这导致了跨域少样本学习问题，即基础类别和新颖类别之间存在很大的领域转移。虽然存在对跨域少样本情景的研究，但这些工作仅限于仍然包含高度视觉相似性的自然图像。尚未有研究涉及到在现实场景中出现的不同成像方法之间的少样本学习，如航空和医学成像。在本文中，我们提出了更广泛的跨域少样本学习（BSCD-FSL）基准，其中包含来自各种图像获取方法的图像数据。这包括自然图像，例如作物病害图像，但还包括那些与自然图像越来越不相似的图像，如卫星图像、皮肤科图像和放射科图像。对提出的基准进行了广泛实验，评估了最先进的元学习方法、迁移学习方法和用于跨域少样本学习的新方法。结果表明，最先进的元学习方法被早期的元学习方法意外地超越，并且所有的元学习方法相对于简单的微调平均准确率低12.8%。在某些情况下，元学习甚至不如随机权重的网络。先前观察到的专门用于跨域少样本学习的方法所带来的性能提升在这个更具挑战性的基准中消失。最后，所有方法的准确性往往与数据集与自然图像的相似度相关，验证了该基准更好地代表实践中所见数据的多样性，并指导未来的研究。本文中所有实验的代码将在GitHub上提供。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720120.pdf) |
| 2020 | Mining self-similarity: Label super-resolution with epitomic representations  | Nikolay Malkin, Anthony Ortiz, Nebojsa Jojic  | We show that simple patch-based models, such as epitomes (Jojic et al., 2003), can have superior performance to the current state of the art in semantic segmentation and label super-resolution, which uses deep convolutional neural networks. We derive a new training algorithm for epitomes which allows, for the first time, learning from very large data sets and derive a label super-resolution algorithm as a statistical inference algorithm over epitomic representations. We illustrate our methods on land cover mapping and medical image analysis tasks." | 我们展示了简单的基于补丁的模型，如典范（Jojic等，2003），在语义分割和标签超分辨率方面的性能优于当前使用深度卷积神经网络的现有技术。我们推导了一种新的典范训练算法，首次允许从非常大的数据集中学习，并导出了作为统计推断算法的标签超分辨率算法，该算法基于典范表示。我们在土地覆盖映射和医学图像分析任务中展示了我们的方法。". | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710528.pdf) |
| 2020 | Learning Enriched Features for Real Image Restoration and Enhancement  | Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, Ling Shao  | With the goal of recovering high-quality image content from its degraded version, image restoration enjoys numerous applications, such as in surveillance, computational photography and medical imaging. Recently, convolutional neural networks (CNNs) have achieved dramatic improvements over conventional approaches for image restoration task. Existing CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatially precise but contextually less robust results are achieved, while in the latter case, semantically reliable but spatially less accurate outputs are generated. In this paper, we present an architecture with the collective goals of maintaining spatially-precise high-resolution representations through the entire network and receiving strong contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing several key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) spatial and channel attention mechanisms for capturing contextual information, and (d) attention based multi-scale feature aggregation. In a nutshell, our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on five real image benchmark datasets demonstrate that our method, named as MIRNet, achieves state-of-the-art results for image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNet." | 随着从受损图像恢复高质量图像内容的目标，图像恢复在监控、计算摄影和医学成像等领域有着众多应用。最近，卷积神经网络（CNNs）在图像恢复任务上取得了比传统方法更显著的改进。现有基于CNN的方法通常在完整分辨率或逐渐降低的表示上进行操作。在前一种情况下，可以实现空间精确但上下文不够稳健的结果，而在后一种情况下，则会生成语义可靠但空间不够准确的输出。本文提出了一种旨在通过整个网络保持空间精确高分辨率表示并从低分辨率表示中获得强大上下文信息的架构。我们方法的核心是一个包含几个关键元素的多尺度残差块：（a）用于提取多尺度特征的并行多分辨率卷积流，（b）跨多分辨率流的信息交换，（c）用于捕获上下文信息的空间和通道关注机制，以及（d）基于注意力的多尺度特征聚合。简而言之，我们的方法学习了一个丰富的特征集，结合了来自多个尺度的上下文信息，同时保留了高分辨率的空间细节。在五个真实图像基准数据集上进行的大量实验表明，我们的方法，名为MIRNet，实现了图像去噪、超分辨率和图像增强的最先进结果。源代码和预训练模型可在https://github.com/swz30/MIRNet 上获得。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700494.pdf) |
| 2020 | SideInfNet: A Deep Neural Network for Semi-Automatic Semantic Segmentation with Side Information  | Jing Yu Koh, Duc Thanh Nguyen, Quang-Trung Truong, Sai-Kit Yeung, Alexander Binder  | Fully-automatic execution is the ultimate goal for many Computer Vision applications. However, this objective is not always realistic in tasks associated with high failure costs, such as medical applications. For these tasks, semi-automatic methods allowing minimal effort from users to guide computer algorithms are often preferred due to desirable accuracy and performance. Inspired by the practicality and applicability of the semi-automatic approach, this paper proposes a novel deep neural network architecture, namely SideInfNet that effectively integrates features learnt from images with side information extracted from user annotations. To evaluate our method, we applied the proposed network to three semantic segmentation tasks and conducted extensive experiments on benchmark datasets. Experimental results and comparison with prior work have verified the superiority of our model, suggesting the generality and effectiveness of the model in semi-automatic semantic segmentation. " | 全自动执行是许多计算机视觉应用的最终目标。然而，在与高失败成本相关的任务中，如医疗应用，这一目标并不总是现实的。对于这些任务，通常更喜欢半自动方法，允许用户以最小的努力来引导计算机算法，以实现理想的准确性和性能。受半自动方法的实用性和适用性启发，本文提出了一种新颖的深度神经网络架构，名为SideInfNet，它有效地将从图像学习的特征与从用户注释中提取的辅助信息集成在一起。为了评估我们的方法，我们将提出的网络应用于三个语义分割任务，并在基准数据集上进行了大量实验。实验结果和与先前工作的比较已经验证了我们模型的优越性，表明了在半自动语义分割中模型的普适性和有效性。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690103.pdf) |
| 2020 | Co-Heterogeneous and Adaptive Segmentation from Multi-Source and Multi-Phase CT Imaging Data: A Study on Pathological Liver and Lesion Segmentation  | Ashwin Raju, Chi-Tung Cheng, Yuankai Huo, Jinzheng Cai, Junzhou Huang, Jing Xiao, Le Lu, ChienHung Liao, Adam P. Harrison  | Within medical imaging, organ/pathology segmentation models trained on current publicly available and fully-annotated datasets usually do not well-represent the heterogeneous modalities, phases, pathologies, and clinical scenarios encountered in real environments. On the other hand, there are tremendous amounts of unlabelled patient imaging scans stored by many modern clinical centers. In this work, we present a novel segmentation strategy, co-heterogenous andadaptive segmentation (CHASe), which only requires a small labeled cohort of single phase data to adapt to any unlabeled cohort of heterogenous multi-phase data with possibly new clinical scenarios and pathologies. To do this, we develop a versatile framework that fuses appearance-based semi-supervision, mask-based adversarial domain adaptation, and pseudo-labeling. We also introduce co-heterogeneous training, which is a novel integration of co-training and hetero-modality learning. We evaluate CHASe using a clinically comprehensive and challenging dataset of multi-phase computed tomography (CT) imaging studies (1147 patients and 4577 3D volumes), with a test set of 100 patients. Compared to previous state-of-the-art baselines, CHASe can further improve pathological liver mask Dice-SÃ¸rensen coefficients by ranges of 4.2% to 9.4%, depending on the phase combinations, e.g., from 84.6% to 94.0% on non-contrast CTs." | 在医学成像领域，目前基于公开可用且完全标注数据集训练的器官/病理分割模型通常不能很好地代表真实环境中遇到的异质模态、相位、病变和临床场景。另一方面，许多现代临床中心存储了大量未标记的患者影像扫描。在这项工作中，我们提出了一种新颖的分割策略，即协同异质和自适应分割（CHASe），它只需要少量标记的单相数据集就能适应任何未标记的异质多相数据集，可能涉及新的临床场景和病变。为此，我们开发了一个融合基于外观的半监督、基于掩模的对抗域自适应和伪标记的多功能框架。我们还引入了协同异质训练，这是协同训练和异质模态学习的新颖整合。我们使用了一组临床全面且具有挑战性的多相计算机断层扫描（CT）影像研究数据集（1147名患者和4577个3D体积）对CHASe进行评估，测试集包括100名患者。与先前最先进的基线相比，CHASe可以进一步改善病理性肝脏掩模Dice-Sørensen系数，改善范围为4.2%至9.4%，具体取决于不同相位组合，例如在非造影CT上从84.6%提高到94.0%。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680443.pdf) |
| 2020 | Deep Complementary Joint Model for Complex Scene Registration and Few-shot Segmentation on Medical Images  | Yuting He, Tiantian Li, Guanyu Yang, Youyong Kong, Yang Chen, Huazhong Shu, Jean-Louis Coatrieux, Jean-Louis Dillenseger, Shuo Li  | Deep learning-based medical image registration and segmentation joint models utilize the complementarity (augmentation data or weakly supervised data from registration, region constraints from segmentation) to bring mutual improvement in complex scene and few-shot situation. However, further adoption of the joint models are hindered: 1) the diversity of augmentation data is reduced limiting the further enhancement of segmentation, 2) misaligned regions in weakly supervised data disturb the training process, 3) lack of label-based region constraints in few-shot situation limits the registration performance. We propose a novel Deep Complementary Joint Model (DeepRS) for complex scene registration and few-shot segmentation. We embed a perturbation factor in the registration to increase the activity of deformation thus maintaining the augmentation data diversity. We take a pixel-wise discriminator to extract alignment confidence maps which highlight aligned regions in weakly supervised data so the misaligned regions' disturbance will be suppressed via weighting. The outputs from segmentation model are utilized to implement deep-based region constraints thus relieving the label requirements and bringing fine registration. Extensive experiments on the CT dataset of MM-WHS 2017 Challenge show great advantages of our DeepRS that outperforms the existing state-of-the-art models." | 基于深度学习的医学图像配准和分割联合模型利用互补性（从配准中获取数据增强或弱监督数据，从分割中获取区域约束），在复杂场景和少样本情况下相互改进。然而，联合模型的进一步应用受到阻碍：1）数据增强的多样性减少，限制了对分割的进一步增强，2）弱监督数据中的不对齐区域干扰了训练过程，3）在少样本情况下缺乏基于标签的区域约束限制了配准性能。我们提出了一种新颖的Deep Complementary Joint Model（DeepRS）用于复杂场景配准和少样本分割。我们在配准中嵌入了扰动因子，以增加变形的活动性，从而保持数据增强的多样性。我们采用像素级鉴别器提取对齐置信度图，突出弱监督数据中的对齐区域，因此通过加权来抑制不对齐区域的干扰。分割模型的输出被用于实施基于深度的区域约束，从而减轻标签要求，并实现精细的配准。在MM-WHS 2017挑战赛的CT数据集上进行了大量实验证明了我们的DeepRS具有明显优势，优于现有的最先进模型。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630749.pdf) |
| 2020 | Two Stream Active Query Suggestion for Active Learning in Connectomics  | Zudi Lin, Donglai Wei, Won-Dong Jang, Siyan Zhou, Xupeng Chen, Xueying Wang, Richard Schalek, Daniel Berger, Brian Matejek, Lee Kamentsky, Adi Peleg, Daniel Haehn, Thouis Jones, Toufiq Parag, Jeff Lichtman, Hanspeter Pfister  | For large-scale vision tasks in biomedical images, the labeled data is often limited to train effective deep models. Active learning is a common solution, where a query suggestion method selects representative unlabeled samples for annotation, and the new labels are used to improve the base model. However, most query suggestion models optimize their learnable parameters only on the limited labeled data and consequently become less effective for the more challenging unlabeled data. To tackle this, we propose a two-stream active query suggestion approach. In addition to the supervised feature extractor, we introduce an unsupervised one optimized on all raw images to capture diverse image features, which can later be improved by fine-tuning on new labels. As a use case, we build an end-to-end active learning framework with our query suggestion method for 3D synapse detection and mitochondria segmentation in connectomics. With the framework, we curate, to our best knowledge, the largest connectomics dataset with dense synapses and mitochondria annotation. On this new dataset, our method outperforms previous state-of-the-art methods by 3.1% for synapse and 3.8% for mitochondria in terms of region-of-interest proposal accuracy. We also apply our method to image classification, where it outperforms previous approaches on CIFAR-10 under the same limited annotation budget. The project page is https://zudi-lin.github.io/projects/#two_stream_active." | 对于生物医学图像中的大规模视觉任务，标记数据通常有限，无法训练有效的深度模型。主动学习是一种常见解决方案，其中查询建议方法选择代表性的未标记样本进行注释，并利用新标签来改进基础模型。然而，大多数查询建议模型仅在有限的标记数据上优化其可学习参数，因此对于更具挑战性的未标记数据效果较差。为了解决这个问题，我们提出了一种两流主动查询建议方法。除了有监督的特征提取器外，我们引入了一个无监督的特征提取器，优化所有原始图像以捕获多样化的图像特征，后者可以通过在新标签上微调来改进。作为一个应用案例，我们构建了一个端到端的主动学习框架，其中使用我们的查询建议方法进行3D突触检测和线粒体分割。通过该框架，我们精心整理了迄今为止关于密集突触和线粒体标注的最大连接组学数据集。在这个新数据集上，我们的方法在感兴趣区域提案准确性方面比以往最先进的方法分别提高了3.1%和3.8%。我们还将我们的方法应用于图像分类，在相同的有限标注预算下，它在CIFAR-10上胜过以前的方法。项目页面为https://zudi-lin.github.io/projects/#two_stream_active。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630103.pdf) |
| 2020 | Attention Guided Anomaly Localization in Images  | Shashanka Venkataramanan, Kuan-Chuan Peng, Rajat Vikram Singh, Abhijit Mahalanobis  | Anomaly localization is an important problem in computer vision which involves localizing anomalous regions within images with applications in industrial inspection, surveillance, and medical imaging. This task is challenging due to the small sample size and pixel coverage of the anomaly in real-world scenarios. Most prior works need to use anomalous training images to compute a class-specific threshold to localize anomalies. Without the need of anomalous training images, we propose Convolutional Adversarial Variational autoencoder with Guided Attention (CAVGA), which localizes the anomaly with a convolutional latent variable to preserve the spatial information. In the unsupervised setting, we propose an attention expansion loss where we encourage CAVGA to focus on all normal regions in the image. Furthermore, in the weakly-supervised setting we propose a complementary guided attention loss, where we encourage the attention map to focus on all normal regions while minimizing the attention map corresponding to anomalous regions in the image. CAVGA outperforms the state-of-the-art (SOTA) anomaly localization methods on MVTec Anomaly Detection (MVTAD), modified ShanghaiTech Campus (mSTC) and Large-scale Attention based Glaucoma (LAG) datasets in the unsupervised setting and when using only 2% anomalous images in the weakly-supervised setting. CAVGA also outperforms SOTA anomaly detection methods on the MNIST, CIFAR-10,Fashion-MNIST, MVTAD, mSTC and LAG datasets." | 异常定位是计算机视觉中的一个重要问题，涉及在图像中定位异常区域，其应用包括工业检测、监控和医学成像。由于现实场景中异常的样本数量少且像素覆盖范围小，这一任务具有挑战性。大多数先前的工作需要使用异常训练图像来计算类别特定的阈值来定位异常。在不需要异常训练图像的情况下，我们提出了一种带有引导注意力的卷积对抗变分自编码器（CAVGA），该模型使用卷积潜变量来定位异常以保留空间信息。在无监督设置中，我们提出了一个注意力扩展损失，鼓励CAVGA关注图像中的所有正常区域。此外，在弱监督设置中，我们提出了一个互补的引导注意力损失，鼓励注意力图集中在所有正常区域，同时最小化与图像中异常区域对应的注意力图。CAVGA在无监督设置下优于MVTec异常检测（MVTAD）、修改后的上海科技园区（mSTC）和基于注意力的大规模青光眼（LAG）数据集上的最先进异常定位方法，并且在弱监督设置中仅使用2％异常图像时也表现优异。CAVGA还在MNIST、CIFAR-10、Fashion-MNIST、MVTAD、mSTC和LAG数据集上优于最先进的异常检测方法。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620477.pdf) |
| 2020 | JSSR: A Joint Synthesis, Segmentation, and Registration System for 3D Multi-Modal Image Alignment of Large-scale Pathological CT Scans  | Fengze Liu, Jinzheng Cai, Yuankai Huo, Chi-Tung Cheng, Ashwin Raju, Dakai Jin, Jing Xiao, Alan Yuille, Le Lu, ChienHung Liao, Adam P. Harrison  | Segmentation, and Registration System for 3D Multi-Modal Image Alignment of Large-scale Pathological CT Scans","Multi-modal image registration is a challenging problem that is also an important clinical task for many real applications and scenarios. As a first step in analysis, deformable registration among different image modalities is often required in order to provide complementary visual information. During registration, semantic information is key to match homologous points and pixels. Nevertheless, many conventional registration methods are incapable in capturing high-level semantic anatomical dense correspondences. In this work, we propose a novel multi-task learning system, JSSR, based on an end-to-end 3D convolutional neural network that is composed of a generator, a registration and a segmentation component. The system is optimized to satisfy the implicit constraints between different tasks in an unsupervised manner. It first synthesizes the source domain images into the target domain, then an intra-modal registration is applied on the synthesized images and target images. The segmentation module are then applied on the synthesized and target images, providing additional cues based on semantic correspondences. The supervision from another fully-annotated dataset is used to regularize the segmentation. We extensively evaluate JSSR on a large-scale medical image dataset containing 1,485 patient CT imaging studies of four different contrast phases (i.e., 5,940 3D CT scans with pathological livers) on the registration, segmentation and synthesis tasks. The performance is improved after joint training on the registration and segmentation tasks by 0.9% and 1.9% respectively compared to a highly competitive and accurate deep learning baseline. The registration also consistently outperforms conventional state-of-the-art multi-modal registration methods." | 大规模病理性CT扫描的3D多模态图像配准系统多模态图像配准是一个具有挑战性的问题，对于许多实际应用和场景来说也是一项重要的临床任务。在分析的第一步中，通常需要在不同图像模态之间进行可变形配准，以提供互补的视觉信息。在配准过程中，语义信息对于匹配同源点和像素至关重要。然而，许多传统的配准方法无法捕捉高级语义解剖密集对应关系。在本研究中，我们提出了一种基于端到端3D卷积神经网络的新型多任务学习系统JSSR，该系统由生成器、配准和分割组件组成。该系统经过优化，以无监督的方式满足不同任务之间的隐式约束。它首先将源域图像合成为目标域图像，然后对合成图像和目标图像进行模态内配准。然后，分割模块应用于合成和目标图像，根据语义对应关系提供额外线索。来自另一个完全注释的数据集的监督用于规范分割。我们在一个包含1,485例患者CT成像研究的大规模医学图像数据集上对JSSR进行了广泛评估，其中包含四个不同对比相位（即，具有病理性肝脏的5,940个3D CT扫描）。与高度竞争和准确的深度学习基线相比，通过在配准和分割任务上的联合训练，性能分别提高了0.9%和1.9%。配准也一直优于传统的最先进多模态配准方法。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580256.pdf) |
| 2020 | Structured Landmark Detection via Topology-Adapting Deep Graph Learning  | Weijian Li, Yuhang Lu, Kang Zheng, Haofu Liao, Chihung Lin, Jiebo Luo, Chi-Tung Cheng, Jing Xiao, Le Lu, Chang-Fu Kuo, Shun Miao  | Image landmark detection aims to automatically identify the locations of predefined fiducial points. Despite recent success in this field, higher-ordered structural modeling to capture implicit or explicit relationships among anatomical landmarks has not been adequately exploited. In this work, we present a new topology-adapting deep graph learning approach for accurate anatomical facial and medical (e.g., hand, pelvis) landmark detection. The proposed method constructs graph signals leveraging both local image features and global shape features. The adaptive graph topology naturally explores and lands on task-specific structures which are learned end-to-end with two Graph Convolutional Networks (GCNs). Extensive experiments are conducted on three public facial image datasets (WFLW, 300W, and COFW-68) as well as three real-world X-ray medical datasets (Cephalometric (public), Hand and Pelvis). Quantitative results comparing with the previous state-of-the-art approaches across all studied datasets indicating the superior performance in both robustness and accuracy. Qualitative visualizations of the learned graph topologies demonstrate a physically plausible connectivity laying behind the landmarks." | 图像地标检测旨在自动识别预定义的标志性点的位置。尽管该领域最近取得了成功，但为了捕捉解剖地标之间的隐含或显式关系，尚未充分利用高阶结构建模。在这项工作中，我们提出了一种新的拓扑自适应深度图学习方法，用于准确地检测解剖面部和医学（例如手部、骨盆）地标。所提出的方法构建图信号，利用局部图像特征和全局形状特征。自适应图拓扑自然地探索并落在学习端到端的两个图卷积网络（GCNs）所学习的任务特定结构上。在三个公共面部图像数据集（WFLW、300W和COFW-68）以及三个真实世界的X射线医学数据集（头颅测量（公共）、手部和骨盆）上进行了大量实验。在所有研究数据集上与先前最先进方法进行对比的定量结果表明了在稳健性和准确性方面的优越表现。学习的图拓扑的定性可视化展示了地标背后的物理合理连接。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540256.pdf) |
| 2020 | Regression of Instance Boundary by Aggregated CNN and GCN  | Yanda Meng, Wei Meng, Dongxu Gao, Yitian Zhao, Xiaoyun Yang, Xiaowei Huang, Yalin Zheng  | This paper proposes a straightforward, intuitive deep learning approach for (biomedical) image segmentation tasks. Different from the existing dense pixel classification methods, we develop a novel multilevel aggregation network to directly regress the coordinates of the boundary of instances in an end-to-end manner. The network seamlessly combines standard convolution neural network (CNN) with Attention Refinement Module (ARM) and Graph Convolution Network (GCN). By iteratively and hierarchically fusing the features across different layers of the CNN, our approach gains sufficient semantic information from the input image and pays special attention to the local boundaries with the help of ARM and GCN. In particular, thanks to the proposed aggregation GCN, our network benefits from direct feature learning of the instancesâ boundary locations and the spatial information propagation across the image. Experiments on several challenging datasets demonstrate that our method achieves comparable results with state-of-the-art approaches but requires less inference time on the segmentation of fetal head in ultrasound images and of optic disc and optic cup in color fundus images." | 本文提出了一种直观的深度学习方法，用于（生物医学）图像分割任务。与现有的密集像素分类方法不同，我们开发了一种新颖的多级聚合网络，以端到端的方式直接回归实例边界的坐标。该网络无缝地将标准卷积神经网络（CNN）与注意力细化模块（ARM）和图卷积网络（GCN）相结合。通过在CNN的不同层之间迭代和层次地融合特征，我们的方法从输入图像中获得足够的语义信息，并在ARM和GCN的帮助下特别关注局部边界。特别是，由于提出的聚合GCN，我们的网络受益于直接学习实例边界位置的特征以及跨图像的空间信息传播。对几个具有挑战性的数据集进行的实验表明，我们的方法在胎儿头部超声图像分割和视网膜彩色眼底图像中视盘和视杯的分割方面取得了可比较的结果，但需要更少的推断时间。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530188.pdf) |
| 2020 | TCGM: An Information-Theoretic Framework for Semi-Supervised Multi-Modality Learning  | Xinwei Sun, Yilun Xu, Peng Cao, Yuqing Kong, Lingjing Hu, Shanghang Zhang, Yizhou Wang  | Fusing data from multiple modalities provides more information to train machine learning systems. However, it is prohibitively expensive and time-consuming to label each modality with a large amount of data, which leads to a crucial problem of such semi-supervised multi-modal learning. Existing methods suffer from either ineffective fusion across modalities or lack of theoretical results under proper assumptions. In this paper, we propose a novel information-theoretic approach \-- namely, extbf{T}otal extbf{C}orrelation extbf{G}ain extbf{M}aximization (TCGM) \--- for semi-supervised multi-modal learning, which is endowed with promising properties: (i) it can utilize effectively the information across different modalities of unlabeled data points to facilitate training classifiers of each modality (ii) has theoretical guarantee to have theoretical guarantee to identify Bayesian classifiers, i.e., the ground truth posteriors of all modalities. Specifically, by maximizing TC-induced loss (namely TC gain) over classifiers of all modalities, these classifiers can cooperatively discover the equivalent class of ground-truth classifiers; and identify the unique ones by leveraging a limited percentage of labeled data. We apply our method and can achieve state-of-the-art results on various datasets, including the Newsgroup dataset, Emotion recognition (IEMOCAP and MOSI) and Medical Imaging (Alzheimerâs Disease Neuroimaging Initiative). | 融合多模态数据为训练机器学习系统提供更多信息。然而，对每种模态进行大量数据标记的成本高昂且耗时，这导致了这种半监督多模态学习的一个关键问题。现有方法要么在模态之间融合效果不佳，要么缺乏在适当假设下的理论结果。在本文中，我们提出了一种新颖的信息理论方法 - 即，总相关增益最大化（TCGM） - 用于半监督多模态学习，具有以下有前景的特性：（i）它可以有效利用未标记数据点的不同模态之间的信息，以促进每种模态的分类器训练（ii）具有理论保证，可以识别贝叶斯分类器，即所有模态的地面真实后验。具体来说，通过最大化 TC 引起的损失（即 TC 增益）覆盖所有模态的分类器，这些分类器可以合作地发现地面真实分类器的等价类；并通过利用有限百分比的标记数据来识别独特的分类器。我们应用我们的方法，并在各种数据集上取得了最先进的结果，包括新闻组数据集、情感识别（IEMOCAP 和 MOSI）和医学成像（阿尔茨海默病神经影像研究倡议）。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480171.pdf) |
| 2020 | Synthesize then Compare: Detecting Failures and Anomalies for Semantic Segmentation  | Yingda Xia, Yi Zhang, Fengze Liu, Wei Shen, Alan L. Yuille  | The ability to detect failures and anomalies are fundamental requirements for building reliable systems for computer vision applications, especially safety-critical applications of semantic segmentation, such as autonomous driving and medical image analysis. In this paper, we systematically study failure and anomaly detection for semantic segmentation and propose a unified framework, consisting of two modules, to address these two related problems. The first module is an image synthesis module, which generates a synthesized image from a segmentation layout map, and the second is a comparison module, which computes the difference between the synthesized image and the input image. We validate our framework on three challenging datasets and improve the state-of-the-arts by large margins, i.e., 6% AUPR-Error on Cityscapes, 7% Pearson correlation on pancreatic tumor segmentation in MSD and 20% AUPR on StreetHazards anomaly segmentation." | 对于构建可靠的计算机视觉应用系统，尤其是语义分割的安全关键应用，如自动驾驶和医学图像分析，检测故障和异常的能力是基本要求。本文系统研究了语义分割的故障和异常检测，并提出了一个统一的框架，包括两个模块，以解决这两个相关问题。第一个模块是图像合成模块，从分割布局图生成合成图像，第二个是比较模块，计算合成图像与输入图像之间的差异。我们在三个具有挑战性的数据集上验证了我们的框架，并显著改善了现有技术水平，即在Cityscapes上的AUPR-Error提高了6％，在MSD胰腺肿瘤分割上的Pearson相关系数提高了7％，在StreetHazards异常分割上的AUPR提高了20％。 | [link](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460137.pdf) |
