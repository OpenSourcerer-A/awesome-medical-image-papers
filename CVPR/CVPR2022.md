| 年份 | 题目 | 作者 | 摘要 | 中文摘要 | link |
| --- | --- | --- | --- | --- | --- |
| 2022 | Closing the Generalization Gap of Cross-Silo Federated Medical Image Segmentation | An Xu, Wenqi Li, Pengfei Guo, Dong Yang, Holger R. Roth, Ali Hatamizadeh, Can Zhao, Daguang Xu, Heng Huang, Ziyue Xu | Cross-silo federated learning (FL) has attracted much attention in medical imaging analysis with deep learning in recent years as it can resolve the critical issues of insufficient data, data privacy, and training efficiency. However, there can be a generalization gap between the model trained from FL and the one from centralized training. This important issue comes from the non-iid data distribution of the local data in the participating clients and is well-known as client drift. In this work, we propose a novel training framework FedSM to avoid the client drift issue and successfully close the generalization gap compared with the centralized training for medical image segmentation tasks for the first time. We also propose a novel personalized FL objective formulation and a new method SoftPull to solve it in our proposed framework FedSM. We conduct rigorous theoretical analysis to guarantee its convergence for optimizing the non-convex smooth objective function. Real-world medical image segmentation experiments using deep FL validate the motivations and effectiveness of our proposed method. | 近年来，跨领域联邦学习（FL）在医学影像分析中备受关注，因为它可以解决数据不足、数据隐私和训练效率等关键问题。然而，从FL训练的模型和从集中式训练得到的模型之间可能存在泛化差距。这一重要问题源于参与客户端的本地数据的非iid数据分布，被称为客户端漂移。在这项工作中，我们提出了一个新颖的训练框架FedSM，以避免客户端漂移问题，并成功地关闭了与集中式训练相比在医学图像分割任务中的泛化差距。我们还提出了一种新颖的个性化FL目标公式和一种新方法SoftPull来解决在我们提出的框架FedSM中的问题。我们进行了严格的理论分析，以保证其收敛性，以优化非凸光滑目标函数。使用深度FL进行的现实世界医学图像分割实验验证了我们提出的方法的动机和有效性。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Closing_the_Generalization_Gap_of_Cross-Silo_Federated_Medical_Image_Segmentation_CVPR_2022_paper.pdf) |
| 2022 | FIBA: Frequency-Injection Based Backdoor Attack in Medical Image Analysis | Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, Dacheng Tao | In recent years, the security of AI systems has drawn increasing research attention, especially in the medical imaging realm. To develop a secure medical image analysis (MIA) system, it is a must to study possible backdoor attacks (BAs), which can embed hidden malicious behaviors into the system. However, designing a unified BA method that can be applied to various MIA systems is challenging due to the diversity of imaging modalities (e.g., X-Ray, CT, and MRI) and analysis tasks (e.g., classification, detection, and segmentation). Most existing BA methods are designed to attack natural image classification models, which apply spatial triggers to training images and inevitably corrupt the semantics of poisoned pixels, leading to the failures of attacking dense prediction models. To address this issue, we propose a novel Frequency-Injection based Backdoor Attack method (FIBA) that is capable of delivering attacks in various MIA tasks. Specifically, FIBA leverages a trigger function in the frequency domain that can inject the low-frequency information of a trigger image into the poisoned image by linearly combining the spectral amplitude of both images. Since it preserves the semantics of the poisoned image pixels, FIBA can perform attacks on both classification and dense prediction models. Experiments on three benchmarks in MIA (i.e., ISIC-2019 for skin lesion classification, KiTS-19 for kidney tumor segmentation, and EAD-2019 for endoscopic artifact detection), validate the effectiveness of FIBA and its superiority over state-of-the-art methods in attacking MIA models as well as bypassing backdoor defense. The code will be released. | 近年来，人工智能系统的安全性引起了越来越多的研究关注，尤其是在医学影像领域。为了开发一个安全的医学图像分析（MIA）系统，必须研究可能的后门攻击（BAs），这可以将隐藏的恶意行为嵌入到系统中。然而，设计一个统一的BA方法，可以应用于各种不同的MIA系统是具有挑战性的，因为成像模态（例如X射线、CT和MRI）和分析任务（例如分类、检测和分割）的多样性。大多数现有的BA方法都设计用于攻击自然图像分类模型，这些模型对训练图像应用空间触发器，并不可避免地损坏了受污染像素的语义，导致攻击密集预测模型失败。为了解决这个问题，我们提出了一种新颖的基于频率注入的后门攻击方法（FIBA），能够在各种MIA任务中发动攻击。具体来说，FIBA利用频率域中的触发器函数，可以通过线性组合两个图像的频谱幅度，将触发图像的低频信息注入到受污染图像中。由于它保留了受污染图像像素的语义，FIBA可以对分类和密集预测模型进行攻击。在MIA的三个基准测试中进行的实验证实了FIBA的有效性，并证明了它在攻击MIA模型以及绕过后门防御方面优于最先进的方法。代码将会发布。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Feng_FIBA_Frequency-Injection_Based_Backdoor_Attack_in_Medical_Image_Analysis_CVPR_2022_paper.pdf) |
| 2022 | How Much More Data Do I Need? Estimating Requirements for Downstream Tasks | Rafid Mahmood, James Lucas, David Acuna, Daiqing Li, Jonah Philion, Jose M. Alvarez, Zhiding Yu, Sanja Fidler, Marc T. Law | Given a small training data set and a learning algorithm, how much more data is necessary to reach a target validation or test performance? This question is of critical importance in applications such as autonomous driving or medical imaging where collecting data is expensive and time-consuming. Overestimating or underestimating data requirements incurs substantial costs that could be avoided with an adequate budget. Prior work on neural scaling laws suggest that the power-law function can fit the validation performance curve and extrapolate it to larger data set sizes. We find that this does not immediately translate to the more difficult downstream task of estimating the required data set size to meet a target performance. In this work, we consider a broad class of computer vision tasks and systematically investigate a family of functions that generalize the power-law function to allow for better estimation of data requirements. Finally, we show that incorporating a tuned correction factor and collecting over multiple rounds significantly improves the performance of the data estimators. Using our guidelines, practitioners can accurately estimate data requirements of machine learning systems to gain savings in both development time and data acquisition costs. | 给定一个小的训练数据集和一个学习算法，需要多少更多的数据才能达到目标验证或测试性能？这个问题在自动驾驶或医学成像等应用中至关重要，因为收集数据是昂贵且耗时的。高估或低估数据需求会带来可避免的巨大成本。先前关于神经网络规模定律的研究表明，幂律函数可以拟合验证性能曲线，并将其外推到更大的数据集大小。我们发现这并不能立即转化为更困难的下游任务，即估计满足目标性能所需的数据集大小。在这项工作中，我们考虑了广泛的计算机视觉任务，并系统地调查了一类函数族，这些函数将幂律函数推广，以便更好地估计数据需求。最后，我们表明，将一个经过调整的校正因子纳入，并在多个轮次中收集，显著提高了数据估计器的性能。使用我们的指导方针，从业者可以准确估计机器学习系统的数据需求，从而节省开发时间和数据采集成本。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Mahmood_How_Much_More_Data_Do_I_Need_Estimating_Requirements_for_CVPR_2022_paper.pdf) |
| 2022 | Sparse Object-Level Supervision for Instance Segmentation With Pixel Embeddings | Adrian Wolny, Qin Yu, Constantin Pape, Anna Kreshuk | Most state-of-the-art instance segmentation methods have to be trained on densely annotated images. While difficult in general, this requirement is especially daunting for biomedical images, where domain expertise is often required for annotation and no large public data collections are available for pre-training. We propose to address the dense annotation bottleneck by introducing a proposal-free segmentation approach based on non-spatial embeddings, which exploits the structure of the learned embedding space to extract individual instances in a differentiable way. The segmentation loss can then be applied directly to instances and the overall pipeline can be trained in a fully- or weakly supervised manner. We consider the challenging case of positive-unlabeled supervision, where a novel self-supervised consistency loss is introduced for the unlabeled parts of the training data. We evaluate the proposed method on 2D and 3D segmentation problems in different microscopy modalities as well as on the Cityscapes and CVPPP instance segmentation benchmarks, achieving state-of-the-art results on the latter. | 大多数最先进的实例分割方法必须在密集注释的图像上进行训练。虽然一般情况下很困难，但这种要求对于生物医学图像来说尤为艰巨，因为通常需要领域专业知识来进行注释，而没有大型公共数据集可用于预训练。我们提出通过引入基于非空间嵌入的无提案分割方法来解决密集注释瓶颈问题，该方法利用学习的嵌入空间的结构以可微的方式提取单个实例。然后可以直接将分割损失应用于实例，并可以在完全监督或弱监督的方式下对整个流程进行训练。我们考虑了正样本-无标签监督的挑战情况，在训练数据的无标签部分引入了一种新颖的自监督一致性损失。我们在不同显微镜模态的2D和3D分割问题以及Cityscapes和CVPPP实例分割基准上评估了所提出的方法，在后者取得了最先进的结果。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Wolny_Sparse_Object-Level_Supervision_for_Instance_Segmentation_With_Pixel_Embeddings_CVPR_2022_paper.pdf) |
| 2022 | Rethinking Bayesian Deep Learning Methods for Semi-Supervised Volumetric Medical Image Segmentation | Jianfeng Wang, Thomas Lukasiewicz | Recently, several Bayesian deep learning methods have been proposed for semi-supervised medical image segmentation. Although they have achieved promising results on medical benchmarks, some problems are still existing. Firstly, their overall architectures belong to the discriminative models, and hence, in the early stage of training, they only use labeled data for training, which might make them overfit to the labeled data. Secondly, in fact, they are only partially based on Bayesian deep learning, as their overall architectures are not designed under the Bayesian framework. However, unifying the overall architecture under the Bayesian perspective can make the architecture have a rigorous theoretical basis, so that each part of the architecture can have a clear probabilistic interpretation. Therefore, to solve the problems, we propose a new generative Bayesian deep learning (GBDL) architecture. GBDL belongs to the generative models, whose target is to estimate the joint distribution of input medical volumes and their corresponding labels. Estimating the joint distribution implicitly involves the distribution of data, so both labeled and unlabeled data can be utilized in the early stage of training, which alleviates the potential overfitting problem. Besides, GBDL is completely designed under the Bayesian framework, and thus we give its full Bayesian formulation, which lays a theoretical probabilistic foundation for our architecture. Extensive experiments show that our GBDL outperforms previous state-of-the-art methods in terms of four commonly used evaluation indicators on three public medical datasets. | 最近，已经提出了几种贝叶斯深度学习方法用于半监督医学图像分割。尽管它们在医学基准上取得了有希望的结果，但仍然存在一些问题。首先，它们的整体架构属于判别模型，因此在训练的早期阶段，它们只使用标记数据进行训练，这可能会使它们过度拟合于标记数据。其次，事实上，它们只部分基于贝叶斯深度学习，因为它们的整体架构并未设计在贝叶斯框架下。然而，将整体架构统一到贝叶斯视角下可以使架构具有严密的理论基础，以便架构的每个部分都能有清晰的概率解释。因此，为了解决这些问题，我们提出了一种新的生成式贝叶斯深度学习（GBDL）架构。GBDL属于生成模型，其目标是估计输入医学数据体和其对应标签的联合分布。隐式估计联合分布涉及数据的分布，因此在训练的早期阶段可以利用标记和未标记数据，从而缓解潜在的过度拟合问题。此外，GBDL完全设计在贝叶斯框架下，因此我们给出了它的完整贝叶斯公式，为我们的架构奠定了理论概率基础。大量实验证明，我们的GBDL在三个公共医学数据集上的四个常用评估指标方面优于先前的最先进方法。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Rethinking_Bayesian_Deep_Learning_Methods_for_Semi-Supervised_Volumetric_Medical_Image_CVPR_2022_paper.pdf) |
| 2022 | Weakly-Supervised Metric Learning With Cross-Module Communications for the Classification of Anterior Chamber Angle Images | Jingqi Huang, Yue Ning, Dong Nie, Linan Guan, Xiping Jia | As the basis for developing glaucoma treatment strategies, Anterior Chamber Angle (ACA) evaluation is usually dependent on experts' judgements. However, experienced ophthalmologists needed for these judgements are not widely available. Thus, computer-aided ACA evaluations become a pressing and efficient solution for this issue. In this paper, we propose a novel end-to-end framework GCNet for automated Glaucoma Classification based on ACA images or other Glaucoma-related medical images. We first collect and label an ACA image dataset with some pixel-level annotations. Next, we introduce a segmentation module and an embedding module to enhance the performance of classifying ACA images. Within GCNet, we design a Cross-Module Aggregation Net (CMANet) which is a weakly-supervised metric learning network to capture contextual information exchanging across these modules. We conduct experiments on the ACA dataset and two public datasets REFUGE and SIGF. Our experimental results demonstrate that GCNet outperforms several state-of-the-art deep models in the tasks of glaucoma medical image classifications. The source code of GCNet can be found at https://github.com/Jingqi-H/GCNet. | 作为制定青光眼治疗策略的基础，前房角（ACA）评估通常依赖于专家的判断。然而，需要进行这些判断的经验丰富的眼科医生并不是普遍可得的。因此，计算机辅助的ACA评估成为解决这一问题的紧迫且高效的解决方案。在本文中，我们提出了一个新颖的端到端框架GCNet，用于基于ACA图像或其他与青光眼相关的医学图像的自动青光眼分类。我们首先收集并标记了一个带有一些像素级标注的ACA图像数据集。接下来，我们引入了一个分割模块和一个嵌入模块，以提高对ACA图像进行分类的性能。在GCNet中，我们设计了一个跨模块聚合网络（CMANet），这是一个弱监督度量学习网络，用于捕获这些模块之间交换的上下文信息。我们在ACA数据集和两个公共数据集REFUGE和SIGF上进行实验。我们的实验结果表明，在青光眼医学图像分类任务中，GCNet优于几种最先进的深度模型。GCNet的源代码可以在https://github.com/Jingqi-H/GCNet找到。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_Weakly-Supervised_Metric_Learning_With_Cross-Module_Communications_for_the_Classification_of_CVPR_2022_paper.pdf) |
| 2022 | Adaptive Early-Learning Correction for Segmentation From Noisy Annotations | Sheng Liu, Kangning Liu, Weicheng Zhu, Yiqiu Shen, Carlos Fernandez-Granda | Deep learning in the presence of noisy annotations has been studied extensively in classification, but much less in segmentation tasks. In this work, we study the learning dynamics of deep segmentation networks trained on inaccurately-annotated data. We discover a phenomenon that has been previously reported in the context of classification: the networks tend to first fit the clean pixel-level labels during an "early-learning" phase, before eventually memorizing the false annotations. However, in contrast to classification, memorization in segmentation does not arise simultaneously for all semantic categories. Inspired by these findings, we propose a new method for segmentation from noisy annotations with two key elements. First, we detect the beginning of the memorization phase separately for each category during training. This allows us to adaptively correct the noisy annotations in order to exploit early learning. Second, we incorporate a regularization term that enforces consistency across scales to boost robustness against annotation noise. Our method outperforms standard approaches on a medical-imaging segmentation task where noises are synthesized to mimic human annotation errors. It also provides robustness to realistic noisy annotations present in weakly-supervised semantic segmentation, achieving state-of-the-art results on PASCAL VOC 2012. Code is available at https://github.com/Kangningthu/ADELE | 在分类任务中，已经广泛研究了存在嘈杂标注的深度学习，但在分割任务中的研究较少。在这项工作中，我们研究了在不准确标注数据上训练的深度分割网络的学习动态。我们发现了一个在分类环境中先前报道的现象：网络倾向于在“早期学习”阶段首先适应干净的像素级标签，然后最终记忆错误的标注。然而，与分类不同，分割中的记忆并不是同时出现在所有语义类别中。受到这些发现的启发，我们提出了一种新的基于嘈杂标注的分割方法，其中包含两个关键元素。首先，在训练过程中分别检测每个类别的记忆阶段的开始。这使我们能够自适应地纠正嘈杂的标注以利用早期学习。其次，我们加入了一个强制在各个尺度上保持一致性的正则化项，以增强对标注噪声的鲁棒性。我们的方法在医学图像分割任务中表现优越，该任务中合成噪声以模拟人类标注错误。它还能够应对弱监督语义分割中存在的真实嘈杂标注，实现了PASCAL VOC 2012上的最新成果。我们的代码可在https://github.com/Kangningthu/ADELE 上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Adaptive_Early-Learning_Correction_for_Segmentation_From_Noisy_Annotations_CVPR_2022_paper.pdf) |
| 2022 | Efficient Classification of Very Large Images With Tiny Objects | Fanjie Kong, Ricardo Henao | An increasing number of applications in computer vision, specially, in medical imaging and remote sensing, become challenging when the goal is to classify very large images with tiny informative objects. Specifically, these classification tasks face two key challenges: i) the size of the input image is usually in the order of mega- or giga-pixels, however, existing deep architectures do not easily operate on such big images due to memory constraints, consequently, we seek a memory-efficient method to process these images; and ii) only a very small fraction of the input images are informative of the label of interest, resulting in low region of interest (ROI) to image ratio. However, most of the current convolutional neural networks (CNNs) are designed for image classification datasets that have relatively large ROIs and small image sizes (sub-megapixel). Existing approaches have addressed these two challenges in isolation. We present an end-to-end CNN model termed Zoom-In network that leverages hierarchical attention sampling for classification of large images with tiny objects using a single GPU. We evaluate our method on four large-image histopathology, road-scene and satellite imaging datasets, and one gigapixel pathology dataset. Experimental results show that our model achieves higher accuracy than existing methods while requiring less memory resources. | 计算机视觉中的应用越来越多，特别是在医学成像和遥感方面，当目标是对具有微小信息对象的非常大图像进行分类时，变得具有挑战性。具体来说，这些分类任务面临两个关键挑战：i) 输入图像的大小通常在百万或十亿像素的数量级，然而，由于内存限制，现有的深度架构不易处理这样大的图像，因此，我们寻求一种内存高效的方法来处理这些图像；ii) 只有非常小部分的输入图像对所需标签具有信息量，导致感兴趣区域（ROI）与图像比率较低。然而，大多数当前的卷积神经网络（CNNs）是为具有相对较大ROI和较小图像尺寸（次百万像素）的图像分类数据集设计的。现有方法已经分别解决了这两个挑战。我们提出了一种端到端的CNN模型，称为Zoom-In网络，利用分层注意力采样来对具有微小对象的大图像进行分类，仅使用单个GPU。我们在四个大图像组织病理学、道路场景和卫星成像数据集以及一个十亿像素病理学数据集上评估了我们的方法。实验结果表明，我们的模型在需要更少的内存资源的情况下实现了比现有方法更高的准确性。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Kong_Efficient_Classification_of_Very_Large_Images_With_Tiny_Objects_CVPR_2022_paper.pdf) |
| 2022 | A Variational Bayesian Method for Similarity Learning in Non-Rigid Image Registration | Daniel Grzech, Mohammad Farid Azampour, Ben Glocker, Julia Schnabel, Nassir Navab, Bernhard Kainz, LoÃ¯c Le Folgoc | We propose a novel variational Bayesian formulation for diffeomorphic non-rigid registration of medical images, which learns in an unsupervised way a data-specific similarity metric. The proposed framework is general and may be used together with many existing image registration models. We evaluate it on brain MRI scans from the UK Biobank and show that use of the learnt similarity metric, which is parametrised as a neural network, leads to more accurate results than use of traditional functions, e.g. SSD and LCC, to which we initialise the model, without a negative impact on image registration speed or transformation smoothness. In addition, the method estimates the uncertainty associated with the transformation. The code and the trained models are available in a public repository: https://github.com/dgrzech/learnsim. | 我们提出了一种新颖的变分贝叶斯形式，用于医学图像的非刚性变形配准，该方法以无监督的方式学习数据特定的相似度度量。所提出的框架是通用的，可以与许多现有的图像配准模型一起使用。我们在来自英国生物库的脑部MRI扫描上进行了评估，并展示了使用学习到的相似度度量（参数化为神经网络）比使用传统函数（如SSD和LCC）更准确的结果，我们初始化了模型，而不会对图像配准速度或变换平滑性产生负面影响。此外，该方法还估计了与变换相关的不确定性。代码和训练模型可在公共存储库中找到：https://github.com/dgrzech/learnsim。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Grzech_A_Variational_Bayesian_Method_for_Similarity_Learning_in_Non-Rigid_Image_CVPR_2022_paper.pdf) |
| 2022 | Medial Spectral Coordinates for 3D Shape Analysis | Morteza Rezanejad, Mohammad Khodadad, Hamidreza Mahyar, Herve Lombaert, Michael Gruninger, Dirk Walther, Kaleem Siddiqi | In recent years there has been a resurgence of interest in our community in the shape analysis of 3D objects represented by surface meshes, their voxelized interiors, or surface point clouds. In part, this interest has been stimulated by the increased availability of RGBD cameras, and by applications of computer vision to autonomous driving, medical imaging, and robotics. In these settings, spectral coordinates have shown promise for shape representation due to their ability to incorporate both local and global shape properties in a manner that is qualitatively invariant to isometric transformations. Yet, surprisingly, such coordinates have thus far typically considered only local surface positional or derivative information. In the present article, we propose to equip spectral coordinates with medial (object width) information, so as to enrich them. The key idea is to couple surface points that share a medial ball, via the weights of the adjacency matrix. We develop a spectral feature using this idea, and the algorithms to compute it. The incorporation of object width and medial coupling has direct benefits, as illustrated by our experiments on object classification, object part segmentation, and surface point correspondence. | 近年来，我们社区对由表面网格、体素化内部或表面点云表示的3D物体的形状分析再次产生了兴趣。部分地，这种兴趣是由于RGBD相机的增加可用性以及计算机视觉在自动驾驶、医学成像和机器人技术中的应用所激发的。在这些环境中，频谱坐标已经显示出形状表示的潜力，因为它们能够以一种对等距变换具有定性不变性的方式结合局部和全局形状特性。然而，令人惊讶的是，迄今为止，这些坐标通常只考虑局部表面位置或导数信息。在本文中，我们提出为频谱坐标加入中轴（物体宽度）信息，以丰富它们。关键思想是通过邻接矩阵的权重将共享中轴球的表面点进行耦合。我们利用这一思想开发了一种频谱特征以及计算它的算法。物体宽度和中轴耦合的结合具有直接的益处，正如我们在物体分类、物体部分分割和表面点对应的实验中所展示的。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Rezanejad_Medial_Spectral_Coordinates_for_3D_Shape_Analysis_CVPR_2022_paper.pdf) |
| 2022 | CD2-pFed: Cyclic Distillation-Guided Channel Decoupling for Model Personalization in Federated Learning | Yiqing Shen, Yuyin Zhou, Lequan Yu | Federated learning (FL) is a distributed learning paradigm that enables multiple clients to collaboratively learn a shared global model. Despite the recent progress, it remains challenging to deal with heterogeneous data clients, as the discrepant data distributions usually prevent the global model from delivering good generalization ability on each participating client. In this paper, we propose CD^2-pFed, a novel Cyclic Distillation-guided Channel Decoupling framework, to personalize the global model in FL, under various settings of data heterogeneity. Different from previous works which establish layer-wise personalization to overcome the non-IID data across different clients, we make the first attempt at channel-wise assignment for model personalization, referred to as channel decoupling. To further facilitate the collaboration between private and shared weights, we propose a novel cyclic distillation scheme to impose a consistent regularization between the local and global model representations during the federation. Guided by the cyclical distillation, our channel decoupling framework can deliver more accurate and generalized results for different kinds of heterogeneity, such as feature skew, label distribution skew, and concept shift. Comprehensive experiments on four benchmarks, including natural image and medical image analysis tasks, demonstrate the consistent effectiveness of our method on both local and external validations. | 联邦学习（FL）是一种分布式学习范式，使多个客户端能够共同学习一个共享的全局模型。尽管最近取得了进展，但处理异构数据客户端仍然具有挑战性，因为不一致的数据分布通常会阻止全局模型在每个参与客户端上具有良好的泛化能力。在本文中，我们提出了CD^2-pFed，一种新颖的循环蒸馏引导的通道解耦框架，用于个性化FL中的全局模型，在各种数据异质性设置下。与以往建立逐层个性化以克服不同客户端之间的非IID数据的工作不同，我们首次尝试通过通道级分配进行模型个性化，称为通道解耦。为进一步促进私有和共享权重之间的协作，我们提出了一种新颖的循环蒸馏方案，在联邦过程中在本地和全局模型表示之间施加一致的正则化。在循环蒸馏的指导下，我们的通道解耦框架可以为不同类型的异质性提供更准确和广义的结果，例如特征偏差、标签分布偏差和概念转变。对包括自然图像和医学图像分析任务在内的四个基准上的综合实验表明，我们的方法在本地和外部验证中一致有效。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Shen_CD2-pFed_Cyclic_Distillation-Guided_Channel_Decoupling_for_Model_Personalization_in_Federated_CVPR_2022_paper.pdf) |
| 2022 | Robust Equivariant Imaging: A Fully Unsupervised Framework for Learning To Image From Noisy and Partial Measurements | Dongdong Chen, JuliÃ¡n Tachella, Mike E. Davies | Deep networks provide state-of-the-art performance in multiple imaging inverse problems ranging from medical imaging to computational photography. However, most existing networks are trained with clean signals which are often hard or impossible to obtain. Equivariant imaging (EI) is a recent self-supervised learning framework that exploits the group invariance present in signal distributions to learn a reconstruction function from partial measurement data alone. While EI results are impressive, its performance degrades with increasing noise. In this paper, we propose a Robust Equivariant Imaging (REI) framework which can learn to image from noisy partial measurements alone. The proposed method uses Stein's Unbiased Risk Estimator (SURE) to obtain a fully unsupervised training loss that is robust to noise. We show that REI leads to considerable performance gains on linear and nonlinear inverse problems, thereby paving the way for robust unsupervised imaging with deep networks. Code is available at https://github.com/edongdongchen/REI. | 深度网络在从医学影像到计算摄影等多个成像逆问题中提供了最先进的性能。然而，大多数现有网络是使用很难或不可能获得的清洁信号进行训练的。等变成像（EI）是一种最近的自监督学习框架，利用信号分布中存在的群不变性，仅从部分测量数据中学习重构函数。虽然EI的结果令人印象深刻，但随着噪声增加，其性能会下降。在本文中，我们提出了一个鲁棒等变成像（REI）框架，它可以仅从嘈杂的部分测量数据中学习成像。所提出的方法使用Stein的无偏风险估计器（SURE）来获得一个对噪声鲁棒的完全无监督训练损失。我们展示REI在线性和非线性逆问题上带来了显著的性能提升，从而为深度网络的鲁棒无监督成像铺平了道路。代码可在https://github.com/edongdongchen/REI 上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Robust_Equivariant_Imaging_A_Fully_Unsupervised_Framework_for_Learning_To_CVPR_2022_paper.pdf) |
| 2022 | RSCFed: Random Sampling Consensus Federated Semi-Supervised Learning | Xiaoxiao Liang, Yiqun Lin, Huazhu Fu, Lei Zhu, Xiaomeng Li | Federated semi-supervised learning (FSSL) aims to derive a global model by jointly training fully-labeled and fully-unlabeled clients. The existing approaches work well when local clients have independent and identically distributed (IID) data but fail to generalize to a more practical FSSL setting, i.e., Non-IID setting. In this paper, we present a Random Sampling Consensus Federated learning, namely RSCFed, by considering the uneven reliability among models from labeled clients and unlabeled clients. Our key motivation is that given models with large deviations from either labeled clients or unlabeled clients, the consensus could be reached by performing random sup-sampling over clients. To achieve it, instead of directly aggregating local models, we first distill several sub-consensus models by random sub-sampling over clients and then aggregating the sub-consensus models to the global model. To enhance the robustness of sub-consensus models, we also develop a novel distance-reweighted model aggregation method. Experimental results show that our method outperforms state-of-the-art methods on three benchmarked datasets, including both natural images and medical images. | 联邦半监督学习（FSSL）旨在通过联合训练完全标记和完全未标记的客户端来推导全局模型。现有方法在本地客户端具有独立同分布（IID）数据时表现良好，但无法推广到更实用的FSSL设置，即非IID设置。本文提出了一种随机抽样一致性联邦学习方法，即RSCFed，考虑到标记客户端和未标记客户端之间的不均匀可靠性。我们的主要动机是，鉴于标记客户端或未标记客户端的模型存在较大偏差，通过对客户端进行随机子抽样可以实现共识。为了实现这一目标，我们首先通过对客户端进行随机子抽样来提炼出几个子共识模型，然后再将这些子共识模型聚合到全局模型中。为了增强子共识模型的鲁棒性，我们还开发了一种新颖的距离加权模型聚合方法。实验结果表明，我们的方法在三个基准数据集上表现优于现有方法，包括自然图像和医学图像。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_RSCFed_Random_Sampling_Consensus_Federated_Semi-Supervised_Learning_CVPR_2022_paper.pdf) |
| 2022 | Catching Both Gray and Black Swans: Open-Set Supervised Anomaly Detection | Choubo Ding, Guansong Pang, Chunhua Shen | Despite most existing anomaly detection studies assume the availability of normal training samples only, a few labeled anomaly examples are often available in many real-world applications, such as defect samples identified during random quality inspection, lesion images confirmed by radiologists in daily medical screening, etc. These anomaly examples provide valuable knowledge about the application-specific abnormality, enabling significantly improved detection of similar anomalies in some recent models. However, those anomalies seen during training often do not illustrate every possible class of anomaly, rendering these models ineffective in generalizing to unseen anomaly classes. This paper tackles open-set supervised anomaly detection, in which we learn detection models using the anomaly examples with the objective to detect both seen anomalies ('gray swans') and unseen anomalies ('black swans'). We propose a novel approach that learns disentangled representations of abnormalities illustrated by seen anomalies, pseudo anomalies, and latent residual anomalies (i.e., samples that have unusual residuals compared to the normal data in a latent space), with the last two abnormalities designed to detect unseen anomalies. Extensive experiments on nine real-world anomaly detection datasets show superior performance of our model in detecting seen and unseen anomalies under diverse settings. Code and data are available at: https://github.com/choubo/DRA. | 尽管大多数现有的异常检测研究假定只有正常训练样本可用，但在许多现实世界的应用中通常有一些标记的异常示例，比如在随机质量检查中发现的缺陷样本，放射科医生在日常医学筛查中确认的病变图像等。这些异常示例提供了关于特定应用的异常性的宝贵知识，使得一些最近的模型能够显着改善类似异常的检测。然而，在训练过程中看到的这些异常通常并不能说明每一种可能的异常类别，因此这些模型在泛化到未见异常类别方面效果不佳。本文解决了开放式监督异常检测问题，我们使用异常示例学习检测模型，目标是检测已知异常（“灰天鹅”）和未知异常（“黑天鹅”）。我们提出了一种新颖的方法，学习了由已知异常、伪异常和潜在残留异常（即与正常数据在潜在空间中具有异常残差的样本）展示的异常的分解表示，最后两种异常旨在检测未知异常。在九个真实世界的异常检测数据集上进行的大量实验显示，我们的模型在不同设置下能够更好地检测已知和未知异常。代码和数据可在https://github.com/choubo/DRA 上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Catching_Both_Gray_and_Black_Swans_Open-Set_Supervised_Anomaly_Detection_CVPR_2022_paper.pdf) |
| 2022 | What Makes Transfer Learning Work for Medical Images: Feature Reuse & Other Factors | Christos Matsoukas, Johan Fredin Haslum, Moein Sorkhei, Magnus SÃ¶derberg, Kevin Smith | Transfer learning is a standard technique to transfer knowledge from one domain to another. For applications in medical imaging, transfer from ImageNet has become the de-facto approach, despite differences in the tasks and image characteristics between the domains. However, it is unclear what factors determine whether - and to what extent - transfer learning to the medical domain is useful. The long-standing assumption that features from the source domain get reused has recently been called into question. Through a series of experiments on several medical image benchmark datasets, we explore the relationship between transfer learning, data size, the capacity and inductive bias of the model, as well as the distance between the source and target domain. Our findings suggest that transfer learning is beneficial in most cases, and we characterize the important role feature reuse plays in its success. | 迁移学习是一种将知识从一个领域转移到另一个领域的标准技术。在医学影像应用中，尽管不同领域之间的任务和图像特征存在差异，但从ImageNet进行迁移已成为事实上的方法。然而，目前尚不清楚决定迁移学习是否以及在何种程度上对医学领域有用的因素。长期以来，来自源领域的特征被重新使用的假设最近受到质疑。通过在几个医学图像基准数据集上进行一系列实验，我们探讨了迁移学习、数据规模、模型容量和归纳偏好，以及源领域和目标领域之间的距离之间的关系。我们的研究结果表明，在大多数情况下，迁移学习是有益的，并且我们描述了特征重用在其成功中起到的重要作用。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Matsoukas_What_Makes_Transfer_Learning_Work_for_Medical_Images_Feature_Reuse_CVPR_2022_paper.pdf) |
| 2022 | DiRA: Discriminative, Restorative, and Adversarial Learning for Self-Supervised Medical Image Analysis | Fatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher, Michael B. Gotway, Jianming Liang | Discriminative learning, restorative learning, and adversarial learning have proven beneficial for self-supervised learning schemes in computer vision and medical imaging. Existing efforts, however, omit their synergistic effects on each other in a ternary setup, which, we envision, can significantly benefit deep semantic representation learning. To realize this vision, we have developed DiRA, the first framework that unites discriminative, restorative, and adversarial learning in a unified manner to collaboratively glean complementary visual information from unlabeled medical images for fine-grained semantic representation learning. Our extensive experiments demonstrate that DiRA (1) encourages collaborative learning among three learning ingredients, resulting in more generalizable representation across organs, diseases, and modalities; (2) outperforms fully supervised ImageNet models and increases robustness in small data regimes, reducing annotation cost across multiple medical imaging applications; (3) learns fine-grained semantic representation, facilitating accurate lesion localization with only image-level annotation; and (4) enhances state-of-the-art restorative approaches, revealing that DiRA is a general mechanism for united representation learning. All code and pretrained models are available at https://github.com/JLiangLab/DiRA. | 歧视性学习、恢复性学习和对抗性学习已被证明对计算机视觉和医学影像的自监督学习方案有益。然而，现有的努力忽略了它们在三元设置中相互之间的协同作用，我们认为这可以显著有益于深度语义表示学习。为了实现这一愿景，我们开发了DiRA，这是第一个以统一方式将歧视性、恢复性和对抗性学习结合起来的框架，以协同地从未标记的医学图像中获取互补的视觉信息，用于细粒度语义表示学习。我们的广泛实验表明，DiRA（1）鼓励三种学习成分之间的协同学习，导致在器官、疾病和模态之间更具一般化的表示；（2）胜过完全监督的ImageNet模型，并增强小数据情景下的稳健性，降低多个医学影像应用中的注释成本；（3）学习细粒度语义表示，促进仅通过图像级注释实现准确的病变定位；（4）增强了最先进的恢复性方法，揭示了DiRA是统一表示学习的一般机制。所有代码和预训练模型均可在https://github.com/JLiangLab/DiRA 上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Haghighi_DiRA_Discriminative_Restorative_and_Adversarial_Learning_for_Self-Supervised_Medical_Image_CVPR_2022_paper.pdf) |
| 2022 | ContIG: Self-Supervised Multimodal Contrastive Learning for Medical Imaging With Genetics | Aiham Taleb, Matthias Kirchler, Remo Monti, Christoph Lippert | High annotation costs are a substantial bottleneck in applying modern deep learning architectures to clinically relevant medical use cases, substantiating the need for novel algorithms to learn from unlabeled data. In this work, we propose ContIG, a self-supervised method that can learn from large datasets of unlabeled medical images and genetic data. Our approach aligns images and several genetic modalities in the feature space using a contrastive loss. We design our method to integrate multiple modalities of each individual person in the same model end-to-end, even when the available modalities vary across individuals. Our procedure outperforms state-of-the-art self-supervised methods on all evaluated downstream benchmark tasks. We also adapt gradient-based explainability algorithms to better understand the learned cross-modal associations between the images and genetic modalities. Finally, we perform genome-wide association studies on the features learned by our models, uncovering interesting relationships between images and genetic data. | 高昂的注释成本是将现代深度学习架构应用于临床相关医学用例的重要瓶颈，这证实了需要新算法来从未标记的数据中学习。在这项工作中，我们提出了ContIG，这是一种自监督方法，可以从大量未标记的医学图像和基因数据集中学习。我们的方法使用对比损失在特征空间中对齐图像和几种基因模态。我们设计我们的方法来将每个个体的多个模态集成到同一个模型中，即使可用的模态在不同个体之间变化。我们的方法在所有评估的下游基准任务中表现优于最先进的自监督方法。我们还将基于梯度的可解释性算法调整得更好，以更好地理解图像和基因模态之间学到的跨模态关联。最后，我们对我们模型学习到的特征进行全基因组关联研究，揭示了图像和基因数据之间的有趣关系。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Taleb_ContIG_Self-Supervised_Multimodal_Contrastive_Learning_for_Medical_Imaging_With_Genetics_CVPR_2022_paper.pdf) |
| 2022 | Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis | Yucheng Tang, Dong Yang, Wenqi Li, Holger R. Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, Ali Hatamizadeh | Vision Transformers (ViT)s have shown great performance in self-supervised learning of global and local representations that can be transferred to downstream applications. Inspired by these results, we introduce a novel self-supervised learning framework with tailored proxy tasks for medical image analysis. Specifically, we propose: (i) a new 3D transformer-based model, dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for self-supervised pre-training; (ii) tailored proxy tasks for learning the underlying pattern of human anatomy. We demonstrate successful pre-training of the proposed model on 5050 publicly available computed tomography (CT) images from various body organs. The effectiveness of our approach is validated by fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV) Segmentation Challenge with 13 abdominal organs and segmentation tasks from the Medical Segmentation Decathlon (MSD) dataset. Our model is currently the state-of-the-art on the public test leaderboards of both MSD and BTCV datasets. Code: https://monai.io/research/swin-unetr. | 视觉Transformer(ViT)在自监督全局和局部表示学习方面表现出色，这些表示可以转移到下游应用程序中。受到这些结果的启发，我们引入了一种新颖的自监督学习框架，为医学图像分析定制了代理任务。具体来说，我们提出：(i)一种新的基于3D Transformer的模型，名为Swin UNEt TRansformers(Swin UNETR)，具有分层编码器用于自监督预训练；(ii)定制的代理任务，用于学习人体解剖学的潜在模式。我们展示了所提出模型在来自各种身体器官的5050张公开可用计算机断层扫描(CT)图像上的成功预训练。通过在Beyond the Cranial Vault(BTCV)分割挑战赛上对预训练模型进行微调，验证了我们方法的有效性，该挑战包括13个腹部器官和来自医学分割十项赛(MSD)数据集的分割任务。我们的模型目前是MSD和BTCV数据集公开测试排行榜上的最先进模型。 代码：https://monai.io/research/swin-unetr. | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf) |
| 2022 | M3T: Three-Dimensional Medical Image Classifier Using Multi-Plane and Multi-Slice Transformer | Jinseong Jang, Dosik Hwang | In this study, we propose a three-dimensional Medical image classifier using Multi-plane and Multi-slice Transformer (M3T) network to classify Alzheimer's disease (AD) in 3D MRI images. The proposed network synergically combines 3D CNN, 2D CNN, and Transformer for accurate AD classification. The 3D CNN is used to perform natively 3D representation learning, while 2D CNN is used to utilize the pre-trained weights on large 2D databases and 2D representation learning. It is possible to efficiently extract the locality information for AD-related abnormalities in the local brain using CNN networks with inductive bias. The transformer network is also used to obtain attention relationships among multi-plane (axial, coronal, and sagittal) and multi-slice images after CNN. It is also possible to learn the abnormalities distributed over the wider region in the brain using the transformer without inductive bias. In this experiment, we used a training dataset from the Alzheimer's Disease Neuroimaging Initiative (ADNI) which contains a total of 4,786 3D T1-weighted MRI images. For the validation data, we used dataset from three different institutions: The Australian Imaging, Biomarker and Lifestyle Flagship Study of Ageing (AIBL), The Open Access Series of Imaging Studies (OASIS), and some set of ADNI data independent from the training dataset. Our proposed M3T is compared to conventional 3D classification networks based on an area under the curve (AUC) and classification accuracy for AD classification. This study represents that the proposed network M3T achieved the highest performance in multi-institutional validation database, and demonstrates the feasibility of the method to efficiently combine CNN and Transformer for 3D medical images. | 在这项研究中，我们提出了一个使用多平面和多层切片变压器（M3T）网络对3D MRI图像中的阿尔茨海默病（AD）进行分类的三维医学图像分类器。所提出的网络将3D CNN、2D CNN和Transformer结合起来，以实现准确的AD分类。3D CNN用于进行本地3D表示学习，而2D CNN用于利用在大型2D数据库上预训练的权重和2D表示学习。使用带有归纳偏差的CNN网络可以有效地提取出局部大脑中与AD相关的异常信息。变压器网络也用于在CNN之后获取多平面（轴位、冠状位和矢状位）和多层切片图像之间的关注关系。利用变压器可以学习分布在大脑更广泛区域的异常情况，而不需要归纳偏差。在这个实验中，我们使用了来自阿尔茨海默病神经影像学倡议（ADNI）的训练数据集，其中包含4786个3D T1加权MRI图像。对于验证数据，我们使用了来自三个不同机构的数据集：澳大利亚老龄化影像、生物标志物和生活方式旗舰研究（AIBL）、开放获取影像研究系列（OASIS）以及一些独立于训练数据集的ADNI数据。我们提出的M3T与基于曲线下面积（AUC）和AD分类的分类准确度的传统3D分类网络进行了比较。这项研究表明，所提出的网络M3T在多机构验证数据库中取得了最佳性能，并展示了将CNN和Transformer有效结合用于3D医学图像的方法的可行性。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Jang_M3T_Three-Dimensional_Medical_Image_Classifier_Using_Multi-Plane_and_Multi-Slice_Transformer_CVPR_2022_paper.pdf) |
| 2022 | Multimodal Dynamics: Dynamical Fusion for Trustworthy Multimodal Classification | Zongbo Han, Fan Yang, Junzhou Huang, Changqing Zhang, Jianhua Yao | Integration of heterogeneous and high-dimensional data (e.g., multiomics) is becoming increasingly important. Existing multimodal classification algorithms mainly focus on improving performance by exploiting the complementarity from different modalities. However, conventional approaches are basically weak in providing trustworthy multimodal fusion, especially for safety-critical applications (e.g., medical diagnosis). For this issue, we propose a novel trustworthy multimodal classification algorithm termed Multimodal Dynamics, which dynamically evaluates both the feature-level and modality-level informativeness for different samples and thus trustworthily integrates multiple modalities. Specifically, a sparse gating is introduced to capture the information variation of each within-modality feature and the true class probability is employed to assess the classification confidence of each modality. Then a transparent fusion algorithm based on the dynamical informativeness estimation strategy is induced. To the best of our knowledge, this is the first work to jointly model both feature and modality variation for different samples to provide trustworthy fusion in multi-modal classification. Extensive experiments are conducted on multimodal medical classification datasets. In these experiments, superior performance and trustworthiness of our algorithm are clearly validated compared to the state-of-the-art methods. | 异构和高维数据（如多组学）的整合变得越来越重要。现有的多模态分类算法主要集中在通过利用不同模态的互补性来提高性能。然而，传统方法在提供可信的多模态融合方面基本上较弱，特别是对于安全关键应用（如医学诊断）。针对这个问题，我们提出了一种新颖的可信任的多模态分类算法，称为多模态动态，它动态评估不同样本的特征级和模态级信息的可靠性，从而可信地整合多个模态。具体地，引入了稀疏门控来捕捉每个模态内特征的信息变化，并采用真实类概率来评估每个模态的分类置信度。然后基于动态信息估计策略引入了一个透明的融合算法。据我们所知，这是第一项共同对不同样本的特征和模态变化进行建模，以提供多模态分类中可信赖的融合的工作。在多模态医学分类数据集上进行了大量实验。在这些实验中，与最先进的方法相比，我们的算法的优越性能和可信度得到了明显验证。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Multimodal_Dynamics_Dynamical_Fusion_for_Trustworthy_Multimodal_Classification_CVPR_2022_paper.pdf) |
| 2022 | Topology-Preserving Shape Reconstruction and Registration via Neural Diffeomorphic Flow | Shanlin Sun, Kun Han, Deying Kong, Hao Tang, Xiangyi Yan, Xiaohui Xie | Deep Implicit Functions (DIFs) represent 3D geometry with continuous signed distance functions learned through deep neural nets. Recently DIFs-based methods have been proposed to handle shape reconstruction and dense point correspondences simultaneously, capturing semantic relationships across shapes of the same class by learning a DIFs-modeled shape template. These methods provide great flexibility and accuracy in reconstructing 3D shapes and inferring correspondences. However, the point correspondences built from these methods do not intrinsically preserve the topology of the shapes, unlike mesh-based template matching methods. This limits their applications on 3D geometries where underlying topological structures exist and matter, such as anatomical structures in medical images. In this paper, we propose a new model called Neural Diffeomorphic Flow (NDF) to learn deep implicit shape templates, representing shapes as conditional diffeomorphic deformations of templates, intrinsically preserving shape topologies. The diffeomorphic deformation is realized by an auto-decoder consisting of Neural Ordinary Differential Equation (NODE) blocks that progressively map shapes to implicit templates. We conduct extensive experiments on several medical image organ segmentation datasets to evaluate the effectiveness of NDF on reconstructing and aligning shapes. NDF achieves consistently state-of-the-art organ shape reconstruction and registration results in both accuracy and quality. The source code is publicly available at https://github.com/Siwensun/Neural_Diffeomorphic_Flow--NDF. | 深度隐式函数（DIFs）通过深度神经网络学习连续的带符号距离函数来表示3D几何形状。最近，基于DIFs的方法已被提出，用于同时处理形状重建和密集点对应，通过学习DIFs建模的形状模板来捕捉同一类别形状之间的语义关系。这些方法在重建3D形状和推断对应关系方面提供了极大的灵活性和准确性。然而，这些方法构建的点对应关系并不固有地保留形状的拓扑结构，不像基于网格的模板匹配方法。这限制了它们在存在并且重要的3D几何结构，如医学图像中的解剖结构上的应用。在本文中，我们提出了一个称为神经可微流（NDF）的新模型，用于学习深度隐式形状模板，将形状表示为模板的条件可微形变，固有地保留形状拓扑。可微形变通过由神经普通微分方程（NODE）块组成的自动解码器实现，逐渐将形状映射到隐式模板。我们在几个医学图像器官分割数据集上进行了广泛的实验，评估了NDF在重建和对齐形状方面的有效性。NDF在准确性和质量方面均取得了始终如一的最先进的器官形状重建和配准结果。源代码公开可在https://github.com/Siwensun/Neural_Diffeomorphic_Flow--NDF 获取。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Topology-Preserving_Shape_Reconstruction_and_Registration_via_Neural_Diffeomorphic_Flow_CVPR_2022_paper.pdf) |
| 2022 | Aladdin: Joint Atlas Building and Diffeomorphic Registration Learning With Pairwise Alignment | Zhipeng Ding, Marc Niethammer | Atlas building and image registration are important tasks for medical image analysis. Once one or multiple atlases from an image population have been constructed, commonly (1) images are warped into an atlas space to study intra-subject or inter-subject variations or (2) a possibly probabilistic atlas is warped into image space to assign anatomical labels. Atlas estimation and nonparametric transformations are computationally expensive as they usually require numerical optimization. Additionally, previous approaches for atlas building often define similarity measures between a fuzzy atlas and each individual image, which may cause alignment difficulties because a fuzzy atlas does not exhibit clear anatomical structures in contrast to the individual images. This work explores using a convolutional neural network (CNN) to jointly predict the atlas and a stationary velocity field (SVF) parameterization for diffeomorphic image registration with respect to the atlas. Our approach does not require affine pre-registrations and utilizes pairwise image alignment losses to increase registration accuracy. We evaluate our model on 3D knee magnetic resonance images (MRI) from the OAI-ZIB dataset. Our results show that the proposed framework achieves better performance than other state-of-the-art image registration algorithms, allows for end-to-end training, and for fast inference at test time. | Atlas建模和图像配准是医学图像分析中重要的任务。一旦从图像群体中构建了一个或多个Atlas，通常会(1)将图像变形到Atlas空间中以研究个体内或个体间的变化，或者(2)将可能是概率Atlas变形到图像空间中以分配解剖标签。Atlas估计和非参数变换在计算上是昂贵的，因为它们通常需要数值优化。此外，先前的Atlas建模方法通常定义了模糊Atlas与每个个体图像之间的相似性度量，这可能会导致对齐困难，因为与个体图像相比，模糊Atlas不显示清晰的解剖结构。本研究探讨了使用卷积神经网络(CNN)来联合预测Atlas和静止速度场(SVF)参数化，用于关于Atlas的微分同胚图像配准。我们的方法不需要仿射预配准，并利用成对图像对齐损失来提高配准准确性。我们在OAI-ZIB数据集的3D膝关节磁共振图像(MRI)上评估了我们的模型。我们的结果显示，所提出的框架比其他最先进的图像配准算法表现更好，允许端到端训练，并在测试时实现快速推理。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Aladdin_Joint_Atlas_Building_and_Diffeomorphic_Registration_Learning_With_Pairwise_CVPR_2022_paper.pdf) |
| 2022 | SMPL-A: Modeling Person-Specific Deformable Anatomy | Hengtao Guo, Benjamin Planche, Meng Zheng, Srikrishna Karanam, Terrence Chen, Ziyan Wu | A variety of diagnostic and therapeutic protocols rely on locating in vivo target anatomical structures, which can be obtained from medical scans. However, organs move and deform as the patient changes his/her pose. In order to obtain accurate target location information, clinicians have to either conduct frequent intraoperative scans, resulting in higher exposition of patients to radiations, or adopt proxy procedures (e.g., creating and using custom molds to keep patients in the exact same pose during both preoperative organ scanning and subsequent treatment. Such custom proxy methods are typically sub-optimal, constraining the clinicians and costing precious time and money to the patients. To the best of our knowledge, this work is the first to present a learning-based approach to estimate the patient's internal organ deformation for arbitrary human poses in order to assist with radiotherapy and similar medical protocols. The underlying method first leverages medical scans to learn a patient-specific representation that potentially encodes the organ's shape and elastic properties. During inference, given the patient's current body pose information and the organ's representation extracted from previous medical scans, our method can estimate their current organ deformation to offer guidance to clinicians. We conduct experiments on a well-sized dataset which is augmented through real clinical data using finite element modeling. Our results suggest that pose-dependent organ deformation can be learned through a point cloud autoencoder conditioned on the parametric pose input. We hope that this work can be a starting point for future research towards closing the loop between human mesh recovery and anatomical reconstruction, with applications beyond the medical domain. | 一些诊断和治疗协议依赖于定位体内目标解剖结构，这些结构可以通过医学扫描获得。然而，随着患者改变姿势，器官会移动和变形。为了获得准确的目标位置信息，临床医生必须要么进行频繁的术中扫描，导致患者暴露于辐射，要么采用代理程序（例如，创建和使用定制模具来保持患者在术前器官扫描和随后治疗期间保持完全相同的姿势）。这种定制代理方法通常是次优的，限制了临床医生，并给患者带来宝贵的时间和金钱成本。据我们所知，这项工作是第一个提出一种基于学习的方法来估计患者在任意人体姿势下的内部器官变形，以协助放射治疗和类似的医疗协议。基础方法首先利用医学扫描来学习患者特定的表示，潜在地编码器官的形状和弹性特性。在推理过程中，考虑到患者当前的身体姿势信息和从先前医学扫描中提取的器官表示，我们的方法可以估计它们当前的器官变形，以向临床医生提供指导。我们在一个通过有限元建模使用真实临床数据增强的规模适中的数据集上进行实验。我们的结果表明，可以通过受参数姿态输入条件的点云自动编码器来学习姿势相关的器官变形。我们希望这项工作可以成为未来研究的起点，以缩小人体网格恢复和解剖重建之间的联系，适用于医学领域以外的应用。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_SMPL-A_Modeling_Person-Specific_Deformable_Anatomy_CVPR_2022_paper.pdf) |
| 2022 | Affine Medical Image Registration With Coarse-To-Fine Vision Transformer | Tony C. W. Mok, Albert C. S. Chung | Affine registration is indispensable in a comprehensive medical image registration pipeline. However, only a few studies focus on fast and robust affine registration algorithms. Most of these studies utilize convolutional neural networks (CNNs) to learn joint affine and non-parametric registration, while the standalone performance of the affine subnetwork is less explored. Moreover, existing CNN-based affine registration approaches focus either on the local misalignment or the global orientation and position of the input to predict the affine transformation matrix, which are sensitive to spatial initialization and exhibit limited generalizability apart from the training dataset. In this paper, we present a fast and robust learning-based algorithm, Coarse-to-Fine Vision Transformer (C2FViT), for 3D affine medical image registration. Our method naturally leverages the global connectivity and locality of the convolutional vision transformer and the multi-resolution strategy to learn the global affine registration. We evaluate our method on 3D brain atlas registration and template-matching normalization. Comprehensive results demonstrate that our method is superior to the existing CNNs-based affine registration methods in terms of registration accuracy, robustness and generalizability while preserving the runtime advantage of the learning-based methods. The source code is available at https://github.com/cwmok/C2FViT. | 仿射配准在综合医学图像配准流程中是不可或缺的。然而，只有少数研究集中在快速和稳健的仿射配准算法上。大多数这些研究利用卷积神经网络（CNNs）学习联合仿射和非参数配准，而仿射子网络的独立性能较少被探索。此外，现有基于CNN的仿射配准方法要么关注输入的局部错位，要么关注全局方向和位置以预测仿射变换矩阵，这些方法对空间初始化敏感，除训练数据集外具有有限的泛化能力。在本文中，我们提出了一个快速且稳健的基于学习的算法，即粗到细视觉变压器（C2FViT），用于3D仿射医学图像配准。我们的方法自然地利用了卷积视觉变压器的全局连接性和局部性以及多分辨率策略来学习全局仿射配准。我们在3D大脑图谱配准和模板匹配归一化上评估了我们的方法。全面的结果表明，我们的方法在配准精度、稳健性和泛化能力方面优于现有基于CNN的仿射配准方法，同时保持了学习型方法的运行时优势。源代码可在https://github.com/cwmok/C2FViT上获得。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Mok_Affine_Medical_Image_Registration_With_Coarse-To-Fine_Vision_Transformer_CVPR_2022_paper.pdf) |
| 2022 | Harmony: A Generic Unsupervised Approach for Disentangling Semantic Content From Parameterized Transformations | Mostofa Rafid Uddin, Gregory Howe, Xiangrui Zeng, Min Xu | In many real-life image analysis applications, particularly in biomedical research domains, the objects of interest undergo multiple transformations that alters their visual properties while keeping the semantic content unchanged. Disentangling images into semantic content factors and transformations can provide significant benefits into many domain-specific image analysis tasks. To this end, we propose a generic unsupervised framework, Harmony, that simultaneously and explicitly disentangles semantic content from multiple parameterized transformations. Harmony leverages a simple cross-contrastive learning framework with multiple explicitly parameterized latent representations to disentangle content from transformations. To demonstrate the efficacy of Harmony, we apply it to disentangle image semantic content from several parameterized transformations (rotation, translation, scaling, and contrast). Harmony achieves significantly improved disentanglement over the baseline models on several image datasets of diverse domains. With such disentanglement, Harmony is demonstrated to incentivize bioimage analysis research by modeling structural heterogeneity of macromolecules from cryo-ET images and learning transformation-invariant representations of protein particles from single-particle cryo-EM images. Harmony also performs very well in disentangling content from 3D transformations and can perform coarse and fast alignment of 3D cryo-ET subtomograms. Therefore, Harmony is generalizable to many other imaging domains and can potentially be extended to domains beyond imaging as well. | 在许多现实生活中的图像分析应用中，特别是在生物医学研究领域，感兴趣的对象经历多次变换，改变其视觉属性，同时保持语义内容不变。将图像解缠为语义内容因子和变换可以为许多特定领域的图像分析任务带来显著的好处。为此，我们提出了一个通用的无监督框架Harmony，同时明确地解开了来自多个参数化变换的语义内容。Harmony利用简单的交叉对比学习框架和多个明确参数化的潜在表示来解开内容和变换。为了展示Harmony的有效性，我们将其应用于从几种参数化变换（旋转、平移、缩放和对比度）中解开图像的语义内容。Harmony在多个不同领域的图像数据集上实现了显著改进的解缠效果。通过这种解缠，Harmony被证明可以通过对冷冻电子显微成像图像中大分子的结构异质性建模，以及从单颗粒冷冻电子显微成像图像中学习转换不变表示来推动生物图像分析研究。Harmony在解开内容和3D变换方面表现出色，可以对3D冷冻电子显微成像子体进行粗略而快速的对齐。因此，Harmony具有普遍适用于许多其他成像领域，并有可能扩展到超出成像领域的领域。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Uddin_Harmony_A_Generic_Unsupervised_Approach_for_Disentangling_Semantic_Content_From_CVPR_2022_paper.pdf) |
| 2022 | BoostMIS: Boosting Medical Image Semi-Supervised Learning With Adaptive Pseudo Labeling and Informative Active Annotation | Wenqiao Zhang, Lei Zhu, James Hallinan, Shengyu Zhang, Andrew Makmur, Qingpeng Cai, Beng Chin Ooi | In this paper, we propose a novel semi-supervised learning (SSL) framework named BoostMIS that combines adaptive pseudo labeling and informative active annotation to unleash the potential of medical image SSL models: (1) BoostMIS can adaptively leverage the cluster assumption and consistency regularization of the unlabeled data according to the current learning status. This strategy can adaptively generate one-hot "hard" labels converted from task model predictions for better task model training. (2) For the unselected unlabeled images with low confidence, we introduce an Active learning (AL) algorithm to find the informative samples as the annotation candidates by exploiting virtual adversarial perturbation and model's density-aware entropy. These informative candidates are subsequently fed into the next training cycle for better SSL label propagation. Notably, the adaptive pseudo-labeling and informative active annotation form a learning closed-loop that are mutually collaborative to boost medical image SSL. To verify the effectiveness of the proposed method, we collected a metastatic epidural spinal cord compression (MESCC) dataset that aims to optimize MESCC diagnosis and classification for improved specialist referral and treatment. We conducted an extensive experimental study of BoostMIS on MESCC and another public dataset COVIDx. The experimental results verify our framework's effectiveness and generalisability for different medical image datasets with a significant improvement over various state-of-the-art methods. | 在本文中，我们提出了一种名为BoostMIS的新型半监督学习（SSL）框架，结合自适应伪标记和信息性主动标注，释放医学图像SSL模型的潜力：（1）BoostMIS可以根据当前学习状态自适应地利用未标记数据的聚类假设和一致性正则化。该策略可以自适应地生成从任务模型预测转换而来的一对“硬”标签，以更好地训练任务模型。（2）对于置信度较低的未选择未标记图像，我们引入主动学习（AL）算法，通过利用虚拟对抗扰动和模型的密度感知熵来找到信息样本作为标注候选样本。这些信息候选样本随后被输入到下一个训练周期中，以更好地进行SSL标签传播。值得注意的是，自适应伪标记和信息性主动标注形成了一个学习闭环，彼此协作以提升医学图像SSL。为验证所提方法的有效性，我们收集了一个旨在优化椎管外转移性脊髓压迫（MESCC）诊断和分类以改善专科转诊和治疗的数据集。我们在MESCC和另一个公共数据集COVIDx上进行了广泛的实验研究。实验结果验证了我们框架在不同医学图像数据集上的有效性和泛化能力，显著改善了各种最新方法。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_BoostMIS_Boosting_Medical_Image_Semi-Supervised_Learning_With_Adaptive_Pseudo_Labeling_CVPR_2022_paper.pdf) |
| 2022 | Exploring Endogenous Shift for Cross-Domain Detection: A Large-Scale Benchmark and Perturbation Suppression Network | Renshuai Tao, Hainan Li, Tianbo Wang, Yanlu Wei, Yifu Ding, Bowei Jin, Hongping Zhi, Xianglong Liu, Aishan Liu | Existing cross-domain detection methods mostly study the domain shifts where differences between domains are often caused by external environment and perceivable for humans. However, in real-world scenarios (e.g., MRI medical diagnosis, X-ray security inspection), there still exists another type of shift, named endogenous shift, where the differences between domains are mainly caused by the intrinsic factors (e.g., imaging mechanisms, hardware components, etc.), and usually inconspicuous. This shift can also severely harm the cross-domain detection performance but has been rarely studied. To support this study, we contribute the first Endogenous Domain Shift (EDS) benchmark, X-ray security inspection, where the endogenous shifts among the domains are mainly caused by different X-ray machine types with different hardware parameters, wear degrees, etc. EDS consists of 14,219 images including 31,654 common instances from three domains (X-ray machines), with bounding-box annotations from 10 categories. To handle the endogenous shift, we further introduce the Perturbation Suppression Network (PSN), motivated by the fact that this shift is mainly caused by two types of perturbations: category-dependent and category-independent ones. PSN respectively exploits local prototype alignment and global adversarial learning mechanism to suppress these two types of perturbations. The comprehensive evaluation results show that PSN outperforms SOTA methods, serving a new perspective to the cross-domain research community. | 现有的跨领域检测方法主要研究领域转移，其中领域之间的差异通常是由外部环境引起的，并且对人类是可感知的。然而，在现实世界的场景中（例如MRI医学诊断，X光安全检查），仍然存在另一种类型的转移，名为内生转移，其中领域之间的差异主要是由内在因素（例如成像机制，硬件组件等）引起的，通常不显著。这种转移也严重影响跨领域检测性能，但很少被研究。为了支持这项研究，我们提出了第一个内生领域转移（EDS）基准，即X光安全检查，其中领域之间的内生转移主要是由不同X光机型的不同硬件参数、磨损程度等引起的。EDS包含14,219张图像，包括来自三个领域（X光机器）的31,654个常见实例，其中包含来自10个类别的边界框注释。为了处理内生转移，我们进一步引入了扰动抑制网络（PSN），受到这种转移主要由两种类型的扰动引起的事实的启发：类别相关和类别无关的扰动。PSN分别利用局部原型对齐和全局对抗学习机制来抑制这两种类型的扰动。全面的评估结果表明，PSN优于SOTA方法，为跨领域研究社区提供了一个新的视角。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_Exploring_Endogenous_Shift_for_Cross-Domain_Detection_A_Large-Scale_Benchmark_and_CVPR_2022_paper.pdf) |
| 2022 | HyperSegNAS: Bridging One-Shot Neural Architecture Search With 3D Medical Image Segmentation Using HyperNet | Cheng Peng, Andriy Myronenko, Ali Hatamizadeh, Vishwesh Nath, Md Mahfuzur Rahman Siddiquee, Yufan He, Daguang Xu, Rama Chellappa, Dong Yang | Semantic segmentation of 3D medical images is a challenging task due to the high variability of the shape and pattern of objects (such as organs or tumors). Given the recent success of deep learning in medical image segmentation, Neural Architecture Search (NAS) has been introduced to find high-performance 3D segmentation network architectures. However, because of the massive computational requirements of 3D data and the discrete optimization nature of architecture search, previous NAS methods require a long search time or necessary continuous relaxation, and commonly lead to sub-optimal network architectures. While one-shot NAS can potentially address these disadvantages, its application in the segmentation domain has not been well studied in the expansive multi-scale multi-path search space. To enable one-shot NAS for medical image segmentation, our method, named HyperSegNAS, introduces a HyperNet to assist super-net training by incorporating architecture topology information. Such a HyperNet can be removed once the super-net is trained and introduces no overhead during architecture search. We show that HyperSegNAS yields better performing and more intuitive architectures compared to the previous state-of-the-art (SOTA) segmentation networks; furthermore, it can quickly and accurately find good architecture candidates under different computing constraints. Our method is evaluated on public datasets from the Medical Segmentation Decathlon (MSD) challenge, and achieves SOTA performances. | 3D医学图像的语义分割是一个具有挑战性的任务，因为对象（如器官或肿瘤）的形状和模式具有高度的变异性。鉴于深度学习在医学图像分割中取得的最近成功，神经架构搜索（NAS）已被引入以找到高性能的3D分割网络架构。然而，由于3D数据的巨大计算需求以及架构搜索的离散优化特性，先前的NAS方法需要很长的搜索时间或必要的连续放松，并且通常会导致次优网络架构。虽然一次性NAS潜在地可以解决这些缺点，但其在分割领域的应用尚未在广阔的多尺度多路径搜索空间中得到很好的研究。为了实现医学图像分割的一次性NAS，我们的方法命名为HyperSegNAS，引入了一个HyperNet来辅助超网络训练，通过整合架构拓扑信息。这样的HyperNet在超网络训练完成后可以被移除，并且在架构搜索过程中不会增加额外负担。我们展示了HyperSegNAS相对于先前的最先进（SOTA）分割网络产生了表现更好且更直观的架构；此外，它可以快速准确地在不同的计算约束下找到良好的架构候选者。我们的方法在医学分割十项挑战（MSD）的公共数据集上进行评估，并取得了SOTA的表现。. | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Peng_HyperSegNAS_Bridging_One-Shot_Neural_Architecture_Search_With_3D_Medical_Image_CVPR_2022_paper.pdf) |
| 2022 | NODEO: A Neural Ordinary Differential Equation Based Optimization Framework for Deformable Image Registration | Yifan Wu, Tom Z. Jiahao, Jiancong Wang, Paul A. Yushkevich, M. Ani Hsieh, James C. Gee | Deformable image registration (DIR), aiming to find spatial correspondence between images, is one of the most critical problems in the domain of medical image analysis. In this paper, we present a novel, generic, and accurate diffeomorphic image registration framework that utilizes neural ordinary differential equations (NODEs). We model each voxel as a moving particle and consider the set of all voxels in a 3D image as a high-dimensional dynamical system whose trajectory determines the targeted deformation field. Our method leverages deep neural networks for their expressive power in modeling dynamical systems, and simultaneously optimizes for a dynamical system between the image pairs and the corresponding transformation. Our formulation allows various constraints to be imposed along the transformation to maintain desired regularities. Our experiment results show that our method outperforms the benchmarks under various metrics. Additionally, we demonstrate the feasibility to expand our framework to register multiple image sets using a unified form of transformation, which could possibly serve a wider range of applications. | 图像可变形配准（DIR）旨在找到图像之间的空间对应关系，是医学图像分析领域中最关键的问题之一。在本文中，我们提出了一种新颖、通用和准确的使用神经常微分方程（NODEs）的微分同胚图像配准框架。我们将每个体素建模为一个移动粒子，并将3D图像中的所有体素视为一个高维动力系统，其轨迹决定了目标变形场。我们的方法利用深度神经网络在建模动力系统方面的表达力，并同时优化图像对之间和相应变换之间的动力系统。我们的公式允许在变换过程中施加各种约束以保持所需的规律性。我们的实验结果表明，我们的方法在各种指标下优于基准。此外，我们展示了将我们的框架扩展到使用统一形式的变换注册多个图像集合的可行性，这可能为更广泛的应用提供服务。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_NODEO_A_Neural_Ordinary_Differential_Equation_Based_Optimization_Framework_for_CVPR_2022_paper.pdf) |
| 2022 | Which Images To Label for Few-Shot Medical Landmark Detection? | Quan Quan, Qingsong Yao, Jun Li, S. Kevin Zhou | The success of deep learning methods relies on the availability of well-labeled large-scale datasets. However, for medical images, annotating such abundant training data often requires experienced radiologists and consumes their limited time. Few-shot learning is developed to alleviate this burden, which achieves competitive performance with only several labeled data. However, a crucial yet previously overlooked problem in few-shot learning is about the selection of the template images for annotation before learning, which affects the final performance. We herein propose a novel Sample Choosing Policy (SCP) to select "the most worthy" images as the templates, in the context of medical landmark detection. SCP consists of three parts: 1) Self-supervised training for building a pre-trained deep model to extract features from radiological images, 2) Key Point Proposal for localizing informative patches, and 3) Representative Score Estimation for searching most representative samples or templates. The performance of SCP is demonstrated by various experiments on several widely-used public datasets. For one-shot medical landmark detection, the mean radial errors on Cephalometric and HandXray datasets are reduced from 3.595mm to 3.083mm and 4.114mm to 2.653mm, respectively. | 深度学习方法的成功依赖于大规模数据集的可用性。然而，对于医学图像，标注如此丰富的训练数据通常需要经验丰富的放射科医师，并消耗他们有限的时间。少样本学习被开发出来以减轻这种负担，仅使用少量标记数据即可获得竞争性性能。然而，在少样本学习中一个关键但以前被忽视的问题是在学习之前选择模板图像的标注，这会影响最终性能。我们在此提出了一种新颖的样本选择策略（SCP），用于选择“最有价值”的图像作为模板，在医学标志检测的背景下。SCP包括三个部分：1）自监督训练，用于构建一个预训练的深度模型从放射图像中提取特征，2）关键点提议，用于定位信息丰富的补丁，以及3）代表性评分估计，用于搜索最具代表性的样本或模板。SCP的性能通过对几个广泛使用的公共数据集进行各种实验来展示。对于一次性医学标志检测，Cephalometric和HandXray数据集上的平均径向误差从3.595mm降至3.083mm和4.114mm降至2.653mm。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Quan_Which_Images_To_Label_for_Few-Shot_Medical_Landmark_Detection_CVPR_2022_paper.pdf) |
| 2022 | Synthetic Generation of Face Videos With Plethysmograph Physiology | Zhen Wang, Yunhao Ba, Pradyumna Chari, Oyku Deniz Bozkurt, Gianna Brown, Parth Patwa, Niranjan Vaddi, Laleh Jalilian, Achuta Kadambi | Accelerated by telemedicine, advances in Remote Photoplethysmography (rPPG) are beginning to offer a viable path toward non-contact physiological measurement. Unfortunately, the datasets for rPPG are limited as they require videos of the human face paired with ground-truth, synchronized heart rate data from a medical-grade health monitor. Also troubling is that the datasets are not inclusive of diverse populations, i.e., current real rPPG facial video datasets are imbalanced in terms of races or skin tones, leading to accuracy disparities on different demographic groups. This paper proposes a scalable biophysical learning based method to generate physio-realistic synthetic rPPG videos given any reference image and target rPPG signal and shows that it could further improve the state-of-the-art physiological measurement and reduce the bias among different groups. We also collect the largest rPPG dataset of its kind (UCLA-rPPG) with a diverse presence of subject skin tones, in the hope that this could serve as a benchmark dataset for different skin tones in this area and ensure that advances of the technique can benefit all people for healthcare equity. The dataset is available at https://visual.ee.ucla.edu/rppg_avatars.htm/. | 通过远程医疗技术的推动，遥感光电容积脉动图（rPPG）的进展开始为非接触生理测量提供可行的途径。不幸的是，rPPG的数据集受限，因为它们需要人脸视频与医用健康监测器同步心率数据的地面真实数据。另一个令人困扰的问题是，数据集不包括多样化的人口，即当前真实的rPPG人脸视频数据集在种族或肤色方面存在不平衡，导致不同人群之间的准确性差异。本文提出了一种基于可扩展生理学学习的方法，可以根据任何参考图像和目标rPPG信号生成生理逼真的合成rPPG视频，并表明这可以进一步改善最先进的生理测量方法，并减少不同群体之间的偏见。我们还收集了具有不同肤色主体存在的最大规模的rPPG数据集（UCLA-rPPG），希望这可以成为该领域不同肤色的基准数据集，并确保该技术的进步可以使所有人受益于医疗平等。该数据集可在https://visual.ee.ucla.edu/rppg_avatars.htm/ 上获得。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Synthetic_Generation_of_Face_Videos_With_Plethysmograph_Physiology_CVPR_2022_paper.pdf) |
| 2022 | Generalizable Cross-Modality Medical Image Segmentation via Style Augmentation and Dual Normalization | Ziqi Zhou, Lei Qi, Xin Yang, Dong Ni, Yinghuan Shi | For medical image segmentation, imagine if a model was only trained using MR images in source domain, how about its performance to directly segment CT images in target domain? This setting, namely generalizable cross-modality segmentation, owning its clinical potential, is much more challenging than other related settings, e.g., domain adaptation. To achieve this goal, we in this paper propose a novel dual-normalization model by leveraging the augmented source-similar and source-dissimilar images during our generalizable segmentation. To be specific, given a single source domain, aiming to simulate the possible appearance change in unseen target domains, we first utilize a nonlinear transformation to augment source-similar and source-dissimilar images. Then, to sufficiently exploit these two types of augmentations, our proposed dual-normalization based model employs a shared backbone yet independent batch normalization layer for separate normalization. Afterward, we put forward a style-based selection scheme to automatically choose the appropriate path in the test stage. Extensive experiments on three publicly available datasets, i.e., BraTS, Cross-Modality Cardiac, and Abdominal Multi-Organ datasets, have demonstrated that our method outperforms other state-of-the-art domain generalization methods. Code is available at https://github.com/zzzqzhou/Dual-Normalization. | 在医学图像分割中，想象一下如果一个模型仅在源域中使用MR图像进行训练，那么它在直接分割目标域中的CT图像时的表现如何？这种设置，即通用的跨模态分割，具有临床潜力，比其他相关设置（如域自适应）更具挑战性。为了实现这一目标，本文提出了一种新颖的双归一化模型，利用增强的源相似和源不相似图像进行通用分割。具体而言，针对单一源域，旨在模拟未见目标域中可能的外观变化，我们首先利用非线性转换来增强源相似和源不相似图像。然后，为了充分利用这两种增强类型，我们提出的基于双归一化的模型采用共享主干但独立批归一化层进行分开归一化。随后，我们提出了一种基于样式的选择方案，在测试阶段自动选择合适的路径。在三个公开数据集（即BraTS，跨模态心脏和腹部多器官数据集）上进行的大量实验表明，我们的方法优于其他最先进的域泛化方法。代码可在https://github.com/zzzqzhou/Dual-Normalization 上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Generalizable_Cross-Modality_Medical_Image_Segmentation_via_Style_Augmentation_and_Dual_CVPR_2022_paper.pdf) |
| 2022 | VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning | Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny | The limited availability of annotated data often hinders real-world applications of machine learning. To efficiently learn from small quantities of multimodal data, we leverage the linguistic knowledge from a large pre-trained language model (PLM) and quickly adapt it to new domains of image captioning. To effectively utilize a pretrained model, it is critical to balance the visual input and prior linguistic knowledge from pretraining. We propose VisualGPT, which employs a novel self-resurrecting encoder-decoder attention mechanism to quickly adapt the PLM with a small amount of in-domain image-text data. The proposed self-resurrecting activation unit produces sparse activations that prevent accidental overwriting of linguistic knowledge. When trained on 0.1%, 0.5% and 1% of the respective training sets, VisualGPT surpasses the best baseline by up to 10.0% CIDEr on MS COCO and 17.9% CIDEr on Conceptual Captions. Furthermore, VisualGPT achieves the state-of-the-art result on IU X-ray, a medical report generation dataset. Our code is available at https://github.com/Vision-CAIR/VisualGPT. | 有限的标记数据的可用性经常阻碍了机器学习在现实世界中的应用。为了有效地从小量多模态数据中学习，我们利用了一个大型预训练语言模型（PLM）中的语言知识，并快速将其调整到图像字幕的新领域。为了有效利用预训练模型，关键是要平衡来自预训练的视觉输入和先前的语言知识。我们提出了VisualGPT，它采用一种新颖的自复活编码器-解码器注意机制，以小量领域内图像文本数据快速调整PLM。提出的自复活激活单元产生稀疏激活，防止意外覆盖语言知识。当在各自训练集的0.1％、0.5％和1％上训练时，VisualGPT在MS COCO上的CIDEr指标最高超过最佳基准10.0％，在概念字幕上达到17.9％的CIDEr。此外，VisualGPT在IU X射线上实现了最先进的结果，这是一个医疗报告生成数据集。我们的代码可以在https://github.com/Vision-CAIR/VisualGPT 上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_VisualGPT_Data-Efficient_Adaptation_of_Pretrained_Language_Models_for_Image_Captioning_CVPR_2022_paper.pdf) |
| 2022 | Deep Decomposition for Stochastic Normal-Abnormal Transport | Peirong Liu, Yueh Lee, Stephen Aylward, Marc Niethammer | Advection-diffusion equations describe a large family of natural transport processes, e.g., fluid flow, heat transfer, and wind transport. They are also used for optical flow and perfusion imaging computations. We develop a machine learning model, D^2-SONATA, built upon a stochastic advection-diffusion equation, which predicts the velocity and diffusion fields that drive 2D/3D image time-series of transport. In particular, our proposed model incorporates a model of transport atypicality, which isolates abnormal differences between expected normal transport behavior and the observed transport. In a medical context such a normal-abnormal decomposition can be used, for example, to quantify pathologies. Specifically, our model identifies the advection and diffusion contributions from the transport time-series and simultaneously predicts an anomaly value field to provide a decomposition into normal and abnormal advection and diffusion behavior. To achieve improved estimation performance for the velocity and diffusion-tensor fields underlying the advection-diffusion process and for the estimation of the anomaly fields, we create a 2D/3D anomaly-encoded advection-diffusion simulator, which allows for supervised learning. We further apply our model on a brain perfusion dataset from ischemic stroke patients via transfer learning. Extensive comparisons demonstrate that our model successfully distinguishes stroke lesions (abnormal) from normal brain regions, while reconstructing the underlying velocity and diffusion tensor fields. | Advection-diffusion equations描述了一大类自然传输过程，例如流体流动、热传递和风传输。它们还用于光流和灌注成像计算。我们开发了一个机器学习模型，名为D^2-SONATA，基于随机对流扩散方程，可以预测驱动2D/3D图像时间序列传输的速度和扩散场。特别是，我们提出的模型包含了一种传输非典型性模型，它可以分离预期的正常传输行为与观察到的传输之间的异常差异。在医学背景下，这种正常-异常分解可以用来量化病变。具体地，我们的模型识别了传输时间序列中的对流和扩散贡献，并同时预测异常值场，以提供对正常和异常对流和扩散行为的分解。为了提高对支配对流扩散过程的速度和扩散张量场以及异常场的估计性能，我们创建了一个2D/3D异常编码对流扩散模拟器，可以进行监督学习。我们进一步将我们的模型应用于缺血性中风患者的脑灌注数据集，通过迁移学习。广泛的比较表明，我们的模型成功区分了中风病变（异常）和正常脑区域，同时重建了潜在的速度和扩散张量场。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Deep_Decomposition_for_Stochastic_Normal-Abnormal_Transport_CVPR_2022_paper.pdf) |
| 2022 | Temporal Context Matters: Enhancing Single Image Prediction With Disease Progression Representations | Aishik Konwer, Xuan Xu, Joseph Bae, Chao Chen, Prateek Prasanna | Clinical outcome or severity prediction from medical images has largely focused on learning representations from single-timepoint or snapshot scans. It has been shown that disease progression can be better characterized by temporal imaging. We therefore hypothesized that outcome predictions can be improved by utilizing the disease progression information from sequential images. We present a deep learning approach that leverages temporal progression information to improve clinical outcome predictions from single-timepoint images. In our method, a self-attention based Temporal Convolutional Network (TCN) is used to learn a representation that is most reflective of the disease trajectory. Meanwhile, a Vision Transformer is pretrained in a self-supervised fashion to extract features from single-timepoint images. The key contribution is to design a recalibration module that employs maximum mean discrepancy loss (MMD) to align distributions of the above two contextual representations. We train our system to predict clinical outcomes and severity grades from single-timepoint images. Experiments on chest and osteoarthritis radiography datasets demonstrate that our approach outperforms other state-of-the-art techniques. | 临床结果或严重程度预测从医学图像主要集中在从单个时间点或快照扫描中学习表示。已经表明，疾病进展可以通过时间性成像更好地表征。因此，我们假设通过利用连续图像中的疾病进展信息可以改善结果预测。我们提出了一种深度学习方法，利用时间进展信息来改善从单个时间点图像中的临床结果预测。在我们的方法中，使用基于自我注意力的时间卷积网络（TCN）学习最能反映疾病轨迹的表示。同时，一个视觉变换器以自监督方式预训练，从单个时间点图像中提取特征。关键贡献是设计一个重新校准模块，利用最大均值差异损失（MMD）来对齐上述两种上下文表示的分布。我们训练我们的系统从单个时间点图像中预测临床结果和严重等级。对胸部和骨关节炎放射学数据集的实验表明，我们的方法优于其他最先进的技术。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Konwer_Temporal_Context_Matters_Enhancing_Single_Image_Prediction_With_Disease_Progression_CVPR_2022_paper.pdf) |
| 2022 | Contour-Hugging Heatmaps for Landmark Detection | James McCouat, Irina Voiculescu | We propose an effective and easy-to-implement method for simultaneously performing landmark detection in images and obtaining an ingenious uncertainty measurement for each landmark. Uncertainty measurements for landmarks are particularly useful in medical imaging applications: rather than giving an erroneous reading, a landmark detection system is more useful when it flags its level of confidence in its prediction. When an automated system is unsure of its predictions, the accuracy of the results can be further improved manually by a human. In the medical domain, being able to review an automated system's level of certainty significantly improves a clinician's trust in it. This paper obtains landmark predictions with uncertainty measurements using a three stage method: 1) We train our network on one-hot heatmap images, 2) We calibrate the uncertainty of the network using temperature scaling, 3) We calculate a novel statistic called 'Expected Radial Error' to obtain uncertainty measurements. We find that this method not only achieves localisation results on par with other state-of-the-art methods but also an uncertainty score which correlates with the true error for each landmark thereby bringing an overall step change in what a generic computer vision method for landmark detection should be capable of. In addition, we show that our uncertainty measurement can be used to classify, with good accuracy, what landmark predictions are likely to be inaccurate. Code available at: https://github.com/jfm15/ContourHuggingHeatmaps.git | 我们提出了一种有效且易于实现的方法，可以同时在图像中进行地标检测，并为每个地标获取巧妙的不确定性测量。对于地标的不确定性测量在医学影像应用中特别有用：与其给出错误的读数，当地标检测系统标记其对预测的信心水平时，它更有用。当自动化系统对其预测感到不确定时，通过人工进一步改进结果的准确性。在医学领域，能够审查自动化系统的确定性水平显著提高临床医生对其的信任。本文使用三阶段方法获得带有不确定性测量的地标预测：1)我们在单热图像上训练我们的网络，2)我们使用温度缩放校准网络的不确定性，3)我们计算一种称为“预期径向误差”的新统计量来获得不确定性测量。我们发现，这种方法不仅可以实现与其他最先进方法相媲美的定位结果，还可以获得与每个地标的真实误差相关的不确定性评分，从而全面改变了通用计算机视觉方法在地标检测方面应具备的能力。此外，我们展示了我们的不确定性测量可以用于分类哪些地标预测可能不准确，代码可在以下链接找到：https://github.com/jfm15/ContourHuggingHeatmaps.git。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/McCouat_Contour-Hugging_Heatmaps_for_Landmark_Detection_CVPR_2022_paper.pdf) |
| 2022 | Incremental Cross-View Mutual Distillation for Self-Supervised Medical CT Synthesis | Chaowei Fang, Liang Wang, Dingwen Zhang, Jun Xu, Yixuan Yuan, Junwei Han | Due to the constraints of the imaging device and high cost in operation time, computer tomography (CT) scans are usually acquired with low within-slice resolution. Improving the inter-slice resolution is beneficial to the disease diagnosis for both human experts and computer-aided systems. To this end, this paper builds a novel medical slice synthesis to increase the inter-slice resolution. Considering that the ground-truth intermediate medical slices are always absent in clinical practice, we introduce the incremental cross-view mutual distillation strategy to accomplish this task in the self-supervised learning manner. Specifically, we model this problem from three different views: slice-wise interpolation from axial view and pixel-wise interpolation from coronal and sagittal views. Under this circumstance, the models learned from different views can distill valuable knowledge to guide the learning processes of each other. We can repeat this process to make the models synthesize intermediate slice data with increasing between-slice resolution. To demonstrate the effectiveness of the proposed approach, we conduct comprehensive experiments on a large-scale CT dataset. Quantitative and qualitative comparison results show that our method outperforms state-of-the-art algorithms by clear margins. | 由于成像设备的限制和操作时间的高成本，计算机断层扫描（CT）通常具有低切片分辨率。提高切片间分辨率有利于人类专家和计算机辅助系统的疾病诊断。为此，本文建立了一种新颖的医学切片合成方法，以增加切片间分辨率。考虑到在临床实践中缺乏真实的中间医学切片，我们引入了增量交叉视图相互蒸馏策略，以自监督学习的方式完成这项任务。具体而言，我们从三个不同的视图建模这个问题：从轴视图进行逐切片插值，从冠状和矢状视图进行像素级插值。在这种情况下，从不同视图学习的模型可以蒸馏有价值的知识，指导彼此的学习过程。我们可以重复这个过程，使模型合成具有增加切片间分辨率的中间切片数据。为了证明所提方法的有效性，我们在大规模CT数据集上进行了全面的实验。定量和定性比较结果显示，我们的方法在性能上明显优于最先进的算法。. | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Fang_Incremental_Cross-View_Mutual_Distillation_for_Self-Supervised_Medical_CT_Synthesis_CVPR_2022_paper.pdf) |
| 2022 | ACPL: Anti-Curriculum Pseudo-Labelling for Semi-Supervised Medical Image Classification | Fengbei Liu, Yu Tian, Yuanhong Chen, Yuyuan Liu, Vasileios Belagiannis, Gustavo Carneiro | Effective semi-supervised learning (SSL) in medical image analysis (MIA) must address two challenges: 1) work effectively on both multi-class (e.g., lesion classification) and multi-label (e.g., multiple-disease diagnosis) problems, and 2) handle imbalanced learning (because of the high variance in disease prevalence). One strategy to explore in SSL MIA is based on the pseudo labelling strategy, but it has a few shortcomings. Pseudo-labelling has in general lower accuracy than consistency learning, it is not specifically designed for both multi-class and multi-label problems, and it can be challenged by imbalanced learning. In this paper, unlike traditional methods that select confident pseudo label by threshold, we propose a new SSL algorithm, called anti-curriculum pseudo-labelling (ACPL), which introduces novel techniques to select informative unlabelled samples, improving training balance and allowing the model to work for both multi-label and multi-class problems, and to estimate pseudo labels by an accurate ensemble of classifiers (improving pseudo label accuracy). We run extensive experiments to evaluate ACPL on two public medical image classification benchmarks: Chest X-Ray14 for thorax disease multi-label classification and ISIC2018 for skin lesion multi-class classification. Our method outperforms previous SOTA SSL methods on both datasets | 在医学图像分析（MIA）中，有效的半监督学习（SSL）必须解决两个挑战：1）有效处理多类别（例如，病变分类）和多标签（例如，多疾病诊断）问题，2）处理不平衡学习（由于疾病患病率的高方差）。在SSL MIA中探索的一种策略是基于伪标记策略，但它存在一些缺点。伪标记通常比一致性学习的准确率低，它并非专门设计用于多类别和多标签问题，并且可能受到不平衡学习的挑战。本文提出了一种新的SSL算法，称为反课程伪标记（ACPL），与传统方法通过阈值选择自信伪标签不同，ACPL引入了新颖的技术来选择信息量大的无标记样本，改善训练平衡，并使模型适用于多标签和多类别问题，并通过准确的分类器集成来估计伪标签（提高伪标签准确性）。我们进行了大量实验，在两个公共医学图像分类基准上评估了ACPL：用于胸部疾病多标签分类的Chest X-Ray14和用于皮肤病变多类别分类的ISIC2018。我们的方法在两个数据集上均优于先前的SOTA SSL方法。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_ACPL_Anti-Curriculum_Pseudo-Labelling_for_Semi-Supervised_Medical_Image_Classification_CVPR_2022_paper.pdf) |
| 2022 | Cross-Modal Clinical Graph Transformer for Ophthalmic Report Generation | Mingjie Li, Wenjia Cai, Karin Verspoor, Shirui Pan, Xiaodan Liang, Xiaojun Chang | Automatic generation of ophthalmic reports using data-driven neural networks has great potential in clinical practice. When writing a report, ophthalmologists make inferences with prior clinical knowledge. This knowledge has been neglected in prior medical report generation methods. To endow models with the capability of incorporating expert knowledge, we propose a Cross-modal clinical Graph Transformer (CGT) for ophthalmic report generation (ORG), in which clinical relation triples are injected into the visual features as prior knowledge to drive the decoding procedure. However, two major common Knowledge Noise (KN) issues may affect models' effectiveness. 1) Existing general biomedical knowledge bases such as the UMLS may not align meaningfully to the specific context and language of the report, limiting their utility for knowledge injection. 2) Incorporating too much knowledge may divert the visual features from their correct meaning. To overcome these limitations, we design an automatic information extraction scheme based on natural language processing to obtain clinical entities and relations directly from in-domain training reports. Given a set of ophthalmic images, our CGT first restores a sub-graph from the clinical graph and injects the restored triples into visual features. Then visible matrix is employed during the encoding procedure to limit the impact of knowledge. Finally, reports are predicted by the encoded cross-modal features via a Transformer decoder. Extensive experiments on the large-scale FFA-IR benchmark demonstrate that the proposed CGT is able to outperform previous benchmark methods and achieve state-of-the-art performances. | 利用数据驱动的神经网络自动生成眼科报告在临床实践中具有巨大潜力。在撰写报告时，眼科医生根据先前的临床知识进行推断。然而，在先前的医疗报告生成方法中，这种知识经常被忽视。为赋予模型融入专家知识的能力，我们提出了一种用于眼科报告生成（ORG）的跨模态临床图转换器（CGT），其中临床关系三元组被注入到视觉特征中作为先验知识来驱动解码过程。然而，两个主要的普遍的知识噪声（KN）问题可能会影响模型的有效性。1）现有的一般生物医学知识库，如UMLS，可能无法有意义地与报告的特定上下文和语言对齐，从而限制了它们在知识注入方面的效用。2）过多地整合知识可能会使视觉特征偏离其正确含义。为了克服这些限制，我们设计了一个基于自然语言处理的自动信息提取方案，直接从领域内训练报告中获取临床实体和关系。给定一组眼科图像，我们的CGT首先从临床图中恢复一个子图，并将恢复的三元组注入到视觉特征中。然后在编码过程中使用可见矩阵来限制知识的影响。最后，通过Transformer解码器，通过编码的跨模态特征预测报告。大规模FFA-IR基准测试表明，所提出的CGT能够胜过先前的基准方法，并实现最先进的性能。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Cross-Modal_Clinical_Graph_Transformer_for_Ophthalmic_Report_Generation_CVPR_2022_paper.pdf) |
| 2022 | ImplicitAtlas: Learning Deformable Shape Templates in Medical Imaging | Jiancheng Yang, Udaranga Wickramasinghe, Bingbing Ni, Pascal Fua | Deep implicit shape models have become popular in the computer vision community at large but less so for biomedical applications. This is in part because large training databases do not exist and in part because biomedical annotations are often noisy. In this paper, we show that by introducing templates within the deep learning pipeline we can overcome these problems. The proposed framework, named ImplicitAtlas, represents a shape as a deformation field from a learned template field, where multiple templates could be integrated to improve the shape representation capacity at negligible computational cost. Extensive experiments on three medical shape datasets prove the superiority over current implicit representation methods. | 深度隐式形状模型已经在计算机视觉领域变得流行，但在生物医学应用中却较少被采用。部分原因是因为缺乏大型训练数据库，部分原因是因为生物医学注释通常存在噪音。在本文中，我们展示通过在深度学习流程中引入模板，可以克服这些问题。所提出的框架名为ImplicitAtlas，将形状表示为从学习模板场到变形场的变形，其中可以集成多个模板以在可忽略的计算成本下提高形状表示能力。对三个医学形状数据集的大量实验证明了其优越性，超过了当前隐式表示方法。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_ImplicitAtlas_Learning_Deformable_Shape_Templates_in_Medical_Imaging_CVPR_2022_paper.pdf) |
| 2022 | Few-Shot Backdoor Defense Using Shapley Estimation | Jiyang Guan, Zhuozhuo Tu, Ran He, Dacheng Tao | Deep neural networks have achieved impressive performance in a variety of tasks over the last decade, such as autonomous driving, face recognition, and medical diagnosis. However, prior works show that deep neural networks are easily manipulated into specific, attacker-decided behaviors in the inference stage by backdoor attacks which inject malicious small hidden triggers into model training, raising serious security threats. To determine the triggered neurons and protect against backdoor attacks, we exploit Shapley value and develop a new approach called Shapley Pruning (ShapPruning) that successfully mitigates backdoor attacks from models in a data-insufficient situation (1 image per class or even free of data). Considering the interaction between neurons, ShapPruning identifies the few infected neurons (under 1% of all neurons) and manages to protect the model's structure and accuracy after pruning as many infected neurons as possible. To accelerate ShapPruning, we further propose discarding threshold and epsilon-greedy strategy to accelerate Shapley estimation, making it possible to repair poisoned models with only several minutes. Experiments demonstrate the effectiveness and robustness of our method against various attacks and tasks compared to existing methods. | 深度神经网络在过去十年中在各种任务中取得了令人瞩目的表现，如自动驾驶、人脸识别和医学诊断。然而，先前的研究表明，深度神经网络在推断阶段很容易被后门攻击操纵，这种攻击通过向模型训练中注入恶意的小隐藏触发器来决定特定的攻击者行为，提高了严重的安全威胁。为了确定被触发的神经元并防止后门攻击，我们利用Shapley值并开发了一种名为Shapley Pruning（ShapPruning）的新方法，成功地减轻了数据不足情况下（每类1张图像甚至无数据）模型的后门攻击。考虑到神经元之间的相互作用，ShapPruning识别出少数受感染的神经元（不到所有神经元的1%），并设法在修剪尽可能多的受感染神经元后保护模型的结构和准确性。为了加速ShapPruning，我们进一步提出了丢弃阈值和ε-贪心策略，以加速Shapley估计，使修复受污染模型仅需数分钟成为可能。实验证明，与现有方法相比，我们的方法对抗各种攻击和任务的有效性和鲁棒性。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Guan_Few-Shot_Backdoor_Defense_Using_Shapley_Estimation_CVPR_2022_paper.pdf) |
| 2022 | TVConv: Efficient Translation Variant Convolution for Layout-Aware Visual Processing | Jierun Chen, Tianlang He, Weipeng Zhuo, Li Ma, Sangtae Ha, S.-H. Gary Chan | As convolution has empowered many smart applications, dynamic convolution further equips it with the ability to adapt to diverse inputs. However, the static and dynamic convolutions are either layout-agnostic or computation-heavy, making it inappropriate for layout-specific applications, e.g., face recognition and medical image segmentation. We observe that these applications naturally exhibit the characteristics of large intra-image (spatial) variance and small cross-image variance. This observation motivates our efficient translation variant convolution (TVConv) for layout-aware visual processing. Technically, TVConv is composed of affinity maps and a weight-generating block. While affinity maps depict pixel-paired relationships gracefully, the weight-generating block can be explicitly overparameterized for better training while maintaining efficient inference. Although conceptually simple, TVConv significantly improves the efficiency of the convolution and can be readily plugged into various network architectures. Extensive experiments on face recognition show that TVConv reduces the computational cost by up to 3.1x and improves the corresponding throughput by 2.3x while maintaining a high accuracy compared to the depthwise convolution. Moreover, for the same computation cost, we boost the mean accuracy by up to 4.21%. We also conduct experiments on the optic disc/cup segmentation task and obtain better generalization performance, which helps mitigate the critical data scarcity issue. Code is available at https://github.com/JierunChen/TVConv. | 随着卷积技术在许多智能应用中的应用，动态卷积使其具备了适应各种输入的能力。然而，静态和动态卷积都不考虑布局或者计算量大，因此不适用于特定布局的应用，如人脸识别和医学图像分割。我们观察到这些应用自然地表现出图像内部（空间）差异大、图像间差异小的特点。这一观察激发了我们提出的用于布局感知视觉处理的高效转换变体卷积（TVConv）。从技术上讲，TVConv由相似度映射和权重生成块组成。相似度映射优雅地描述了像素配对关系，而权重生成块可以明确地过度参数化以获得更好的训练效果，同时保持高效的推理。尽管在概念上简单，TVConv显著提高了卷积的效率，并可以轻松地插入各种网络架构中。对人脸识别的大量实验表明，TVConv将计算成本降低了最多3.1倍，并将相应的吞吐量提高了2.3倍，同时与深度卷积相比保持了高准确性。此外，对于相同的计算成本，我们将平均准确性提高了最多4.21％。我们还对视盘/杯分割任务进行了实验，获得了更好的泛化性能，有助于缓解关键数据稀缺问题。代码可在https://github.com/JierunChen/TVConv找到。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_TVConv_Efficient_Translation_Variant_Convolution_for_Layout-Aware_Visual_Processing_CVPR_2022_paper.pdf) |
| 2022 | C-CAM: Causal CAM for Weakly Supervised Semantic Segmentation on Medical Image | Zhang Chen, Zhiqiang Tian, Jihua Zhu, Ce Li, Shaoyi Du | Recently, many excellent weakly supervised semantic segmentation (WSSS) works are proposed based on class activation mapping (CAM). However, there are few works that consider the characteristics of medical images. In this paper, we find that there are mainly two challenges of medical images in WSSS: i) the boundary of object foreground and background is not clear; ii) the co-occurrence phenomenon is very severe in training stage. We thus propose a Causal CAM (C-CAM) method to overcome the above challenges. Our method is motivated by two cause-effect chains including category-causality chain and anatomy-causality chain. The category-causality chain represents the image content (cause) affects the category (effect). The anatomy-causality chain represents the anatomical structure (cause) affects the organ segmentation (effect). Extensive experiments were conducted on three public medical image data sets. Our C-CAM generates the best pseudo masks with the DSC of 77.26%, 80.34% and 78.15% on ProMRI, ACDC and CHAOS compared with other CAM-like methods. The pseudo masks of C-CAM are further used to improve the segmentation performance for organ segmentation tasks. Our C-CAM achieves DSC of 83.83% on ProMRI and DSC of 87.54% on ACDC, which outperforms state-of-the-art WSSS methods. Our code is available at https://github.com/Tian-lab/C-CAM. | 最近，许多优秀的基于类激活映射（CAM）的弱监督语义分割（WSSS）工作被提出。然而，在考虑医学图像的特征方面，很少有研究。在本文中，我们发现医学图像在WSSS中主要存在两个挑战：i）对象前景和背景的边界不清晰；ii）训练阶段中共现现象非常严重。因此，我们提出了一种因果CAM（C-CAM）方法来克服上述挑战。我们的方法受到两个因果链的启发，包括类别因果链和解剖因果链。类别因果链表示图像内容（原因）影响类别（结果）。解剖因果链表示解剖结构（原因）影响器官分割（结果）。我们在三个公共医学图像数据集上进行了大量实验。与其他类似CAM方法相比，我们的C-CAM在ProMRI、ACDC和CHAOS上生成了最佳伪掩模，其DSC分别为77.26％、80.34％和78.15％。 C-CAM的伪掩模进一步用于改善器官分割任务的分割性能。我们的C-CAM在ProMRI上实现了83.83％的DSC，在ACDC上实现了87.54％的DSC，优于最先进的WSSS方法。我们的代码可在https://github.com/Tian-lab/C-CAM 上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_C-CAM_Causal_CAM_for_Weakly_Supervised_Semantic_Segmentation_on_Medical_CVPR_2022_paper.pdf) |
| 2022 | CycleMix: A Holistic Strategy for Medical Image Segmentation From Scribble Supervision | Ke Zhang, Xiahai Zhuang | Curating a large set of fully annotated training data can be costly, especially for the tasks of medical image segmentation. Scribble, a weaker form of annotation, is more obtainable in practice, but training segmentation models from limited supervision of scribbles is still challenging. To address the difficulties, we propose a new framework for scribble learning-based medical image segmentation, which is composed of mix augmentation and cycle consistency and thus is referred to as CycleMix. For augmentation of supervision, CycleMix adopts the mixup strategy with a dedicated design of random occlusion, to perform increments and decrements of scribbles. For regularization of supervision, CycleMix intensifies the training objective with consistency losses to penalize inconsistent segmentation, which results in significant improvement of segmentation performance. Results on two open datasets, i.e., ACDC and MSCMRseg, showed that the proposed method achieved exhilarating performance, demonstrating comparable or even better accuracy than the fully-supervised methods. The code and expert-made scribble annotations for MSCMRseg are publicly available at https://github.com/BWGZK/CycleMIx. | 在医学图像分割等任务中，筛选一大批完全注释的训练数据可能成本高昂。在实践中，草图是一种更可获得的注释形式，但是通过有限监督的草图训练分割模型仍然具有挑战性。为解决这些困难，我们提出了一种新的基于草图学习的医学图像分割框架，称为CycleMix，由混合增强和循环一致性组成。为了增强监督，CycleMix采用了mixup策略并设计了随机遮挡，以增加和减少草图。为了规范监督，CycleMix强化了训练目标，使用一致性损失惩罚不一致的分割，从而显著提高了分割性能。在两个开放数据集ACDC和MSCMRseg上的结果表明，所提出的方法取得了令人振奋的表现，展示出与完全监督方法相当甚至更好的准确性。MSCMRseg的代码和专家制作的草图注释可在https://github.com/BWGZK/CycleMIx上公开获取。 | [link](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_CycleMix_A_Holistic_Strategy_for_Medical_Image_Segmentation_From_Scribble_CVPR_2022_paper.pdf) |
