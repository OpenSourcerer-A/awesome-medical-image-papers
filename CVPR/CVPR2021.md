| 年份 | 题目 | 作者 | 摘要 | 中文摘要 | link |
| --- | --- | --- | --- | --- | --- |
| 2021 | Brain Image Synthesis With Unsupervised Multivariate Canonical CSCl4Net | Yawen Huang, Feng Zheng, Danyang Wang, Weilin Huang, Matthew R. Scott, Ling Shao | Recent advances in neuroscience have highlighted the effectiveness of multi-modal medical data for investigating certain pathologies and understanding human cognition. However, obtaining full sets of different modalities is limited by various factors, such as long acquisition times, high examination costs and artifact suppression. In addition, the complexity, high dimensionality and heterogeneity of neuroimaging data remains another key challenge in leveraging existing randomized scans effectively, as data of the same modality is often measured differently by different machines. There is a clear need to go beyond the traditional imaging-dependent process and synthesize anatomically specific target-modality data from a source input. In this paper, we propose to learn dedicated features that cross both intre- and intra-modal variations using a novel CSCl_4Net. Through an initial unification of intra-modal data in the feature maps and multivariate canonical adaptation, CSCl_4Net facilitates feature-level mutual transformation. The positive definite Riemannian manifold-penalized data fidelity term further enables CSCl_4Net to reconstruct missing measurements according to transformed features. Finally, the maximization l_4-norm boils down to a computationally efficient optimization problem. Extensive experiments validate the ability and robustness of our CSCl_4Net compared to the state-of-the-art methods on multiple datasets. | 近年来神经科学的进展突显了多模态医学数据在研究某些病理和理解人类认知方面的有效性。然而，由于各种因素如长时间采集、高检查成本和伪影抑制等限制，获取完整的不同模态数据集受到限制。此外，神经影像数据的复杂性、高维度和异质性仍然是利用现有随机扫描的另一个关键挑战，因为同一模态的数据通常由不同机器以不同方式测量。迫切需要超越传统的依赖于影像的过程，从源输入中合成解剖特定的目标模态数据。在本文中，我们提出使用一种新颖的CSCl_4Net学习跨越模态内部和模态之间变化的专用特征。通过在特征图中对模态内部数据进行初始统一和多变量规范化适应，CSCl_4Net促进了特征级别的相互转换。正定黎曼流形惩罚数据保真度项进一步使CSCl_4Net能够根据转换的特征重建缺失的测量值。最后，最大化l_4-范数简化为一个计算效率高的优化问题。大量实验证实了我们的CSCl_4Net相对于最先进方法在多个数据集上的能力和稳健性。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Brain_Image_Synthesis_With_Unsupervised_Multivariate_Canonical_CSCl4Net_CVPR_2021_paper.pdf) |
| 2021 | Joint Deep Model-Based MR Image and Coil Sensitivity Reconstruction Network (Joint-ICNet) for Fast MRI | Yohan Jun, Hyungseob Shin, Taejoon Eo, Dosik Hwang | Magnetic resonance imaging (MRI) can provide diagnostic information with high-resolution and high-contrast images. However, MRI requires a relatively long scan time compared to other medical imaging techniques, where long scan time might occur patient's discomfort and limit the increase in resolution of magnetic resonance (MR) image. In this study, we propose a Joint Deep Model-based MR Image and Coil Sensitivity Reconstruction Network, called Joint-ICNet, which jointly reconstructs an MR image and coil sensitivity maps from undersampled multi-coil k-space data using deep learning networks combined with MR physical models. Joint-ICNet has two main blocks, where one is an MR image reconstruction block that reconstructs an MR image from undersampled multi-coil k-space data and the other is a coil sensitivity maps reconstruction block that estimates coil sensitivity maps from undersampled multi-coil k-space data. The desired MR image and coil sensitivity maps can be obtained by sequentially estimating them with two blocks based on the unrolled network architecture. To demonstrate the performance of Joint-ICNet, we performed experiments with a fastMRI brain dataset for two reduction factors (R = 4 and 8). With qualitative and quantitative results, we demonstrate that our proposed Joint-ICNet outperforms conventional parallel imaging and deep-learning-based methods in reconstructing MR images from undersampled multi-coil k-space data. | 磁共振成像（MRI）可以提供具有高分辨率和高对比度图像的诊断信息。然而，与其他医学成像技术相比，MRI需要相对较长的扫描时间，长时间的扫描可能会导致患者不适，并限制磁共振（MR）图像分辨率的提高。在本研究中，我们提出了一种基于联合深度模型的MR图像和线圈灵敏度重建网络，称为Joint-ICNet，它利用深度学习网络结合MR物理模型从欠采样的多线圈k空间数据中联合重建MR图像和线圈灵敏度图。Joint-ICNet具有两个主要模块，其中一个是MR图像重建模块，从欠采样的多线圈k空间数据中重建MR图像，另一个是线圈灵敏度图重建模块，从欠采样的多线圈k空间数据中估计线圈灵敏度图。通过基于展开网络架构的两个模块依次估计它们，可以获得所需的MR图像和线圈灵敏度图。为了展示Joint-ICNet的性能，我们在一个快速MRI脑数据集上进行了两个减少因子（R=4和8）的实验。通过定性和定量结果，我们展示了我们提出的Joint-ICNet在从欠采样的多线圈k空间数据中重建MR图像方面优于传统的并行成像和基于深度学习的方法。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Jun_Joint_Deep_Model-Based_MR_Image_and_Coil_Sensitivity_Reconstruction_Network_CVPR_2021_paper.pdf) |
| 2021 | What's in the Image? Explorable Decoding of Compressed Images | Yuval Bahat, Tomer Michaeli | The ever-growing amounts of visual contents captured on a daily basis necessitate the use of lossy compression methods in order to save storage space and transmission bandwidth. While extensive research efforts are devoted to improving compression techniques, every method inevitably discards information. Especially at low bit rates, this information often corresponds to semantically meaningful visual cues, so that decompression involves significant ambiguity. In spite of this fact, existing decompression algorithms typically produce only a single output, and do not allow the viewer to explore the set of images that map to the given compressed code. In this work we propose the first image decompression method to facilitate user-exploration of the diverse set of natural images that could have given rise to the compressed input code, thus granting users the ability to determine what could and what could not have been there in the original scene. Specifically, we develop a novel deep-network based decoder architecture for the ubiquitous JPEG standard, which allows traversing the set of decompressed images that are consistent with the compressed JPEG file. To allow for simple user interaction, we develop a graphical user interface comprising several intuitive exploration tools, including an automatic tool for examining specific solutions of interest. We exemplify our framework on graphical, medical and forensic use cases, demonstrating its wide range of potential applications. | 随着每天捕获的视觉内容数量不断增长，需要使用有损压缩方法来节省存储空间和传输带宽。虽然大量的研究工作致力于改进压缩技术，但每种方法都不可避免地丢弃信息。特别是在低比特率下，这些信息通常对应于语义上有意义的视觉线索，因此解压缩涉及显著的歧义。尽管存在这一事实，现有的解压缩算法通常仅产生单个输出，并且不允许查看者探索映射到给定压缩代码的一组图像。在这项工作中，我们提出了第一种图像解压缩方法，以促进用户探索可能导致压缩输入代码的各种自然图像集，从而赋予用户确定原始场景中可能存在和不存在的能力。具体来说，我们为普遍的JPEG标准开发了一种新颖的基于深度网络的解码器架构，该架构允许遍历与压缩的JPEG文件一致的解压缩图像集。为了实现简单的用户交互，我们开发了一个包含几种直观探索工具的图形用户界面，包括用于检查感兴趣的特定解决方案的自动工具。我们在图形、医学和法医学用例上展示了我们的框架，展示了其广泛的潜在应用。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Bahat_Whats_in_the_Image_Explorable_Decoding_of_Compressed_Images_CVPR_2021_paper.pdf) |
| 2021 | DiNTS: Differentiable Neural Network Topology Search for 3D Medical Image Segmentation | Yufan He, Dong Yang, Holger Roth, Can Zhao, Daguang Xu | Recently, neural architecture search(NAS) has been applied to automatically search high-performance networks for medical image segmentation. The NAS search space usually contains a network topology level(controlling connections among cells with different spatial scales) and a cell level(operations within each cell). Existing methods either require long searching time for large-scale 3D image datasets, or are limited to pre-defined topologies (such as U-shaped or single-path). In this work, we focus on three important aspects of NAS in 3D medical image segmentation: flexible multi-path network topology, high search efficiency, and budgeted GPU memory usage. A novel differentiable search framework is proposed to support fast gradient-based search within a highly flexible network topology search space. The discretization of the searched optimal continuous model in differentiable scheme may produce a sub-optimal final discrete model (discretization gap). Therefore, we propose a topology loss to alleviate this problem. In addition, the GPU memory usage for the searched 3D model is limited with budget constraints during search. Our Differentiable Network Topology Search scheme(DiNTS) is evaluated on the Medical Segmentation Decathlon (MSD) challenge, which contains ten challenging segmentation tasks. Our method achieves the state-of-the-art performance and the top ranking on the MSD challenge leaderboard. | 最近，神经架构搜索（NAS）已被应用于自动搜索医学图像分割的高性能网络。NAS搜索空间通常包含网络拓扑级别（控制不同空间尺度细胞之间的连接）和单元级别（每个单元内的操作）。现有方法要么需要长时间搜索大规模3D图像数据集，要么仅限于预定义的拓扑结构（如U形或单路径）。在这项工作中，我们关注NAS在3D医学图像分割中的三个重要方面：灵活的多路径网络拓扑、高效的搜索效率和有限的GPU内存使用。我们提出了一种新颖的可微分搜索框架，以支持在高度灵活的网络拓扑搜索空间内进行快速基于梯度的搜索。在可微分方案中对搜索到的最优连续模型进行离散化可能会产生次优的最终离散模型（离散化差距）。因此，我们提出了一种拓扑损失来缓解这个问题。此外，在搜索过程中限制了搜索到的3D模型的GPU内存使用，以符合预算约束。我们的可微分网络拓扑搜索方案（DiNTS）在医学分割十项挑战赛（MSD）上进行评估，该挑战包含十个具有挑战性的分割任务。我们的方法在MSD挑战榜上取得了最先进的性能和排名。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/He_DiNTS_Differentiable_Neural_Network_Topology_Search_for_3D_Medical_Image_CVPR_2021_paper.pdf) |
| 2021 | Multiresolution Knowledge Distillation for Anomaly Detection | Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H. Rohban, Hamid R. Rabiee | Unsupervised representation learning has proved to be a critical component of anomaly detection/localization in images. The challenges to learn such a representation are two-fold. Firstly, the sample size is not often large enough to learn a rich generalizable representation through conventional techniques. Secondly, while only normal samples are available at training, the learned features should be discriminative of normal and anomalous samples. Here, we propose to use the "distillation" of features at various layers of an expert network, which is pre-trained on ImageNet, into a simpler cloner network to tackle both issues. We detect and localize anomalies using the discrepancy between the expert and cloner networks' intermediate activation values given an input sample. We show that considering multiple intermediate hints in distillation leads to better exploitation of the expert's knowledge and a more distinctive discrepancy between the two networks, compared to utilizing only the last layer activation values. Notably, previous methods either fail in precise anomaly localization or need expensive region-based training. In contrast, with no need for any special or intensive training procedure, we incorporate interpretability algorithms in our novel framework to localize anomalous regions. Despite the striking difference between some test datasets and ImageNet, we achieve competitive or significantly superior results compared to SOTA on MNIST, F-MNIST, CIFAR-10, MVTecAD, Retinal-OCT, and two other medical datasets on both anomaly detection and localization. | 无监督表示学习已被证明是图像异常检测/定位的关键组成部分。学习这种表示的挑战是双重的。首先，样本大小通常不足以通过传统技术学习到丰富的可泛化表示。其次，在训练时只有正常样本可用，学到的特征应该能够区分正常和异常样本。在这里，我们建议使用在ImageNet上预先训练的专家网络的各个层的特征"蒸馏"到一个更简单的克隆网络中，以解决这两个问题。我们使用专家网络和克隆网络的中间激活值之间的差异来检测和定位异常，给定一个输入样本。我们展示，考虑到蒸馏中的多个中间提示会更好地利用专家的知识，并且会导致两个网络之间的差异更为显著，相比仅利用最后一层的激活值。值得注意的是，先前的方法要么在精确的异常定位上失败，要么需要昂贵的基于区域的训练。相比之下，我们在我们的新框架中结合了可解释性算法，无需任何特殊或繁重的训练程序，以定位异常区域。尽管一些测试数据集与ImageNet之间存在明显差异，但在MNIST、F-MNIST、CIFAR-10、MVTecAD、Retinal-OCT和另外两个医学数据集上，我们在异常检测和定位方面取得了与SOTA相媲美或显著优越的结果。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Salehi_Multiresolution_Knowledge_Distillation_for_Anomaly_Detection_CVPR_2021_paper.pdf) |
| 2021 | Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation | Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou | Automatically generating radiology reports can improve current clinical practice in diagnostic radiology. On one hand, it can relieve radiologists from the heavy burden of report writing; On the other hand, it can remind radiologists of abnormalities and avoid the misdiagnosis and missed diagnosis. Yet, this task remains a challenging job for data-driven neural networks, due to the serious visual and textual data biases. To this end, we propose a Posterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to imitate the working patterns of radiologists, who will first examine the abnormal regions and assign the disease topic tags to the abnormal regions, and then rely on the years of prior medical knowledge and prior working experience accumulations to write reports. Thus, the PPKED includes three modules: Posterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and Multi-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior knowledge, which provides explicit abnormal visual regions to alleviate visual data bias; PrKE explores the prior knowledge from the prior medical knowledge graph (medical knowledge) and prior radiology reports (working experience) to alleviate textual data bias. The explored knowledge is distilled by the MKD to generate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our method is able to outperform previous state-of-the-art models on these two datasets. | 自动生成放射学报告可以改善当前诊断放射学的临床实践。一方面，它可以减轻放射科医生撰写报告的沉重负担；另一方面，它可以提醒放射科医生异常情况，并避免误诊和漏诊。然而，由于严重的视觉和文本数据偏差，这项任务仍然是数据驱动的神经网络的一项具有挑战性的工作。为此，我们提出了一种后验和先验知识探索和精炼方法（PPKED），来模仿放射科医生的工作模式，他们将首先检查异常区域并为异常区域分配疾病主题标签，然后依靠多年的先前医学知识和工作经验积累来撰写报告。因此，PPKED包括三个模块：后验知识探索器（PoKE），先验知识探索器（PrKE）和多领域知识精炼器（MKD）。具体而言，PoKE探索后验知识，提供明确的异常视觉区域以减轻视觉数据偏差；PrKE从先前医学知识图（医学知识）和先前放射学报告（工作经验）中探索先验知识，以减轻文本数据偏差。探索的知识由MKD精炼，生成最终报告。在MIMIC-CXR和IU-Xray数据集上进行评估，我们的方法能够在这两个数据集上胜过先前的最先进模型。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Exploring_and_Distilling_Posterior_and_Prior_Knowledge_for_Radiology_Report_CVPR_2021_paper.pdf) |
| 2021 | Causal Hidden Markov Model for Time Series Disease Forecasting | Jing Li, Botong Wu, Xinwei Sun, Yizhou Wang | We propose a causal hidden Markov model to achieve robust prediction of irreversible disease at an early stage, which is safety-critical and vital for medical treatment in early stages. Specifically, we introduce the hidden variables which propagate to generate medical data at each time step. To avoid learning spurious correlation (e.g., confounding bias), we explicitly separate these hidden variables into three parts: a) the disease (clinical)-related part; b) the disease (non-clinical)-related part; c) others, with only a),b) causally related to the disease however c) may contain spurious correlations (with the disease) inherited from the data provided. With personal attributes and disease label respectively provided as side information and supervision, we prove that these disease-related hidden variables can be disentangled from others, implying the avoidance of spurious correlation for generalization to medical data from other (out-of-) distributions. Guaranteed by this result, we propose a sequential variational auto-encoder with a reformulated objective function. We apply our model to the early prediction of peripapillary atrophy and achieve promising results on out-of-distribution test data. Further, the ablation study empirically shows the effectiveness of each component in our method. And the visualization shows the accurate identification of lesion regions from others. | 我们提出了一种因果隐藏马尔可夫模型，以实现对不可逆疾病在早期阶段的稳健预测，这对于早期医疗治疗而言至关重要且具有安全性。具体而言，我们引入了隐藏变量，这些变量在每个时间步长生成医疗数据。为了避免学习虚假相关性（例如，混淆偏差），我们明确将这些隐藏变量分为三部分：a）与疾病（临床）相关的部分；b）与疾病（非临床）相关的部分；c）其他部分，仅a）、b）与疾病有因果关系，然而c）可能包含继承自所提供数据的与疾病相关的虚假相关性。在个人属性和疾病标签分别作为辅助信息和监督的情况下，我们证明这些与疾病相关的隐藏变量可以与其他变量解耦，这意味着避免虚假相关性对来自其他（分布之外的）医疗数据的泛化。通过这一结果的保证，我们提出了一个具有重构目标函数的顺序变分自动编码器。我们将我们的模型应用于早期预测视盘周围萎缩，并在分布外测试数据上取得了令人满意的结果。此外，消融研究在实证上显示了我们方法中每个组件的有效性。可视化显示了对病灶区域的准确识别。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Causal_Hidden_Markov_Model_for_Time_Series_Disease_Forecasting_CVPR_2021_paper.pdf) |
| 2021 | Learning Calibrated Medical Image Segmentation via Multi-Rater Agreement Modeling | Wei Ji, Shuang Yu, Junde Wu, Kai Ma, Cheng Bian, Qi Bi, Jingjing Li, Hanruo Liu, Li Cheng, Yefeng Zheng | In medical image analysis, it is typical to collect multiple annotations, each from a different clinical expert or rater, in the expectation that possible diagnostic errors could be mitigated. Meanwhile, from the computer vision practitioner viewpoint, it has been a common practice to adopt the ground-truth obtained via either the majority-vote or simply one annotation from a preferred rater. This process, however, tends to overlook the rich information of agreement or disagreement ingrained in the raw multi-rater annotations. To address this issue, we propose to explicitly model the multi-rater (dis-)agreement, dubbed MRNet, which has two main contributions. First, an expertise-aware inferring module or EIM is devised to embed the expertise level of individual raters as prior knowledge, to form high-level semantic features. Second, our approach is capable of reconstructing multi-rater gradings from coarse predictions, with the multi-rater (dis-)agreement cues being further exploited to improve the segmentation performance. To our knowledge, our work is the first in producing calibrated predictions under different expertise levels for medical image segmentation. Extensive empirical experiments are conducted across five medical segmentation tasks of diverse imaging modalities. In these experiments, superior performance of our MRNet is observed comparing to the state-of-the-arts, indicating the effectiveness and applicability of our MRNet toward a wide range of medical segmentation tasks. | 在医学图像分析中，通常会收集多个注释，每个注释来自不同的临床专家或评分者，期望能够减轻可能的诊断错误。与此同时，从计算机视觉从业者的角度来看，通常会采用通过多数投票或仅从首选评分者获取的基准数据。然而，这个过程往往忽视了蕴含在原始多评分者注释中的协议或不一致的丰富信息。为了解决这个问题，我们提出了显式建模多评分者（不）一致性的MRNet模型，其主要贡献有两点。首先，我们设计了一个专家意识推断模块（EIM），将个体评分者的专业水平作为先验知识嵌入，形成高级语义特征。其次，我们的方法能够从粗糙预测中重新构建多评分者的评分，进一步利用多评分者的（不）一致性线索来提高分割性能。据我们所知，我们的工作是第一个针对医学图像分割在不同专业水平下产生校准预测的方法。我们在不同成像模态的五个医学分割任务中进行了大量实证实验。在这些实验中，我们观察到我们的MRNet相对于现有技术表现出更优异的性能，表明了我们的MRNet在各种医学分割任务中的有效性和适用性。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Learning_Calibrated_Medical_Image_Segmentation_via_Multi-Rater_Agreement_Modeling_CVPR_2021_paper.pdf) |
| 2021 | DeepTag: An Unsupervised Deep Learning Method for Motion Tracking on Cardiac Tagging Magnetic Resonance Images | Meng Ye, Mikael Kanski, Dong Yang, Qi Chang, Zhennan Yan, Qiaoying Huang, Leon Axel, Dimitris Metaxas | Cardiac tagging magnetic resonance imaging (t-MRI) is the gold standard for regional myocardium deformation and cardiac strain estimation. However, this technique has not been widely used in clinical diagnosis, as a result of the difficulty of motion tracking encountered with t-MRI images. In this paper, we propose a novel deep learning-based fully unsupervised method for in vivo motion tracking on t-MRI images. We first estimate the motion field (INF) between any two consecutive t-MRI frames by a bi-directional generative diffeomorphic registration neural network. Using this result, we then estimate the Lagrangian motion field between the reference frame and any other frame through a differentiable composition layer. By utilizing temporal information to perform reasonable estimations on spatio-temporal motion fields, this novel method provides a useful solution for motion tracking and image registration in dynamic medical imaging. Our method has been validated on a representative clinical t-MRI dataset; the experimental results show that our method is superior to conventional motion tracking methods in terms of landmark tracking accuracy and inference efficiency. | 心脏标记磁共振成像（t-MRI）是区域心肌变形和心脏应变估计的金标准。然而，由于t-MRI图像中遇到的运动跟踪困难，这种技术在临床诊断中并没有被广泛使用。在本文中，我们提出了一种基于深度学习的全面无监督方法，用于t-MRI图像的体内运动跟踪。我们首先通过双向生成可微分注册神经网络估计任意两个连续t-MRI帧之间的运动场（INF）。利用这一结果，我们通过一个可微分组合层估计参考帧和任何其他帧之间的拉格朗日运动场。通过利用时间信息对时空运动场进行合理估计，这种新颖方法为动态医学成像中的运动跟踪和图像配准提供了一个有用的解决方案。我们的方法已在代表性临床t-MRI数据集上进行了验证；实验结果表明，我们的方法在地标跟踪准确性和推断效率方面优于传统的运动跟踪方法。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Ye_DeepTag_An_Unsupervised_Deep_Learning_Method_for_Motion_Tracking_on_CVPR_2021_paper.pdf) |
| 2021 | Semantic Segmentation With Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization | Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, Sanja Fidler | Training deep networks with limited labeled data while achieving a strong generalization ability is key in the quest to reduce human annotation efforts. This is the goal of semi-supervised learning, which exploits more widely available unlabeled data to complement small labeled data sets. In this paper, we propose a novel framework for discriminative pixel-level tasks using a generative model of both images and labels. Concretely, we learn a generative adversarial network that captures the joint image-label distribution and is trained efficiently using a large set of unlabeled images supplemented with only few labeled ones. We build our architecture on top of StyleGAN2, augmented with a label synthesis branch. Image labeling at test time is achieved by first embedding the target image into the joint latent space via an encoder network and test-time optimization, and then generating the label from the inferred embedding. We evaluate our approach in two important domains: medical image segmentation and part-based face segmentation. We demonstrate strong in-domain performance compared to several baselines, and are the first to showcase extreme out-of-domain generalization, such as transferring from CT to MRI in medical imaging, and photographs of real faces to paintings, sculptures, and even cartoons and animal faces. | 在努力减少人工标注工作的过程中，训练具有有限标记数据的深度网络并实现强大的泛化能力至关重要。这是半监督学习的目标，利用更广泛可用的未标记数据来补充少量标记数据集。在本文中，我们提出了一个新颖的框架，用于使用图像和标签的生成模型进行判别像素级任务。具体而言，我们学习了一个能够捕捉联合图像-标签分布的生成对抗网络，并使用大量未标记图像有效训练，仅辅以少量标记图像。我们基于StyleGAN2构建了我们的架构，并增加了一个标签合成分支。在测试时，通过编码器网络和测试时优化将目标图像嵌入联合潜在空间，然后从推断的嵌入中生成标签。我们在两个重要领域评估了我们的方法：医学图像分割和基于部位的人脸分割。与几种基线方法相比，我们展示了强大的领域内性能，并且首次展示了极端的领域外泛化能力，例如从CT到MRI在医学成像中的转移，以及从真实面部照片到绘画、雕塑，甚至卡通和动物面孔的转移。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Semantic_Segmentation_With_Generative_Models_Semi-Supervised_Learning_and_Strong_Out-of-Domain_CVPR_2021_paper.pdf) |
| 2021 | Every Annotation Counts: Multi-Label Deep Supervision for Medical Image Segmentation | Simon Reiss, Constantin Seibold, Alexander Freytag, Erik Rodner, Rainer Stiefelhagen | Pixel-wise segmentation is one of the most data and annotation hungry tasks in our field. Providing representative and accurate annotations is often mission-critical especially for challenging medical applications. In this paper, we propose a semi-weakly supervised segmentation algorithm to overcome this barrier. Our approach is based on a new formulation of deep supervision and student-teacher model and allows for easy integration of different supervision signals. In contrast to previous work, we show that care has to be taken how deep supervision is integrated in lower layers and we present multi-label deep supervision as the most important secret ingredient for success. With our novel training regime for segmentation that flexibly makes use of images that are either fully labeled, marked with bounding boxes, just global labels, or not at all, we are able to cut the requirement for expensive labels by 94.22% - narrowing the gap to the best fully supervised baseline to only 5% mean IoU. Our approach is validated by extensive experiments on retinal fluid segmentation and we provide an in-depth analysis of the anticipated effect each annotation type can have in boosting segmentation performance. | 像素级分割是我们领域中最需要数据和注释的任务之一。提供具有代表性和准确性的注释通常至关重要，特别是对于具有挑战性的医学应用。在本文中，我们提出了一种半弱监督分割算法来克服这一障碍。我们的方法基于深度监督和师生模型的新形式，并允许轻松集成不同的监督信号。与先前的工作相比，我们表明需要注意如何将深度监督集成在较低层中，并提出多标签深度监督是成功的最重要秘密成分。通过我们的新颖分割训练方案，灵活地利用完全标记的图像、标有边界框的图像、全局标签的图像或根本没有标签的图像，我们能够将昂贵标签的要求减少94.22% - 将与最佳完全监督基准之间的差距缩小到仅5%的平均IoU。我们的方法通过对视网膜液体分割的广泛实验进行验证，并提供了深入分析每种注释类型可能对提高分割性能的预期效果。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Reiss_Every_Annotation_Counts_Multi-Label_Deep_Supervision_for_Medical_Image_Segmentation_CVPR_2021_paper.pdf) |
| 2021 | Metadata Normalization | Mandy Lu, Qingyu Zhao, Jiequan Zhang, Kilian M. Pohl, Li Fei-Fei, Juan Carlos Niebles, Ehsan Adeli | Batch Normalization (BN) and its variants have delivered tremendous success in combating the covariate shift induced by the training step of deep learning methods. While these techniques normalize the feature distribution by standardizing with batch statistics, they do not correct the influence on features from extraneous variables or multiple distributions. Such extra variables, referred to as metadata here, may create bias or confounding effects (e.g., race when classifying gender from face images). We introduce the Metadata Normalization (MDN) layer, a new batch-level operation which can be used end-to-end within the training framework, to correct the influence of metadata on the feature distribution. MDN adopts a regression analysis technique traditionally used for preprocessing to remove (regress out) the metadata effects on model features during training. We utilize a metric based on distance correlation to quantify the distribution bias from the metadata and demonstrate that our method successfully removes metadata effects on four diverse settings: one synthetic, one 2D image, one video, and one 3D medical image dataset. | 批量归一化（BN）及其变体在对抗深度学习方法训练过程中引起的协变量偏移方面取得了巨大成功。虽然这些技术通过使用批次统计数据标准化来归一化特征分布，但它们并未纠正来自外部变量或多个分布的特征影响。这些额外变量在此处被称为元数据，可能会产生偏见或混杂效应（例如，在从面部图像中对性别进行分类时，可能会受到种族的影响）。我们引入元数据归一化（MDN）层，这是一种新的批次级操作，可以在训练框架内端到端地使用，以纠正元数据对特征分布的影响。MDN采用传统用于预处理的回归分析技术，以在训练过程中移除（回归出）模型特征上的元数据效应。我们利用基于距离相关性的度量来量化来自元数据的分布偏差，并展示我们的方法成功消除了四种不同设置上的元数据效应：一个合成数据集，一个2D图像数据集，一个视频数据集和一个3D医学图像数据集。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Lu_Metadata_Normalization_CVPR_2021_paper.pdf) |
| 2021 | Passive Inter-Photon Imaging | Atul Ingle, Trevor Seets, Mauro Buttafava, Shantanu Gupta, Alberto Tosi, Mohit Gupta, Andreas Velten | Digital camera pixels measure image intensities by converting incident light energy into an analog electrical current, and then digitizing it into a fixed-width binary representation. This direct measurement method, while conceptually simple, suffers from limited dynamic range and poor performance under extreme illumination --- electronic noise dominates under low illumination, and pixel full-well capacity results in saturation under bright illumination. We propose a novel intensity cue based on measuring inter-photon timing, defined as the time delay between detection of successive photons. Based on the statistics of inter-photon times measured by a time-resolved single-photon sensor, we develop theory and algorithms for a scene brightness estimator which works over extreme dynamic range; we experimentally demonstrate imaging scenes with a dynamic range of over ten million to one. The proposed techniques, aided by the emergence of single-photon sensors such as single-photon avalanche diodes (SPADs) with picosecond timing resolution, will have implications for a wide range of imaging applications: robotics, consumer photography, astronomy, microscopy and biomedical imaging. | 数字相机像素通过将入射光能量转换为模拟电流，然后将其数字化为固定宽度的二进制表示来测量图像强度。这种直接测量方法虽然在概念上简单，但在极端照明下性能有限，动态范围有限，表现不佳---在低照明下，电子噪声占主导地位，在明亮照明下，像素满充容量导致饱和。我们提出了一种基于测量光子间时序的新型强度线索，定义为连续光子检测之间的时间延迟。基于时间分辨单光子传感器测量的光子间时序统计，我们为一个在极端动态范围内工作的场景亮度估计器开发了理论和算法；我们在实验中展示了具有超过一千万对一的动态范围的图像场景。提出的技术，借助单光子传感器的出现，如皮秒定时分辨率的单光子雪崩二极管（SPADs），将对广泛的成像应用产生影响：机器人技术、消费者摄影、天文学、显微镜和生物医学成像。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Ingle_Passive_Inter-Photon_Imaging_CVPR_2021_paper.pdf) |
| 2021 | Progressive Semantic Segmentation | Chuong Huynh, Anh Tuan Tran, Khoa Luu, Minh Hoai | The objective of this work is to segment high-resolution images without overloading GPU memory usage or losing the fine details in the output segmentation map. The memory constraint means that we must either downsample the big image or divide the image into local patches for separate processing. However, the former approach would lose the fine details, while the latter can be ambiguous due to the lack of a global picture. In this work, we present MagNet, a multi-scale framework that resolves local ambiguity by looking at the image at multiple magnification levels. MagNet has multiple processing stages, where each stage corresponds to a magnification level, and the output of one stage is fed into the next stage for coarse-to-fine information propagation. Each stage analyzes the image at a higher resolution than the previous stage, recovering the previously lost details due to the lossy downsampling step, and the segmentation output is progressively refined through the processing stages. Experiments on three high-resolution datasets of urban views, aerial scenes, and medical images shows that MagNet consistently outperforms the state-of-the-art methods by a significant margin. | 本文的目标是在不过度使用GPU内存或丢失输出分割图中的细节的情况下对高分辨率图像进行分割。内存限制意味着我们必须对大图像进行降采样或将图像分割成局部补丁以进行分开处理。然而，前一种方法会丢失细节，而后一种方法由于缺乏全局图像可能会产生歧义。在这项工作中，我们提出了MagNet，这是一个多尺度框架，通过查看多个放大级别的图像来解决局部歧义。MagNet具有多个处理阶段，其中每个阶段对应一个放大级别，一个阶段的输出被馈送到下一个阶段以进行粗到精的信息传播。每个阶段分析的图像分辨率比上一个阶段更高，通过处理阶段逐渐恢复由于有损降采样步骤而丢失的细节，并且分割输出通过处理阶段逐渐得到改进。对城市景观、航空场景和医学图像的三个高分辨率数据集的实验表明，MagNet始终以显著较大的优势优于现有方法。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Huynh_Progressive_Semantic_Segmentation_CVPR_2021_paper.pdf) |
| 2021 | DARCNN: Domain Adaptive Region-Based Convolutional Neural Network for Unsupervised Instance Segmentation in Biomedical Images | Joy Hsu, Wah Chiu, Serena Yeung | In the biomedical domain, there is an abundance of dense, complex data where objects of interest may be challenging to detect or constrained by limits of human knowledge. Labelled domain specific datasets for supervised tasks are often expensive to obtain, and furthermore discovery of novel distinct objects may be desirable for unbiased scientific discovery. Therefore, we propose leveraging the wealth of annotations in benchmark computer vision datasets to conduct unsupervised instance segmentation for diverse biomedical datasets. The key obstacle is thus overcoming the large domain shift from common to biomedical images. We propose a Domain Adaptive Region-based Convolutional Neural Network (DARCNN), that adapts knowledge of object definition from COCO, a large labelled vision dataset, to multiple biomedical datasets. We introduce a domain separation module, a self-supervised representation consistency loss, and an augmented pseudo-labelling stage within DARCNN to effectively perform domain adaptation across such large domain shifts. We showcase DARCNN's performance for unsupervised instance segmentation on numerous biomedical datasets. | 在生物医学领域，存在大量密集、复杂的数据，其中感兴趣的对象可能难以检测，或受限于人类知识的局限。用于监督任务的标记领域特定数据集往往昂贵且难以获取，此外，发现新颖的不同对象可能有利于客观科学发现。因此，我们提出利用基准计算机视觉数据集中的丰富注释，为各种生物医学数据集进行无监督实例分割。关键障碍在于克服从常见到生物医学图像的大领域转移。我们提出了一种域自适应基于区域的卷积神经网络（DARCNN），该网络将来自COCO的对象定义知识适应到多个生物医学数据集中。我们在DARCNN中引入了域分离模块、自监督表示一致性损失和增强的伪标记阶段，以有效地在如此大的领域转移中执行域自适应。我们展示了DARCNN在众多生物医学数据集上进行无监督实例分割的性能。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Hsu_DARCNN_Domain_Adaptive_Region-Based_Convolutional_Neural_Network_for_Unsupervised_Instance_CVPR_2021_paper.pdf) |
| 2021 | SelfAugment: Automatic Augmentation Policies for Self-Supervised Learning | Colorado J Reed, Sean Metzger, Aravind Srinivas, Trevor Darrell, Kurt Keutzer | A common practice in unsupervised representation learning is to use labeled data to evaluate the quality of the learned representations. This supervised evaluation is then used to guide critical aspects of the training process such as selecting the data augmentation policy. However, guiding an unsupervised training process through supervised evaluations is not possible for real-world data that does not actually contain labels (which may be the case, for example, in privacy sensitive fields such as medical imaging). Therefore, in this work we show that evaluating the learned representations with a self-supervised image rotation task is highly correlated with a standard set of supervised evaluations (rank correlation > 0.94). We establish this correlation across hundreds of augmentation policies, training settings, and network architectures and provide an algorithm (SelfAugment) to automatically and efficiently select augmentation policies without using supervised evaluations. Despite not using any labeled data, the learned augmentation policies perform comparably with augmentation policies that were determined using exhaustive supervised evaluations. | 在无监督表示学习中的一个常见做法是使用标记数据来评估学习到的表示的质量。然后利用这种监督评估来指导训练过程的关键方面，如选择数据增强策略。然而，通过监督评估来指导无监督训练过程对于实际上不包含标签的真实世界数据是不可能的（例如，在医学成像等隐私敏感领域可能会出现这种情况）。因此，在这项工作中，我们展示了用自监督图像旋转任务评估学习到的表示与标准的监督评估之间高度相关（等级相关性>0.94）。我们在数百种增强策略、训练设置和网络架构上建立了这种相关性，并提供了一种算法（SelfAugment），可以自动且高效地选择增强策略，而无需使用监督评估。尽管没有使用任何标记数据，但学习到的增强策略表现与通过详尽的监督评估确定的增强策略相当。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Reed_SelfAugment_Automatic_Augmentation_Policies_for_Self-Supervised_Learning_CVPR_2021_paper.pdf) |
| 2021 | FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space | Quande Liu, Cheng Chen, Jing Qin, Qi Dou, Pheng-Ann Heng | Federated learning allows distributed medical institutions to collaboratively learn a shared prediction model with privacy protection. While at clinical deployment, the models trained in federated learning can still suffer from performance drop when applied to completely unseen hospitals outside the federation. In this paper, we point out and solve a novel problem setting of federated domain generalization, which aims to learn a federated model from multiple distributed source domains such that it can directly generalize to unseen target domains. We present a novel approach, named as Episodic Learning in Continuous Frequency Space (ELCFS), for this problem by enabling each client to exploit multi-source data distributions under the challenging constraint of data decentralization. Our approach transmits the distribution information across clients in a privacy-protecting way through an effective continuous frequency space interpolation mechanism. With the transferred multi-source distributions, we further carefully design a boundary-oriented episodic learning paradigm to expose the local learning to domain distribution shifts and particularly meet the challenges of model generalization in medical image segmentation scenario. The effectiveness of our method is demonstrated with superior performance over state-of-the-arts and in-depth ablation experiments on two medical image segmentation tasks. The code is available at "https://github.com/liuquande/FedDG-ELCFS". | 联邦学习允许分布式医疗机构合作学习一个带有隐私保护的共享预测模型。然而，在临床部署时，联邦学习中训练的模型在应用于联邦之外完全未见过的医院时，仍可能遭受性能下降的影响。在本文中，我们指出并解决了一个新颖的问题设置，即联邦域泛化，旨在从多个分布式源域学习一个联邦模型，使其能够直接泛化到未见的目标域。我们提出了一种新颖的方法，命名为连续频率空间中的分集学习（ELCFS），用于在数据分散的挑战性约束下使每个客户端能够利用多源数据分布。我们的方法通过有效的连续频率空间插值机制以保护隐私的方式在客户端之间传输分布信息。通过传输的多源分布，我们进一步精心设计了一个面向边界的分集学习范式，以暴露本地学习对领域分布变化的敏感性，特别是在医学图像分割场景中满足模型泛化的挑战。我们的方法的有效性通过在两个医学图像分割任务上优越的性能和深入的消融实验得到证明。代码可在"https://github.com/liuquande/FedDG-ELCFS"上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_FedDG_Federated_Domain_Generalization_on_Medical_Image_Segmentation_via_Episodic_CVPR_2021_paper.pdf) |
| 2021 | Multi-Institutional Collaborations for Improving Deep Learning-Based Magnetic Resonance Image Reconstruction Using Federated Learning | Pengfei Guo, Puyang Wang, Jinyuan Zhou, Shanshan Jiang, Vishal M. Patel | Fast and accurate reconstruction of magnetic resonance (MR) images from under-sampled data is important in many clinical applications. In recent years, deep learning-based methods have been shown to produce superior performance on MR image reconstruction. However, these methods require large amounts of data which is difficult to collect and share due to the high cost of acquisition and medical data privacy regulations. In order to overcome this challenge, we propose a federated learning (FL) based solution in which we take advantage of the MR data available at different institutions while preserving patients' privacy. However, the generalizability of models trained with the FL setting can still be suboptimal due to domain shift, which results from the data collected at multiple institutions with different sensors, disease types, and acquisition protocols, etc. With the motivation of circumventing this challenge, we propose a cross-site modeling for MR image reconstruction in which the learned intermediate latent features among different source sites are aligned with the distribution of the latent features at the target site. Extensive experiments are conducted to provide various insights about FL for MR image reconstruction. Experimental results demonstrate that the proposed framework is a promising direction to utilize multi-institutional data without compromising patients' privacy for achieving improved MR image reconstruction. Our code is available at https://github.com/guopengf/FL-MRCM | 磁共振（MR）图像从欠采样数据中快速准确地重建在许多临床应用中至关重要。近年来，基于深度学习的方法已被证明在MR图像重建上表现出优越性能。然而，这些方法需要大量数据，由于采集成本高昂和医疗数据隐私规定的限制，很难收集和共享。为了克服这一挑战，我们提出了一种基于联邦学习（FL）的解决方案，利用不同机构可获得的MR数据，同时保护患者的隐私。然而，由于来自具有不同传感器、疾病类型和采集协议等多个机构收集的数据，FL设置下训练的模型的泛化能力仍可能不够理想。为了应对这一挑战，我们提出了一种用于MR图像重建的跨站点建模方法，其中在不同来源站点之间学习的中间潜在特征与目标站点的潜在特征分布对齐。我们进行了大量实验，以提供有关FL用于MR图像重建的各种见解。实验结果表明，所提出的框架是利用多机构数据而不损害患者隐私以实现改进的MR图像重建的有前途的方向。我们的代码可在https://github.com/guopengf/FL-MRCM 上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Multi-Institutional_Collaborations_for_Improving_Deep_Learning-Based_Magnetic_Resonance_Image_Reconstruction_CVPR_2021_paper.pdf) |
| 2021 | A Self-Boosting Framework for Automated Radiographic Report Generation | Zhanyu Wang, Luping Zhou, Lei Wang, Xiu Li | Automated radiographic report generation is a challenging task since it requires to generate paragraphs describing fine-grained visual differences of cases, especially for those between the diseased and the healthy. Existing image captioning methods commonly target at generic images, and lack mechanism to meet this requirement. To bridge this gap, in this paper, we propose a self-boosting framework that improves radiographic report generation based on the cooperation of the main task of report generation and anauxiliary task of image-text matching. The two tasks are built as the two branches of a network model and influence each other in a cooperative way. On one hand, the image-text matching branch helps to learn highly text-correlated visual features for the report generation branch to output high quality reports. One the other hand, the improved reports produced by the report generation branch provideadditional harder samples for the image-text matching task and enforce the latter to improve itself by learning better visual and text feature representations. This, in turn, helps improve the report generation branch again. These two branches are jointly trained to help improve each other iteratively and progressively, so that the whole model is self-boosted without requiring any external resources. Additionally, in the loss function, our model evaluates the quality of the generated reports not only on the word similarity as common approaches do (via minimizing a cross-entropy loss), but also on the feature similarity at high-level, while the latter is provided by the text-encoder of the image-text matching branch. Experimental results demonstrate the effectiveness of our method on two public datasets, showing its superior performance over other state-of-the-art medical report generation methods. | 自动生成放射学报告是一项具有挑战性的任务，因为它需要生成描述病例之间细微视觉差异的段落，特别是疾病和健康之间的差异。现有的图像字幕方法通常针对通用图像，缺乏满足这一要求的机制。为了弥合这一差距，在本文中，我们提出了一个自我增强框架，通过报告生成的主要任务和图像文本匹配的辅助任务的合作来改进放射学报告生成。这两个任务构建为网络模型的两个分支，并以合作的方式相互影响。一方面，图像文本匹配分支有助于学习与报告生成分支输出高质量报告相关的视觉特征。另一方面，报告生成分支生成的改进报告为图像文本匹配任务提供额外的更难的样本，并通过学习更好的视觉和文本特征表示来强化后者。这反过来又有助于再次改进报告生成分支。这两个分支联合训练，相互迭代和逐步改进，使整个模型在不需要任何外部资源的情况下自我增强。此外，在损失函数中，我们的模型评估生成报告的质量不仅仅是通过词相似性（通过最小化交叉熵损失）如常见方法所做的，还评估高级别的特征相似性，后者由图像文本匹配分支的文本编码器提供。实验结果表明，我们的方法在两个公共数据集上的有效性，显示其优于其他最先进的医学报告生成方法。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_A_Self-Boosting_Framework_for_Automated_Radiographic_Report_Generation_CVPR_2021_paper.pdf) |
| 2021 | ATSO: Asynchronous Teacher-Student Optimization for Semi-Supervised Image Segmentation | Xinyue Huo, Lingxi Xie, Jianzhong He, Zijie Yang, Wengang Zhou, Houqiang Li, Qi Tian | Semi-supervised learning is a useful tool for image segmentation, mainly due to its ability in extracting knowledge from unlabeled data to assist learning from labeled data. This paper focuses on a popular pipeline known as self-learning, where we point out a weakness named lazy mimicking that refers to the inertia that a model retains the prediction from itself and thus resists updates. To alleviate this issue, we propose the Asynchronous Teacher-Student Optimization (ATSO) algorithm that (i) breaks up continual learning from teacher to student and (ii) partitions the unlabeled training data into two subsets and alternately uses one subset to fine-tune the model which updates the labels on the other. We show the ability of ATSO on medical and natural image segmentation. In both scenarios, our method reports competitive performance, on par with the state-of-the-arts, in either using partial labeled data in the same dataset or transferring the trained model to an unlabeled dataset. | 半监督学习是图像分割的一种有用工具，主要是因为它能够从未标记数据中提取知识，以辅助从已标记数据中学习。本文关注一种被称为自学习的流行流程，我们指出了一种名为懒惰模仿的弱点，指的是模型保留自身预测的惯性，从而抵制更新。为了缓解这个问题，我们提出了异步师生优化（ATSO）算法，该算法（i）将从师到生的持续学习分解，并（ii）将未标记的训练数据分为两个子集，交替使用一个子集来微调模型，从而更新另一个子集上的标签。我们展示了ATSO在医学和自然图像分割上的能力。在两种情况下，我们的方法报告了具有竞争力的表现，与同一数据集中使用部分标记数据或将训练好的模型转移到未标记数据集中的最新技术水平相当。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Huo_ATSO_Asynchronous_Teacher-Student_Optimization_for_Semi-Supervised_Image_Segmentation_CVPR_2021_paper.pdf) |
| 2021 | Unsupervised Human Pose Estimation Through Transforming Shape Templates | Luca Schmidtke, Athanasios Vlontzos, Simon Ellershaw, Anna Lukens, Tomoki Arichi, Bernhard Kainz | Human pose estimation is a major computer vision problem with applications ranging from augmented reality and video capture to surveillance and movement tracking. In the medical context, the latter may be an important biomarker for neurological impairments in infants. Whilst many methods exist, their application has been limited by the need for well annotated large datasets and the inability to generalize to humans of different shapes and body compositions, e.g. children and infants. In this paper we present a novel method for learning pose estimators for human adults and infants in an unsupervised fashion. We approach this as a learnable template matching problem facilitated by deep feature extractors. Human-interpretable landmarks are estimated by transforming a template consisting of predefined body parts that are characterized by 2D Gaussian distributions. Enforcing a connectivity prior guides our model to meaningful human shape representations. We demonstrate the effectiveness of our approach on two different datasets including adults and infants. | 人体姿势估计是一个重要的计算机视觉问题，应用范围从增强现实和视频捕捉到监视和动作跟踪。在医学背景下，后者可能是婴儿神经损伤的重要生物标志物。虽然存在许多方法，但它们的应用受到对大型数据集进行良好注释的需求的限制，且无法推广到不同形状和体质的人，例如儿童和婴儿。在本文中，我们提出一种新的方法，以无监督的方式学习成人和婴儿的姿势估计器。我们将其视为一个可学习的模板匹配问题，通过深度特征提取器实现。通过变换由预定义身体部位组成的模板来估计人可解释的标记，这些身体部位由2D高斯分布进行描述。强制连接性先验指导我们的模型产生有意义的人体形状表示。我们在包括成人和婴儿的两个不同数据集上展示了我们方法的有效性。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Schmidtke_Unsupervised_Human_Pose_Estimation_Through_Transforming_Shape_Templates_CVPR_2021_paper.pdf) |
| 2021 | DoDNet: Learning To Segment Multi-Organ and Tumors From Multiple Partially Labeled Datasets | Jianpeng Zhang, Yutong Xie, Yong Xia, Chunhua Shen | Due to the intensive cost of labor and expertise in annotating 3D medical images at a voxel level, most benchmark datasets are equipped with the annotations of only one type of organs and/or tumors, resulting in the so-called partially labeling issue. To address this issue, we propose a dynamic on-demand network (DoDNet) that learns to segment multiple organs and tumors on partially labeled datasets. DoDNet consists of a shared encoder-decoder architecture, a task encoding module, a controller for dynamic filter generation, and a single but dynamic segmentation head. The information of current segmentation task is encoded as a task-aware prior to tell the model what the task is expected to achieve. Different from existing approaches which fix kernels after training, the kernels in dynamic head are generated adaptively by the controller, conditioned on both input image and assigned task. Thus, DoDNet is able to segment multiple organs and tumors, as done by multiple networks or a multi-head network, in a much efficient and flexible manner. We created a large-scale partially labeled dataset called MOTS and demonstrated the superior performance of our DoDNet over other competitors on seven organ and tumor segmentation tasks. We also transferred the weights pre-trained on MOTS to a downstream multi-organ segmentation task and achieved state-of-the-art performance. This study provides a general 3D medical image segmentation model that has been pre-trained on a large-scale partially labeled dataset and can be extended (after fine-tuning) to downstream volumetric medical data segmentation tasks. Code and models are available at https://git.io/DoDNet. | 由于在体素级别上对3D医学图像进行标注的劳动力和专业知识的高昂成本，大多数基准数据集仅配备有一个类型的器官和/或肿瘤的注释，导致了所谓的部分标注问题。为了解决这个问题，我们提出了一种动态的按需网络（DoDNet），该网络学习在部分标记的数据集上分割多个器官和肿瘤。DoDNet包括共享的编码器-解码器架构、任务编码模块、用于动态滤波器生成的控制器以及单个但动态的分割头。当前分割任务的信息被编码为任务感知先验，告诉模型任务的预期目标是什么。与现有方法不同，动态头中的滤波器在训练后是自适应生成的，受输入图像和分配的任务的条件影响。因此，DoDNet能够以更高效和灵活的方式分割多个器官和肿瘤，就像多个网络或多头网络所做的那样。我们创建了一个名为MOTS的大规模部分标记的数据集，并展示了我们的DoDNet在七个器官和肿瘤分割任务上比其他竞争对手表现出更优越的性能。我们还将在MOTS上预训练的权重转移到下游多器官分割任务，并实现了最先进的性能。该研究提供了一个通用的3D医学图像分割模型，该模型已在大规模部分标记的数据集上进行了预训练，并可以在微调后扩展到下游体积医学数据分割任务。代码和模型可在https://git.io/DoDNet上获得。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_DoDNet_Learning_To_Segment_Multi-Organ_and_Tumors_From_Multiple_Partially_CVPR_2021_paper.pdf) |
| 2021 | Improving Sign Language Translation With Monolingual Data by Sign Back-Translation | Hao Zhou, Wengang Zhou, Weizhen Qi, Junfu Pu, Houqiang Li | Despite existing pioneering works on sign language translation (SLT), there is a non-trivial obstacle, i.e., the limited quantity of parallel sign-text data. To tackle this parallel data bottleneck, we propose a sign back-translation (SignBT) approach, which incorporates massive spoken language texts into SLT training. With a text-to-gloss translation model, we first back-translate the monolingual text to its gloss sequence. Then, the paired sign sequence is generated by splicing pieces from an estimated gloss-to-sign bank at the feature level. Finally, the synthetic parallel data serves as a strong supplement for the end-to-end training of the encoder-decoder SLT framework. To promote the SLT research, we further contribute CSL-Daily, a large-scale continuous SLT dataset. It provides both spoken language translations and gloss-level annotations. The topic revolves around people's daily lives (e.g., travel, shopping, medical care), the most likely SLT application scenario. Extensive experimental results and analysis of SLT methods are reported on CSL-Daily. With the proposed sign back-translation method, we obtain a substantial improvement over previous state-of-the-art SLT methods. | 尽管在手语翻译（SLT）领域已经有一些开创性的工作，但存在一个非常严重的障碍，即有限数量的平行手语-文本数据。为了解决这一平行数据瓶颈问题，我们提出了一种手语反向翻译（SignBT）方法，该方法将大量口语文本纳入SLT训练中。通过一个文本到手语符号翻译模型，我们首先将单语文本反向翻译为其手语符号序列。然后，通过在特征级别从估计的手语符号库中拼接片段，生成配对的手语序列。最后，合成的平行数据作为编码器-解码器SLT框架端到端训练的强有力补充。为推动SLT研究，我们进一步贡献了CSL-Daily，一个大规模连续SLT数据集。它提供了口语翻译和符号级别注释。该数据集围绕着人们日常生活（如旅行、购物、医疗保健）展开，是最有可能的SLT应用场景。我们在CSL-Daily上报告了大量实验结果和SLT方法的分析。通过提出的手语反向翻译方法，我们实现了对先前最先进的SLT方法的显著改进。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Improving_Sign_Language_Translation_With_Monolingual_Data_by_Sign_Back-Translation_CVPR_2021_paper.pdf) |
| 2021 | Zero-Shot Instance Segmentation | Ye Zheng, Jiahong Wu, Yongqiang Qin, Faen Zhang, Li Cui | Deep learning has significantly improved the precision of instance segmentation with abundant labeled data. However, in many areas like medical and manufacturing, collecting sufficient data is extremely hard and labeling this data requires high professional skills. We follow this motivation and propose a new task set named zero-shot instance segmentation (ZSI). In the training phase of ZSI, the model is trained with seen data, while in the testing phase, it is used to segment all seen and unseen instances. We first formulate the ZSI task and propose a method to tackle the challenge, which consists of Zero-shot Detector, Semantic Mask Head, Background Aware RPN and Synchronized Background Strategy. We present a new benchmark for zero-shot instance segmentation based on the MS-COCO dataset. The extensive empirical results in this benchmark show that our method not only surpasses the state-of-the-art results in zero-shot object detection task but also achieves promising performance on ZSI. Our approach will serve as a solid baseline and facilitate future research in zero-shot instance segmentation. Code available at ZSI. | 深度学习显著提高了具有丰富标记数据的实例分割的精度。然而，在许多领域，如医疗和制造业，收集足够的数据非常困难，并且对这些数据进行标记需要高度专业技能。我们遵循这一动机，并提出了一个名为零样本实例分割（ZSI）的新任务集。在ZSI的训练阶段，模型使用已见数据进行训练，而在测试阶段，它被用于分割所有已见和未见的实例。我们首先制定了ZSI任务，并提出了一种解决挑战的方法，其中包括零样本检测器、语义蒙版头、背景感知RPN和同步背景策略。我们基于MS-COCO数据集提出了一个新的零样本实例分割基准。在这个基准测试中的广泛实证结果显示，我们的方法不仅在零样本目标检测任务中超越了最新技术结果，而且在ZSI上取得了令人满意的表现。我们的方法将作为一个坚实的基线，并促进未来零样本实例分割研究。代码可在ZSI上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Zero-Shot_Instance_Segmentation_CVPR_2021_paper.pdf) |
