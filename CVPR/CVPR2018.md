| 年份 | 题目 | 作者 | 摘要 | 中文摘要 | link |
| --- | --- | --- | --- | --- | --- |
| 2018 | The Power of Ensembles for Active Learning in Image Classification | William H. Beluch, Tim Genewein, Andreas NÃ¼rnberger, Jan M. KÃ¶hler | Deep learning methods have become the de-facto standard for challenging image processing tasks such as image classification. One major hurdle of deep learning approaches is that large sets of labeled data are necessary, which can be prohibitively costly to obtain, particularly in medical image diagnosis applications. Active learning techniques can alleviate this labeling effort. In this paper we investigate some recently proposed methods for active learning with high-dimensional data and convolutional neural network classifiers. We compare ensemble-based methods against Monte-Carlo Dropout and geometric approaches. We find that ensembles perform better and lead to more calibrated predictive uncertainties, which are the basis for many active learning algorithms. To investigate why Monte-Carlo Dropout uncertainties perform worse, we explore potential differences in isolation in a series of experiments. We show results for MNIST and CIFAR-10, on which we achieve a test set accuracy of $90 %$ with roughly 12,200 labeled images, and initial results on ImageNet. Additionally, we show results on a large, highly class-imbalanced diabetic retinopathy dataset. We observe that the ensemble-based active learning effectively counteracts this imbalance during acquisition. | 深度学习方法已成为具有挑战性的图像处理任务（如图像分类）的事实标准。深度学习方法的一个主要障碍是需要大量标记数据，这可能会导致成本过高，特别是在医学图像诊断应用中。主动学习技术可以减轻这种标记工作。本文研究了一些针对高维数据和卷积神经网络分类器的最近提出的主动学习方法。我们将集成方法与蒙特卡罗辍学和几何方法进行了比较。我们发现集成方法表现更好，并导致更加校准的预测不确定性，这是许多主动学习算法的基础。为了探究为什么蒙特卡罗辍学的不确定性表现更差，我们在一系列实验中探索了孤立的潜在差异。我们展示了在MNIST和CIFAR-10上的结果，其中我们通过约12,200张标记图像达到了90％的测试集准确性，并初步展示了在ImageNet上的结果。此外，我们展示了在一个大型、高度类别不平衡的糖尿病视网膜病变数据集上的结果。我们观察到集成式主动学习有效地对抗了这种不平衡现象。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Beluch_The_Power_of_CVPR_2018_paper.pdf) |
| 2018 | Weakly Supervised Learning of Single-Cell Feature Embeddings | Juan C. Caicedo, Claire McQuin, Allen Goodman, Shantanu Singh, Anne E. Carpenter | Many new applications in drug discovery and functional genomics require capturing the morphology of individual imaged cells as comprehensively as possible rather than measuring one particular feature. In these so-called profiling experiments, the goal is to compare populations of cells treated with different chemicals or genetic perturbations in order to identify biomedically important similarities. Deep convolutional neural networks (CNNs) often make excellent feature extractors but require ground truth for training; this is rarely available in biomedical profiling experiments. We therefore propose to train CNNs based on a weakly supervised approach, where the network aims to classify each treatment against all others. Using this network as a feature extractor performed comparably to a network trained on non-biological, natural images on a chemical screen benchmark task, and improved results significantly on a more challenging genetic benchmark presented for the first time. | 药物发现和功能基因组学中许多新应用需要尽可能全面地捕获单个图像细胞的形态特征，而不是测量特定的一个特征。在这些所谓的配置实验中，目标是比较受不同化学物质或基因扰动影响的细胞群，以识别生物医学上重要的相似之处。深度卷积神经网络（CNNs）通常是优秀的特征提取器，但需要训练的基本真相；在生物医学配置实验中，这极少可获得。因此，我们提出基于弱监督方法训练CNNs，其中网络旨在对每种处理进行分类。在化学筛选基准任务上，使用该网络作为特征提取器的性能与在非生物、自然图像上训练的网络相当，并在首次提出的更具挑战性的遗传基准上显著改进结果。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Caicedo_Weakly_Supervised_Learning_CVPR_2018_paper.pdf) |
| 2018 | Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation | Adrian V. Dalca, John Guttag, Mert R. Sabuncu | We consider the problem of segmenting a biomedical image into anatomical regions of interest. We specifically address the frequent scenario where we have no paired training data that contains images and their manual segmentations. Instead, we employ unpaired segmentation images that we use to build an anatomical prior. Critically these segmentations can be derived from imaging data from a different dataset and imaging modality than the current task. We introduce a generative probabilistic model  that employs the learned prior through a convolutional neural network to compute segmentations in an unsupervised setting. We conducted an empirical analysis of the proposed approach in the context of structural brain MRI segmentation, using a multi-study dataset of more than 14,000 scans. Our results show that an anatomical prior enables fast unsupervised segmentation which is typically not possible using standard convolutional networks. The integration of anatomical priors can facilitate CNN-based anatomical segmentation in a range of novel clinical problems, where few or no annotations are available and thus standard networks are not trainable. The code, model definitions and model weights are freely available at http://github.com/adalca/neuron | 我们考虑将生物医学图像分割为感兴趣的解剖区域的问题。我们特别关注通常情况下没有包含图像及其手动分割的配对训练数据的问题。相反，我们使用未配对的分割图像来构建解剖先验。关键是，这些分割可以来自不同数据集和成像模态的成像数据。我们引入了一个生成概率模型，通过卷积神经网络应用学习到的先验来在非监督设置中计算分割。我们在结构性脑MRI分割的背景下对所提出的方法进行了实证分析，使用了超过14,000个扫描的多研究数据集。我们的结果显示，解剖先验使得通常无法使用标准卷积网络进行快速非监督分割成为可能。解剖先验的整合可以促进基于CNN的解剖分割在一系列新的临床问题中的应用，其中很少或没有注释可用，因此标准网络无法进行训练。代码、模型定义和模型权重可以在http://github.com/adalca/neuron免费获取。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Dalca_Anatomical_Priors_in_CVPR_2018_paper.pdf) |
| 2018 | An Unsupervised Learning Model for Deformable Medical Image Registration | Guha Balakrishnan, Amy Zhao, Mert R. Sabuncu, John Guttag, Adrian V. Dalca | We present a fast learning-based algorithm for deformable, pairwise 3D medical image registration. Current registration methods optimize an objective function independently for each pair of images, which can be time-consuming for large data. We define registration as a parametric function, and optimize its parameters given a set of images from a collection of interest. Given a new pair of scans, we can quickly compute a registration field by directly evaluating the function using the learned parameters. We model this function using a CNN, and use a spatial transform layer to reconstruct one image from another while imposing smoothness constraints on the registration field. The proposed method does not require supervised information such as ground truth registration fields or anatomical landmarks. We demonstrate registration accuracy comparable to state-of-the-art 3D image registration, while operating orders of magnitude faster in practice. Our method promises to significantly speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is available at https://github.com/balakg/voxelmorph. | 我们提出了一种快速的基于学习的算法，用于可变形的、成对的三维医学图像配准。当前的配准方法独立为每对图像优化一个客观函数，这对于大型数据可能是耗时的。我们将配准定义为一个参数函数，并在给定一组来自感兴趣的收集的图像时优化其参数。给定一对新的扫描，我们可以通过直接使用学习的参数评估函数来快速计算配准场。我们使用CNN对这个函数建模，并使用空间变换层在配准场上施加平滑约束来重建一个图像到另一个图像。所提出的方法不需要监督信息，如地面真实配准场或解剖标志。我们展示了与最先进的三维图像配准相媲美的配准准确性，同时在实践中操作速度快得多。我们的方法承诺显著加快医学图像分析和处理流程，同时促进基于学习的配准及其应用的新方向。我们的代码可在https://github.com/balakg/voxelmorph 获取。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Balakrishnan_An_Unsupervised_Learning_CVPR_2018_paper.pdf) |
| 2018 | Translating and Segmenting Multimodal Medical Volumes With Cycle- and Shape-Consistency Generative Adversarial Network | Zizhao Zhang, Lin Yang, Yefeng Zheng | Synthesized medical images have several important applications, e.g., as an intermedium in cross-modality image registration and as supplementary training samples to boost the generalization capability of a classifier. Especially, synthesized CT data can provide X-ray attenuation map for radiation therapy planning. In this work, we propose a generic cross-modality synthesis approach with the following targets: 1) synthesizing realistic looking 3D images using unpaired training data, 2) ensuring consistent anatomical structures, which could changed by geometric distortion in cross-modality synthesis and 3) improving volume segmentation by using synthetic data for modalities with limited training samples. We show that these goals can be achieved with an end-to-end 3D convolutional neural network (CNN) composed of mutually-beneficial generators and segmentors for image synthesis and segmentation tasks. The generators are trained with an adversarial loss, a cycle-consistency loss, and also a shape-consistency loss,  which is supervised by segmentors, to reduce the geometric distortion. From the segmentation view, the segmentors are boosted by synthetic data from generators in an online manner. Generators and segmentors prompt each other alternatively in an end-to-end training fashion. With extensive experiments on a dataset including a total of 4,496 CT and MRI cardiovascular volumes, we show both tasks are beneficial to each other and coupling these two tasks results in better performance than solving them exclusively. | 合成医学图像在许多重要应用中发挥着作用，例如作为跨模态图像配准中介和作为补充训练样本以提高分类器的泛化能力。特别地，合成CT数据可以为放射治疗规划提供X射线衰减图。在这项工作中，我们提出了一种通用的跨模态合成方法，其目标包括：1）使用非配对训练数据合成逼真的3D图像，2）确保一致的解剖结构，这些结构可能会因跨模态合成中的几何失真而改变，以及3）通过使用有限训练样本的模态的合成数据来改善体积分割。我们展示了这些目标可以通过由互惠生成器和分割器组成的端对端3D卷积神经网络（CNN）实现，用于图像合成和分割任务。生成器使用对抗损失、循环一致性损失以及由分割器监督的形状一致性损失进行训练，以减少几何失真。从分割角度看，分割器通过在线方式从生成器中获得合成数据的提升。生成器和分割器交替地促进彼此的训练。通过对包括4496个CT和MRI心血管体积的数据集进行广泛实验，我们展示了这两个任务对彼此都是有益的，并将这两个任务耦合在一起会比单独解决它们的表现更好。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Translating_and_Segmenting_CVPR_2018_paper.pdf) |
| 2018 | Generating Synthetic X-Ray Images of a Person From the Surface Geometry | Brian Teixeira, Vivek Singh, Terrence Chen, Kai Ma, Birgi Tamersoy, Yifan Wu, Elena Balashova, Dorin Comaniciu | We present a novel framework that learns to predict human anatomy from body surface. Specifically, our approach generates a synthetic X-ray image of a person only from the person's surface geometry. Furthermore, the synthetic X-ray image is parametrized and can be manipulated by adjusting a set of body markers which are also generated during the X-ray image prediction. With the proposed framework, multiple synthetic X-ray images can easily be generated by varying surface geometry. By perturbing the parameters, several additional synthetic X-ray images can be generated from the same surface geometry. As a result, our approach offers a potential to overcome the training data barrier in the medical domain. This capability is achieved by learning a pair of networks - one learns to generate the full image from the partial image and a set of parameters, and the other learns to estimate the parameters given the full image. During training, the two networks are trained iteratively such that they would converge to a solution where the predicted parameters and the full image are consistent with each other. In addition to medical data enrichment, our framework can also be used for image completion as well as anomaly detection. | 我们提出了一个新颖的框架，学习从人体表面预测人类解剖学。具体来说，我们的方法仅从人体表面几何生成一个人的合成X射线图像。此外，合成X射线图像是参数化的，并且可以通过调整一组在X射线图像预测过程中生成的身体标记来进行操作。借助所提出的框架，可以通过改变表面几何轻松生成多个合成X射线图像。通过扰动参数，可以从相同的表面几何生成几个额外的合成X射线图像。因此，我们的方法提供了一个潜力来克服医学领域中的训练数据障碍。这种能力是通过学习一对网络实现的 - 一个网络从部分图像和一组参数中学习生成完整图像，另一个网络学习估计给定完整图像的参数。在训练过程中，这两个网络经过迭代训练，使它们收敛到一个解决方案，其中预测的参数和完整图像相互一致。除了医学数据丰富化外，我们的框架还可以用于图像补全和异常检测。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Teixeira_Generating_Synthetic_X-Ray_CVPR_2018_paper.pdf) |
| 2018 | TieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-Rays | Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Ronald M. Summers | Chest X-rays are one of the most common radiological examinations in daily clinical routines. Reporting thorax diseases using chest X-rays is often an entry-level task for radiologist trainees. Yet, reading a chest X-ray image remains a challenging job for learning-oriented machine intelligence, due to (1) shortage of large-scale machine-learnable medical image datasets, and (2) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training. In this paper, we show the clinical free-text radiological reports can be utilized as a priori knowledge for tackling these two key problems. We propose a novel Text-Image Embedding network (TieNet) for extracting the distinctive image and text representations. Multi-level attention models are integrated into an end-to-end trainable CNN-RNN architecture for highlighting the meaningful text words and image regions. We first apply TieNet to classify the chest X-rays by using both image features and text embeddings extracted from associated reports. The proposed auto-annotation framework achieves high accuracy (over 0.9 on average in AUCs) in assigning disease labels for our hand-label evaluation dataset. Furthermore, we transform the TieNet into a chest X-ray reporting system. It simulates the reporting process and can output disease classification and a preliminary report together. The classification results are significantly improved (6% increase on average in AUCs) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI). | 胸部X线检查是日常临床工作中最常见的放射学检查之一。使用胸部X线报告胸部疾病往往是放射科医师实习生的入门级任务。然而，对于以学习为导向的机器智能来说，阅读胸部X线图像仍然是一项具有挑战性的工作，原因是（1）缺乏大规模的可机器学习的医学图像数据集，以及（2）缺乏能够模拟需要多年知识积累和专业训练的人类放射科医师高级推理的技术。在本文中，我们展示临床自由文本放射学报告可以作为处理这两个关键问题的先验知识。我们提出了一种新颖的文本-图像嵌入网络（TieNet）用于提取独特的图像和文本表示。多级注意模型被集成到端到端可训练的CNN-RNN架构中，用于突出显示有意义的文本词汇和图像区域。我们首先应用TieNet来对胸部X线进行分类，使用从相关报告中提取的图像特征和文本嵌入。所提出的自动注释框架在我们的手动标注评估数据集中为疾病标签分配高准确度（平均AUC超过0.9）。此外，我们将TieNet转化为胸部X线报告系统。它模拟了报告过程，并可以一起输出疾病分类和初步报告。与未见过的手动标记数据集（OpenI）上的现有基线相比，分类结果显著提高（平均AUC增加6％）。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_TieNet_Text-Image_Embedding_CVPR_2018_paper.pdf) |
| 2018 | Multi-Cell Detection and Classification Using a Generative Convolutional Model | Florence Yellin, Benjamin D. Haeffele, Sophie Roth, RenÃ© Vidal | Detecting, counting, and classifying various cell types in images of human blood is important in many biomedical applications. However, these tasks can be very difficult due to the wide range of biological variability and the resolution limitations of many imaging modalities.  This paper proposes a new approach to detecting, counting and classifying white blood cell populations in holographic images, which capitalizes on the fact that the variability in a mixture of blood cells is constrained by physiology. The proposed approach is based on a probabilistic generative model that describes an image of a population of cells as the sum of atoms from a convolutional dictionary of cell templates. The class of each template is drawn from a prior distribution that captures statistical information about blood cell mixtures. The parameters of the prior distribution are learned from a database of complete blood count results obtained from patients, and the cell templates are learned from images of purified cells from a single cell class using an extension of convolutional dictionary learning. Cell detection, counting and classification is then done using an extension of convolutional sparse coding that accounts for class proportion priors. This method has been successfully used to detect, count and classify white blood cell populations in holographic images of lysed blood obtained from 20 normal blood donors and 12 abnormal clinical blood discard samples. The error from our method is under 6.8% for all class populations, compared to errors of over 28.6% for all other methods tested. | 在许多生物医学应用中，检测、计数和分类人类血液图像中的各种细胞类型是非常重要的。然而，由于生物变异的广泛范围和许多成像模式的分辨率限制，这些任务可能会非常困难。本文提出了一种新的方法来检测、计数和分类全息图像中的白细胞群体，利用了血液细胞混合物的变异受生理学约束的事实。所提出的方法基于一个概率生成模型，将一个细胞群体的图像描述为来自细胞模板卷积字典的原子之和。每个模板的类别来自捕获有关血细胞混合物的统计信息的先验分布。先验分布的参数是从患者获得的完整血细胞计数结果数据库中学习的，细胞模板则是通过使用卷积字典学习的扩展从单个细胞类别的纯化细胞图像中学习的。然后，使用考虑类比例先验的卷积稀疏编码的扩展进行细胞检测、计数和分类。该方法已成功用于检测、计数和分类来自20名正常献血者和12个异常临床血液废弃样本的溶解血液全息图像中的白细胞群体。与所有其他测试方法相比，我们的方法对所有类别的误差均低于6.8%，而其他方法的误差超过28.6%。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Yellin_Multi-Cell_Detection_and_CVPR_2018_paper.pdf) |
| 2018 | Visual Feature Attribution Using Wasserstein GANs | Christian F. Baumgartner, Lisa M. Koch, Kerem Can Tezcan, Jia Xi Ang, Ender Konukoglu | Attributing the pixels of an input image to a certain category is an important and well-studied problem in computer vision, with applications ranging from weakly supervised localisation to understanding hidden effects in the data. In recent years, approaches based on interpreting a previously trained neural network classifier have become the de facto state-of-the-art and are commonly used on medical as well as natural image datasets. In this paper, we discuss a limitation of these approaches which may lead to only a subset of the category specific features being detected. To address this problem we develop a novel feature attribution technique based on Wasserstein Generative Adversarial Networks (WGAN), which does not suffer from this limitation. We show that our proposed method performs substantially better than the state-of-the-art for visual attribution on a synthetic dataset and on real 3D neuroimaging data from patients with mild cognitive impairment (MCI) and Alzheimer's disease (AD). For AD patients the method produces compellingly realistic disease effect maps which are very close to the observed effects. | 将输入图像的像素归因于某一类别是计算机视觉中一个重要且经过充分研究的问题，其应用范围从弱监督定位到理解数据中的隐藏效应。近年来，基于解释先前训练的神经网络分类器的方法已成为事实上的最先进技术，并广泛应用于医学以及自然图像数据集。在本文中，我们讨论了这些方法的一个局限性，可能导致仅检测到类别特定特征的子集。为了解决这个问题，我们开发了一种基于Wasserstein生成对抗网络（WGAN）的新型特征归因技术，不受此限制。我们展示了我们提出的方法在合成数据集和来自轻度认知障碍（MCI）和阿尔茨海默病（AD）患者的真实3D神经影像数据上的视觉归因方面比最先进方法表现显著更好。对于AD患者，该方法产生了令人信服的真实疾病效应图，非常接近观察到的效应。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Baumgartner_Visual_Feature_Attribution_CVPR_2018_paper.pdf) |
| 2018 | Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation | Xiaowei Xu, Qing Lu, Lin Yang, Sharon Hu, Danny Chen, Yu Hu, Yiyu Shi | With pervasive applications of medical imaging in healthcare, biomedical image segmentation plays a central role in quantitative analysis, clinical diagnosis, and medical intervention. Since manual annotation suffers limited reproducibility, arduous efforts, and excessive time, automatic segmentation is desired to process increasingly larger scale histopathological data. Recently, deep neural networks (DNNs), particularly fully convolutional networks (FCNs), have been widely applied to biomedical image segmentation, attaining much improved performance. At the same time, quantization of DNNs has become an active research topic, which aims to represent weights with less memory (precision) to considerably reduce memory and computation requirements of DNNs while maintaining acceptable accuracy. In this paper, we apply quantization techniques to FCNs for accurate biomedical image segmentation. Unlike existing literature on quantization which primarily targets memory and computation complexity reduction, we apply quantization as a method to reduce overfitting in FCNs for better accuracy. Specifically, we focus on a state-of-the-art segmentation framework, suggestive annotation [22], which judiciously extracts representative annotation samples from the original training dataset, obtaining an effective small-sized balanced training dataset. We develop two new quantization processes for this framework: (1) suggestive annotation with quantization for highly representative training samples, and (2) network training with quantization for high accuracy. Extensive experiments on the MICCAI Gland dataset show that both quantization processes can improve the segmentation performance, and our proposed method exceeds the current state-of-the-art performance by up to 1%. In addition, our method have a reduction of up to 6.4x on memory usage. | 随着医学影像在医疗保健中的广泛应用，生物医学图像分割在定量分析、临床诊断和医疗干预中起着核心作用。由于手动标注存在有限的再现性、繁重的工作量和过多的时间消耗，自动分割被期望用于处理日益庞大的组织病理学数据。最近，深度神经网络（DNN），特别是全卷积网络（FCN），已被广泛应用于生物医学图像分割，取得了显著改进的性能。同时，DNN的量化已成为一个活跃的研究课题，旨在用更少的内存（精度）表示权重，从而显著减少DNN的内存和计算需求，同时保持可接受的准确性。本文将量化技术应用于FCN以实现精确的生物医学图像分割。与现有的以减少内存和计算复杂度为主要目标的量化文献不同，我们将量化作为一种减少FCN过度拟合以提高准确性的方法。具体来说，我们专注于一种先进的分割框架，即建议性注释[22]，该框架从原始训练数据集中精心提取代表性的注释样本，获得一个有效的小型平衡训练数据集。我们为这一框架开发了两个新的量化过程：（1）具有量化的建议性注释，用于高度代表性的训练样本，以及（2）具有量化的网络训练，用于高准确性。在MICCAI腺体数据集上的大量实验表明，这两个量化过程都可以提高分割性能，我们提出的方法的性能超过当前的最先进性能高达1%。此外，我们的方法在内存使用上减少了高达6.4倍。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Quantization_of_Fully_CVPR_2018_paper.pdf) |
| 2018 | Conditional Generative Adversarial Network for Structured Domain Adaptation | Weixiang Hong, Zhenzhen Wang, Ming Yang, Junsong Yuan | In recent years, deep neural nets have triumphed over many computer vision problems, including semantic segmentation, which is a critical task in emerging autonomous driving and medical image diagnostics applications. In general, training deep neural nets requires a humongous amount of labeled data, which is laborious and costly to collect and annotate. Recent advances in computer graphics shed light on utilizing photo-realistic synthetic data with computer generated annotations to train neural nets. Nevertheless, the domain mismatch between real images and synthetic ones is the major challenge against harnessing the generated data and labels. In this paper, we propose a principled way to conduct structured domain adaption for semantic segmentation, i.e., integrating GAN into the FCN framework to mitigate the gap between source and target domains. Specifically, we learn a conditional generator to transform features of synthetic images to real-image like features, and a discriminator to distinguish them. For each training batch, the conditional generator and the discriminator compete against each other so that the generator learns to produce real-image like features to fool the discriminator; afterwards, the FCN parameters are updated to accommodate the changes of GAN. In experiments, without using labels of real image data, our method significantly outperforms the baselines as well as state-of-the-art methods by 12% â?20% mean IoU on the Cityscapes dataset. | 近年来，深度神经网络在许多计算机视觉问题中取得了胜利，包括语义分割，在新兴的自动驾驶和医学图像诊断应用中是一个关键任务。一般来说，训练深度神经网络需要大量标记数据，收集和注释这些数据是费时费力的。最近计算机图形学的进步揭示了利用具有计算机生成注释的逼真合成数据来训练神经网络的可能性。然而，真实图像与合成图像之间的域差异是利用生成数据和标签的主要挑战。在本文中，我们提出了一个有原则的方法来进行语义分割的结构域适应，即将GAN集成到FCN框架中，以减轻源域和目标域之间的差距。具体来说，我们学习一个条件生成器，将合成图像的特征转换为类似真实图像的特征，以及一个鉴别器来区分它们。对于每个训练批次，条件生成器和鉴别器相互竞争，使生成器学会生成类似真实图像的特征来欺骗鉴别器；然后，更新FCN参数以适应GAN的变化。在实验中，我们的方法在不使用真实图像数据的标签的情况下，在Cityscapes数据集上比基线和最先进方法表现出显著的优势，平均IoU提高了12%至20%。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.pdf) |
| 2018 | Clinical Skin Lesion Diagnosis Using Representations Inspired by Dermatologist Criteria | Jufeng Yang, Xiaoxiao Sun, Jie Liang, Paul L. Rosin | The skin is the largest organ in human body. Around 30%-70% of individuals worldwide have skin related health problems, for whom effective and efficient diagnosis is necessary. Recently, computer aided diagnosis (CAD) systems have been successfully applied to the recognition of skin cancers in dermatoscopic images. However, little work has concentrated on the commonly encountered skin diseases in clinical images captured by easily-accessed cameras or mobile phones. Meanwhile, for a CAD system, the representations of skin lesions are required to be understandable for dermatologists so that the predictions are convincing. To address this problem, we present effective representations inspired by the accepted dermatological criteria for diagnosing clinical skin lesions. We demonstrate that the dermatological criteria are highly correlated with measurable visual components. Accordingly, we design six medical representations considering different criteria for the recognition of skin lesions, and construct a diagnosis system for clinical skin disease images. Experimental results show that the proposed medical representations can not only capture the manifestations of skin lesions effectively, and consistently with the dermatological criteria, but also improve the prediction performance with respect to the state-of-the-art methods based on uninterpretable features. | 皮肤是人体最大的器官。全球约30%至70%的人有与皮肤相关的健康问题，因此需要进行有效和高效的诊断。最近，计算机辅助诊断（CAD）系统已成功应用于皮肤镜图像中皮肤癌的识别。然而，很少有研究集中在通过易于获得的相机或手机拍摄的临床图像中常见的皮肤疾病上。与此同时，对于CAD系统，皮损的表现需要让皮肤科医生能够理解，以使预测具有说服力。为了解决这个问题，我们提出了受到诊断临床皮损所接受的皮肤科标准启发的有效表现。我们证明了皮肤科标准与可测量的视觉成分高度相关。因此，我们设计了考虑不同标准的六种医学表现，用于识别皮损，并构建了一个临床皮肤疾病图像诊断系统。实验结果表明，所提出的医学表现不仅能有效捕捉皮损的表现，并与皮肤科标准一致，而且相对于基于不可解释特征的最先进方法，提高了预测性能。 | [link](https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Clinical_Skin_Lesion_CVPR_2018_paper.pdf) |
