| 年份 | 题目 | 作者 | 摘要 | 中文摘要 | link |
| --- | --- | --- | --- | --- | --- |
| 2019 | Topology Reconstruction of Tree-Like Structure in Images via Structural Similarity Measure and Dominant Set Clustering | Jianyang Xie,  Yitian Zhao,  Yonghuai Liu,  Pan Su,  Yifan Zhao,  Jun Cheng,  Yalin Zheng,  Jiang Liu | The reconstruction and analysis of tree-like topological structures in the biomedical images is crucial  for biologists and surgeons to understand biomedical conditions and plan surgical procedures. The underlying tree-structure topology reveals how different curvilinear components are anatomically connected to each other. Existing automated topology reconstruction methods have great difficulty in identifying the connectivity when two or more curvilinear components cross or bifurcate, due to their projection ambiguity, imaging noise and low contrast. In this paper, we propose a novel curvilinear structural similarity measure to guide a dominant-set clustering approach to address this indispensable issue. The novel similarity measure takes into account both intensity and geometric properties in representing  the curvilinear structure locally and globally, and group curvilinear objects at crossover points into different connected branches by dominant-set clustering.  The proposed method is applicable to different imaging modalities, and quantitative and qualitative results on retinal vessel, plant root, and neuronal network datasets show that our methodology is capable of advancing the current state-of-the-art techniques. | 在生物医学图像中重建和分析类似树状的拓扑结构对于生物学家和外科医生理解生物医学状况并规划手术程序至关重要。底层的树状结构拓扑揭示了不同曲线组件如何在解剖上相互连接。现有的自动拓扑重建方法在识别两个或多个曲线组件交叉或分叉时存在极大困难，这是由于它们的投影模糊、成像噪声和低对比度造成的。本文提出了一种新颖的曲线结构相似度度量方法，以引导主导集群方法解决这一不可或缺的问题。新颖的相似度度量考虑了局部和全局曲线结构的强度和几何特性，并通过主导集群将交叉点处的曲线对象分组到不同的连接分支中。所提出的方法适用于不同的成像模态，对视网膜血管、植物根系和神经网络数据集的定量和定性结果表明，我们的方法能够推进当前技术的最新进展。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Xie_Topology_Reconstruction_of_Tree-Like_Structure_in_Images_via_Structural_Similarity_CVPR_2019_paper.pdf) |
| 2019 | Metric Learning for Image Registration | Marc Niethammer,  Roland Kwitt,  Francois-Xavier Vialard | Image registration is a key technique in medical image analysis to estimate deformations between image pairs. A good deformation model is important for high-quality estimates. However, most existing approaches use ad-hoc deformation models chosen for mathematical convenience rather than to capture observed data variation. Recent deep learning approaches learn deformation models directly from data. However, they provide limited control over the spatial regularity of transformations. Instead of learning the entire registration approach, we learn a spatially-adaptive regularizer within a registration model. This allows controlling the desired level of regularity and preserving structural properties of a registration model. For example, diffeomorphic transformations can be attained. Our approach is a radical departure from existing deep learning approaches to image registration by embedding a deep learning model in an optimization-based registration algorithm to parameterize and data-adapt the registration model itself.  | 图像配准是医学图像分析中的关键技术，用于估计图像对之间的变形。良好的变形模型对于高质量的估计至关重要。然而，大多数现有方法使用为数学方便而选择的专门变形模型，而不是为了捕捉观察到的数据变化。最近的深度学习方法直接从数据中学习变形模型。然而，它们对于变换的空间规则性提供有限的控制。我们不是学习整个配准方法，而是在配准模型内学习一个空间自适应正则化器。这允许控制所需的规则性水平，并保留配准模型的结构特性。例如，可以实现微分同胚变换。我们的方法是与现有深度学习方法有根本区别的，它通过将深度学习模型嵌入到基于优化的配准算法中来参数化和数据适应配准模型本身。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Niethammer_Metric_Learning_for_Image_Registration_CVPR_2019_paper.pdf) |
| 2019 | Cross-Classification Clustering: An Efficient Multi-Object Tracking Technique for 3-D Instance Segmentation in Connectomics | Yaron Meirovitch,  Lu Mi,  Hayk Saribekyan,  Alexander Matveev,  David Rolnick,  Nir Shavit | Pixel-accurate tracking of objects is a key element in many computer vision applications, often solved by iterated individual object tracking or instance segmentation followed by object matching. Here we introduce cross-classification clustering (3C), a technique that simultaneously tracks complex, interrelated objects in an image stack. The key idea in cross-classification is to efficiently turn a clustering problem into a classification problem by running a logarithmic number of independent classifications per image, letting the cross-labeling of these classifications uniquely classify each pixel to the object labels. We apply the 3C mechanism to achieve state-of-the-art accuracy in connectomics -- the nanoscale mapping of neural tissue from electron microscopy volumes. Our reconstruction system increases scalability by an order of magnitude over existing single-object tracking methods (such as flood-filling networks). This scalability is important for the deployment of connectomics pipelines, since currently the best performing techniques require computing infrastructures that are beyond the reach of most laboratories. Our algorithm may offer benefits in other domains that require pixel-accurate tracking of multiple objects, such as segmentation of videos and medical imagery. | 像素精确跟踪对象是许多计算机视觉应用中的关键元素，通常通过迭代单个对象跟踪或实例分割后跟踪对象匹配来解决。在这里，我们介绍了跨分类聚类（3C），一种可以同时跟踪图像堆栈中复杂、相互关联对象的技术。跨分类中的关键思想是通过在每个图像中运行对数数量的独立分类，将聚类问题有效地转化为分类问题，让这些分类的交叉标记唯一地将每个像素分类到对象标签。我们将3C机制应用于实现连接组学的最新准确性 - 从电子显微镜体积中对神经组织进行纳米级映射。我们的重建系统使可扩展性增加了一个数量级，超过了现有的单个对象跟踪方法（如洪水填充网络）。这种可扩展性对于连接组学流水线的部署至关重要，因为目前表现最佳的技术需要超出大多数实验室的计算基础设施。我们的算法可能在需要多个对象的像素精确跟踪的其他领域中带来好处，例如视频和医学图像的分割。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Meirovitch_Cross-Classification_Clustering_An_Efficient_Multi-Object_Tracking_Technique_for_3-D_Instance_CVPR_2019_paper.pdf) |
| 2019 | Towards Universal Object Detection by Domain Attention | Xudong Wang,  Zhaowei Cai,  Dashan Gao,  Nuno Vasconcelos | Despite increasing efforts on universal representations for visual recognition, few have addressed object detection. In this paper, we develop an effective and efficient universal object detection system that is capable of working on various image domains, from human faces and traffic signs to medical CT images. Unlike multi-domain models, this universal model does not require prior knowledge of the domain of interest. This is achieved by the introduction of a new family of adaptation layers, based on the principles of squeeze and excitation, and a new domain-attention mechanism. In the proposed universal detector, all parameters and computations are shared across domains, and a single network processes all domains all the time. Experiments, on a newly established universal object detection benchmark of 11 diverse datasets, show that the proposed detector outperforms a bank of individual detectors, a multi-domain detector, and a baseline universal detector, with a 1.3x parameter increase over a single-domain baseline detector. The code and benchmark are available at http://www.svcl.ucsd.edu/projects/universal-detection/. | 尽管在视觉识别领域越来越多的努力致力于通用表示，但很少有人研究物体检测。本文提出了一种有效且高效的通用物体检测系统，能够适用于各种图像领域，从人脸和交通标识到医学CT图像。与多领域模型不同，这个通用模型不需要对感兴趣领域有先验知识。这是通过引入一种基于挤压和激活原理的新型适应层系列以及新的领域注意机制实现的。在提出的通用检测器中，所有参数和计算在各个领域之间共享，并且一个网络始终处理所有领域。在一个新建的包含11个不同数据集的通用物体检测基准测试上的实验结果表明，所提出的检测器优于一组单独的检测器、多领域检测器和基准通用检测器，与单领域基准检测器相比，参数增加了1.3倍。代码和基准测试可在http://www.svcl.ucsd.edu/projects/universal-detection/上获得。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Towards_Universal_Object_Detection_by_Domain_Attention_CVPR_2019_paper.pdf) |
| 2019 | Acoustic Non-Line-Of-Sight Imaging | David B. Lindell,  Gordon Wetzstein,  Vladlen Koltun | Non-line-of-sight (NLOS) imaging enables unprecedented capabilities in a wide range of applications, including robotic and machine vision, remote sensing, autonomous vehicle navigation, and medical imaging. Recent approaches to solving this challenging problem employ optical time-of-flight imaging systems with highly sensitive time-resolved photodetectors and ultra-fast pulsed lasers. However, despite recent successes in NLOS imaging using these systems, widespread implementation and adoption of the technology remains a challenge because of the requirement for specialized, expensive hardware. We introduce acoustic NLOS imaging, which is orders of magnitude less expensive than most optical systems and captures hidden 3D geometry at longer ranges with shorter acquisition times compared to state-of-the-art optical methods. Inspired by hardware setups used in radar and algorithmic approaches to model and invert wave-based image formation models developed in the seismic imaging community, we demonstrate a new approach to seeing around corners. | 非直视成像（NLOS）在一系列应用中实现了前所未有的能力，包括机器人和机器视觉、遥感、自主车辆导航和医学成像。解决这一具有挑战性问题的最新方法采用具有高灵敏度的时间分辨光探测器和超快脉冲激光的光时飞行成像系统。然而，尽管最近使用这些系统在NLOS成像方面取得了成功，但由于需要专门的昂贵硬件，该技术的广泛实施和采用仍然是一个挑战。我们引入声学NLOS成像，其成本比大多数光学系统低出数量级，并且与最先进的光学方法相比，以更短的采集时间捕获隐藏的3D几何形状。受雷达中使用的硬件设置和地震成像社区开发的基于波的图像形成模型的算法方法的启发，我们展示了一种新的绕过障碍物的方法。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Lindell_Acoustic_Non-Line-Of-Sight_Imaging_CVPR_2019_paper.pdf) |
| 2019 | Combinatorial Persistency Criteria for Multicut and Max-Cut | Jan-Hendrik Lange,  Bjoern Andres,  Paul Swoboda | In combinatorial optimization, partial variable assignments are called persistent if they agree with some optimal solution. We propose persistency criteria for the multicut and max-cut problem as well as fast combinatorial routines to verify them. The criteria that we derive are based on mappings that improve feasible multicuts, respectively cuts. Our elementary criteria can be checked enumeratively. The more advanced ones rely on fast algorithms for upper and lower bounds for the respective cut problems and max-flow techniques for auxiliary min-cut problems. Our methods can be used as a preprocessing technique for reducing problem sizes or for computing partial optimality guarantees for solutions output by heuristic solvers. We show the efficacy of our methods on instances of both problems from computer vision, biomedical image analysis and statistical physics. | 在组合优化中，部分变量赋值被称为持久性，如果它们与某些最优解一致。我们提出了多切割和最大切割问题的持久性标准，以及快速的组合例程来验证它们。我们导出的标准是基于改进可行多切割或切割的映射。我们的基本标准可以通过枚举检查。更高级的标准依赖于快速算法，用于相应切割问题的上下界，以及辅助最小切割问题的最大流技术。我们的方法可以用作减小问题规模的预处理技术，或用于计算启发式求解器输出的解决方案的部分最优性保证。我们展示了我们的方法在计算机视觉、生物医学图像分析和统计物理等问题实例上的有效性。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Lange_Combinatorial_Persistency_Criteria_for_Multicut_and_Max-Cut_CVPR_2019_paper.pdf) |
| 2019 | Networks for Joint Affine and Non-Parametric Image Registration | Zhengyang Shen,  Xu Han,  Zhenlin Xu,  Marc Niethammer | We introduce an end-to-end deep-learning framework for 3D medical image registration. In contrast to existing approaches, our framework combines two registration methods: an affine registration and a vector momentum-parameterized stationary velocity field (vSVF) model. Specifically, it consists of three stages. In the first stage, a multi-step affine network predicts affine transform parameters. In the second stage, we use a U-Net-like network to generate a momentum, from which a velocity field can be computed via smoothing. Finally, in the third stage, we employ a self-iterable map-based vSVF component to provide a non-parametric refinement based on the current estimate of the transformation map. Once the model is trained, a registration is completed in one forward pass. To evaluate the performance, we conducted longitudinal and cross-subject experiments on 3D magnetic resonance images (MRI) of the knee of the  Osteoarthritis Initiative (OAI) dataset. Results show that our framework achieves comparable performance to state-of-the-art medical image registration approaches, but it is much faster, with a better control of transformation regularity including the ability to produce approximately symmetric transformations, and combining affine as well as non-parametric registration. | 我们介绍了一种用于3D医学图像配准的端到端深度学习框架。与现有方法不同，我们的框架结合了两种配准方法：仿射配准和基于向量动量参数化的稳定速度场（vSVF）模型。具体而言，它包括三个阶段。在第一阶段，多步仿射网络预测仿射变换参数。在第二阶段，我们使用类似U-Net的网络生成动量，通过平滑可以计算出速度场。最后，在第三阶段，我们使用基于自迭代映射的vSVF组件基于当前转换映射的估计提供非参数化的细化。一旦模型训练完成，配准将在一个前向传递中完成。为了评估性能，我们在骨关节炎倡议（OAI）数据集的膝关节3D磁共振图像（MRI）上进行了纵向和跨受试者实验。结果表明，我们的框架实现了与最先进的医学图像配准方法相当的性能，但速度更快，对变换正则性有更好的控制，包括能够产生近似对称的变换，并结合了仿射以及非参数化配准。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Shen_Networks_for_Joint_Affine_and_Non-Parametric_Image_Registration_CVPR_2019_paper.pdf) |
| 2019 | Noise2Void - Learning Denoising From Single Noisy Images | Alexander Krull,  Tim-Oliver Buchholz,  Florian Jug | The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as Noise2Noise (N2N). Here, we introduce Noise2Void (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of Noise2Void drops in moderation and compares favorably to training-free denoising methods. | 图像去噪领域目前主要由基于鉴别式深度学习方法主导，这些方法是在噪声输入和清晰目标图像对上进行训练的。最近已经证明这种方法也可以在没有清晰目标的情况下进行训练。相反，可以使用独立的噪声图像对，这种方法被称为Noise2Noise（N2N）。在这里，我们介绍Noise2Void（N2V），这是一个将这个想法推进一步的训练方案。它不需要噪声图像对，也不需要清晰目标图像。因此，N2V允许我们直接在要去噪的数据集上进行训练，因此可以在其他方法无法应用时使用。特别有趣的是将其应用于生物医学图像数据，其中获取训练目标，无论是清晰的还是嘈杂的，通常是不可能的。我们将N2V的性能与具有清晰目标图像和/或噪声图像对的方法进行比较。直观上，N2V不能预期会胜过在训练过程中获得更多信息的方法。尽管如此，我们观察到Noise2Void的去噪性能中度下降，并与无需训练的去噪方法进行了有利的比较。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Krull_Noise2Void_-_Learning_Denoising_From_Single_Noisy_Images_CVPR_2019_paper.pdf) |
| 2019 | Elastic Boundary Projection for 3D Medical Image Segmentation | Tianwei Ni,  Lingxi Xie,  Huangjie Zheng,  Elliot K. Fishman,  Alan L. Yuille | We focus on an important yet challenging problem: using a 2D deep network to deal with 3D segmentation for medical image analysis. Existing approaches either applied multi-view planar (2D) networks or directly used volumetric (3D) networks for this purpose, but both of them are not ideal: 2D networks cannot capture 3D contexts effectively, and 3D networks are both memory-consuming and less stable arguably due to the lack of pre-trained models.  In this paper, we bridge the gap between 2D and 3D using a novel approach named Elastic Boundary Projection (EBP). The key observation is that, although the object is a 3D volume, what we really need in segmentation is to find its boundary which is a 2D surface. Therefore, we place a number of pivot points in the 3D space, and for each pivot, we determine its distance to the object boundary along a dense set of directions. This creates an elastic shell around each pivot which is initialized as a perfect sphere. We train a 2D deep network to determine whether each ending point falls within the object, and gradually adjust the shell so that it gradually converges to the actual shape of the boundary and thus achieves the goal of segmentation. EBP allows boundary-based segmentation without cutting a 3D volume into slices or patches, which stands out from conventional 2D and 3D approaches. EBP achieves promising accuracy in abdominal organ segmentation. Our code will be released on https://github.com/twni2016/Elastic-Boundary-Projection . | 我们关注一个重要但具有挑战性的问题：使用2D深度网络处理医学图像分割的3D问题。现有方法要么应用多视图平面（2D）网络，要么直接使用体积（3D）网络来实现这一目的，但两者都不理想：2D网络无法有效捕捉3D上下文，而3D网络则既消耗内存又不稳定，这可能是因为缺乏预训练模型。在本文中，我们通过一种名为弹性边界投影（EBP）的新方法来弥合2D和3D之间的差距。关键观察是，尽管对象是一个3D体积，但在分割中我们真正需要的是找到其边界，而边界是一个2D表面。因此，我们在3D空间中放置了许多枢轴点，并针对每个枢轴点确定其沿着密集方向到对象边界的距离。这样就在每个枢轴周围创建了一个弹性壳，该壳被初始化为一个完美的球体。我们训练一个2D深度网络来确定每个结束点是否落在对象内，并逐渐调整壳体，使其逐渐收敛到实际边界的形状，从而实现分割的目标。EBP允许基于边界的分割，而无需将3D体积切成切片或块，这与传统的2D和3D方法不同。EBP在腹部器官分割中取得了令人满意的准确性。我们的代码将在https://github.com/twni2016/Elastic-Boundary-Projection 上发布。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Ni_Elastic_Boundary_Projection_for_3D_Medical_Image_Segmentation_CVPR_2019_paper.pdf) |
| 2019 | Collaborative Learning of Semi-Supervised Segmentation and Classification for Medical Images | Yi Zhou,  Xiaodong He,  Lei Huang,  Li Liu,  Fan Zhu,  Shanshan Cui,  Ling Shao | Medical image analysis has two important research areas: disease grading and fine-grained lesion segmentation. Although the former problem often relies on the latter, the two are usually studied separately. Disease severity grading can be treated as a classification problem, which only requires image-level annotations, while the lesion segmentation requires stronger pixel-level annotations. However, pixel-wise data annotation for medical images is highly time-consuming and requires domain experts. In this paper, we propose a collaborative learning method to jointly improve the performance of disease grading and lesion segmentation by semi-supervised learning with an attention mechanism.  Given a small set of pixel-level annotated data, a multi-lesion mask generation model first performs the traditional semantic segmentation task. Then, based on initially predicted lesion maps for large quantities of image-level annotated data, a lesion attentive disease grading model is designed to improve the severity classification accuracy. Meanwhile, the lesion attention model can refine the lesion maps using class-specific information to fine-tune the segmentation model in a semi-supervised manner. An adversarial architecture is also integrated for training. With extensive experiments on a representative medical problem called diabetic retinopathy (DR), we validate the effectiveness of our method and achieve consistent improvements over state-of-the-art methods on three public datasets. | 医学图像分析有两个重要的研究领域: 疾病分级和细粒度病变分割。虽然前者问题通常依赖于后者，但这两个问题通常是分开研究的。疾病严重程度分级可以被视为一个分类问题，只需要图像级别的注释，而病变分割需要更强的像素级别注释。然而，对医学图像进行像素级数据注释非常耗时，并且需要领域专家。在本文中，我们提出了一种协作学习方法，通过注意机制的半监督学习共同改善疾病分级和病变分割的性能。给定一小部分像素级标注数据，一个多病变掩模生成模型首先执行传统的语义分割任务。然后，基于大量图像级别标注数据的初始预测病变图，设计了一个病变关注的疾病分级模型，以提高严重程度分类的准确性。同时，病变关注模型可以使用特定类别的信息调整病变图，以半监督的方式微调分割模型。还集成了对抗性架构进行训练。通过对一个名为糖尿病视网膜病变（DR）的代表性医学问题进行广泛实验，我们验证了我们方法的有效性，并在三个公共数据集上持续改进了现有方法。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Collaborative_Learning_of_Semi-Supervised_Segmentation_and_Classification_for_Medical_Images_CVPR_2019_paper.pdf) |
| 2019 | Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds | Zihao Liu,  Xiaowei Xu,  Tao Liu,  Qi Liu,  Yanzhi Wang,  Yiyu Shi,  Wujie Wen,  Meiping Huang,  Haiyun Yuan,  Jian Zhuang | Cloud based medical image analysis has become popular recently due to the high computation complexities of various deep neural network (DNN) based frameworks and the increasingly large volume of medical images that need to be processed. It has been demonstrated that for medical images the transmission from local to clouds is much more expensive than the computation in the clouds itself. Towards this, 3D image compression techniques have been widely applied to reduce the data traffic. However, most of the existing image compression techniques are developed around human vision, i.e., they are designed to minimize distortions that can be perceived by human eyes. In this paper, we will use deep learning based medical image segmentation as a vehicle and demonstrate that interestingly, machine and human view the compression quality differently. Medical images compressed with good quality w.r.t. human vision may result in inferior segmentation accuracy. We then design a machine vision oriented 3D image compression framework tailored for segmentation using DNNs. Our method automatically extracts and retains image features that are most important to the segmentation. Comprehensive experiments on widely adopted segmentation frameworks with HVSMR 2016 challenge dataset show that our method can achieve significantly higher segmentation accuracy at the same compression rate, or much better compression rate under the same segmentation accuracy, when compared with the existing JPEG 2000 method. To the best of the authors' knowledge, this is the first machine vision guided medical image compression framework for segmentation in the clouds.   | 最近，基于云的医学图像分析因各种基于深度神经网络（DNN）的框架的高计算复杂性和需要处理的医学图像数量日益增多而变得流行。已经证明，对于医学图像，从本地到云端的传输比云端的计算成本更高。为此，广泛应用了3D图像压缩技术以减少数据流量。然而，大多数现有的图像压缩技术是围绕人类视觉开发的，即它们旨在最小化人眼可以察觉到的失真。在本文中，我们将以基于深度学习的医学图像分割为载体，并展示有趣的是，机器和人类对压缩质量有不同的看法。相对于人类视觉良好压缩的医学图像可能导致较低的分割准确性。然后，我们设计了一种面向机器视觉的3D图像压缩框架，专为使用DNN进行分割而定制。我们的方法自动提取并保留对分割最重要的图像特征。对HVSMR 2016挑战数据集中广泛采用的分割框架进行了全面实验，结果显示，与现有的JPEG 2000方法相比，我们的方法在相同的压缩率下可以实现显着更高的分割准确性，或在相同的分割准确性下获得更好的压缩率。据作者所知，这是第一个用于云中分割的机器视觉引导的医学图像压缩框架。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Machine_Vision_Guided_3D_Medical_Image_Compression_for_Efficient_Transmission_CVPR_2019_paper.pdf) |
| 2019 | Context-Aware Spatio-Recurrent Curvilinear Structure Segmentation | Feigege Wang,  Yue Gu,  Wenxi Liu,  Yuanlong Yu,  Shengfeng He,  Jia Pan | Curvilinear structures are frequently observed in various images in different forms, such as blood vessels or neuronal boundaries in biomedical images. In this paper, we propose a novel curvilinear structure segmentation approach using context-aware spatio-recurrent networks. Instead of directly segmenting the whole image or densely segmenting fixed-sized local patches, our method recurrently samples patches with varied scales from the target image with learned policy and processes them locally, which is similar to the behavior of changing retinal fixations in the human visual system and it is beneficial for capturing the multi-scale or hierarchical modality of the complex curvilinear structures.  In specific, the policy of choosing local patches is attentively learned based on the contextual information of the image and the historical sampling experience.   In this way, with more patches sampled and refined, the segmentation of the whole image can be progressively improved. To validate our approach, comparison experiments on different types of image data are conducted and the sampling procedures for exemplar images are illustrated. We demonstrate that our method achieves the state-of-the-art performance in public datasets.  | 曲线结构经常以不同形式在各种图像中观察到，如生物医学图像中的血管或神经元边界。本文提出了一种使用上下文感知的空间循环网络的新型曲线结构分割方法。与直接分割整个图像或密集分割固定大小的局部补丁不同，我们的方法通过学习策略从目标图像中反复采样具有不同尺度的补丁，并在本地处理它们，这类似于人类视觉系统中改变视网膜固定点的行为，并有利于捕捉复杂曲线结构的多尺度或分层模态。具体而言，选择局部补丁的策略是基于图像的上下文信息和历史采样经验敏锐地学习的。通过更多补丁的采样和精炼，整个图像的分割可以逐步改善。为验证我们的方法，在不同类型的图像数据上进行了比较实验，并说明了示例图像的采样过程。我们证明我们的方法在公共数据集中取得了最先进的性能。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Context-Aware_Spatio-Recurrent_Curvilinear_Structure_Segmentation_CVPR_2019_paper.pdf) |
| 2019 | A Poisson-Gaussian Denoising Dataset With Real Fluorescence Microscopy Images | Yide Zhang,  Yinhao Zhu,  Evan Nichols,  Qingfei Wang,  Siyuan Zhang,  Cody Smith,  Scott Howard | Fluorescence microscopy has enabled a dramatic development in modern biology. Due to its inherently weak signal, fluorescence microscopy is not only much noisier than photography, but also presented with Poisson-Gaussian noise where Poisson noise, or shot noise, is the dominating noise source. To get clean fluorescence microscopy images, it is highly desirable to have effective denoising algorithms and datasets that are specifically designed to denoise fluorescence microscopy images. While such algorithms exist, no such datasets are available. In this paper, we fill this gap by constructing a dataset - the Fluorescence Microscopy Denoising (FMD) dataset - that is dedicated to Poisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence microscopy images obtained with commercial confocal, two-photon, and wide-field microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissues. We use image averaging to effectively obtain ground truth images and 60,000 noisy images with different noise levels. We use this dataset to benchmark 10 representative denoising algorithms and find that deep learning methods have the best performance. To our knowledge, this is the first real microscopy image dataset for Poisson-Gaussian denoising purposes and it could be an important tool for high-quality, real-time denoising applications in biomedical research. | 荧光显微镜已经在现代生物学中取得了巨大的发展。由于其固有的信号较弱，荧光显微镜不仅比摄影噪音要大得多，而且还存在泊松-高斯噪音，其中泊松噪音或称为光子噪音是主要的噪音来源。为了获得清晰的荧光显微镜图像，高效的去噪算法和专门设计用于去噪荧光显微镜图像的数据集是非常必要的。虽然存在这样的算法，但却没有相应的数据集。本文通过构建一个数据集 - 荧光显微镜去噪（FMD）数据集来填补这一空白，该数据集专门用于泊松-高斯去噪。该数据集包括使用商业共焦、双光子和广场显微镜获得的12,000幅真实荧光显微镜图像，以及代表性的生物样本，如细胞、斑马鱼和小鼠脑组织。我们使用图像平均值有效地获得基准图像和60,000幅具有不同噪音水平的嘈杂图像。我们使用该数据集来评估10种代表性的去噪算法，并发现深度学习方法具有最佳性能。据我们所知，这是用于泊松-高斯去噪目的的第一个真实显微镜图像数据集，它可能成为生物医学研究中高质量、实时去噪应用的重要工具。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_A_Poisson-Gaussian_Denoising_Dataset_With_Real_Fluorescence_Microscopy_Images_CVPR_2019_paper.pdf) |
| 2019 | Learning Active Contour Models for Medical Image Segmentation | Xu Chen,  Bryan M. Williams,  Srinivasa R. Vallabhaneni,  Gabriela Czanner,  Rachel Williams,  Yalin Zheng | Image segmentation is an important step in medical image processing and has been widely studied and developed for refinement of clinical analysis and applications. New models based on deep learning have improved results but are restricted to pixel-wise fitting of the segmentation map. Our aim was to tackle this limitation by developing a new model based on deep learning which takes into account the area inside as well as outside the region of interest as well as the size of boundaries during learning. Specifically, we propose a new loss function which incorporates area and size information and integrates this into a dense deep learning model. We evaluated our approach on a dataset of more than 2,000 cardiac MRI scans. Our results show that the proposed loss function outperforms other mainstream loss function Cross-entropy on two common segmentation networks. Our loss function is robust while using different hyperparameter lambda. | 图像分割是医学图像处理中的重要步骤，已经广泛研究和发展，用于改进临床分析和应用。基于深度学习的新模型改善了结果，但仅限于对分割图进行像素级拟合。我们的目标是通过开发一种基于深度学习的新模型来解决这一限制，该模型考虑感兴趣区域内外以及学习过程中边界的大小。具体来说，我们提出了一种新的损失函数，该损失函数结合了区域和大小信息，并将其整合到密集深度学习模型中。我们在超过2000个心脏MRI扫描数据集上评估了我们的方法。我们的结果表明，所提出的损失函数在两种常见分割网络上优于其他主流损失函数Cross-entropy。我们的损失函数在使用不同的超参数lambda时具有鲁棒性。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Learning_Active_Contour_Models_for_Medical_Image_Segmentation_CVPR_2019_paper.pdf) |
| 2019 | Learning From Noisy Labels by Regularized Estimation of Annotator Confusion | Ryutaro Tanno,  Ardavan Saeedi,  Swami Sankaranarayanan,  Daniel C. Alexander,  Nathan Silberman | The predictive performance of supervised learning algorithms depends on the quality of labels. In a typical label collection process, multiple annotators provide subjective noisy estimates of the "truth" under the influence of their varying skill-levels and biases. Blindly treating these noisy labels as the ground truth limits the accuracy of learning algorithms in the presence of strong disagreement. This problem is critical for applications in domains such as medical imaging where both the annotation cost and inter-observer variability are high. In this work, we present a method for simultaneously learning the individual annotator model and the underlying true label distribution, using only noisy observations. Each annotator is modeled by a confusion matrix that is jointly estimated along with the classifier predictions. We propose to add a regularization term to the loss function that encourages convergence to the true annotator confusion matrix. We provide a theoretical argument as to how the regularization is essential to our approach both for the case of single annotator and multiple annotators. Despite the simplicity of the idea, experiments on image classification tasks with both simulated and real labels show that our method either outperforms or performs on par with the state-of-the-art methods and is capable of estimating the skills of annotators even with a single label available per image. | 监督学习算法的预测性能取决于标签的质量。在典型的标签收集过程中，多个标注者在其不同的技能水平和偏见的影响下提供主观嘈杂的“真相”估计。盲目地将这些嘈杂的标签视为基本事实会限制学习算法在存在强烈分歧时的准确性。这一问题在医学影像等领域的应用中至关重要，其中标注成本和观察者之间的差异性都很高。在这项工作中，我们提出了一种方法，用于同时学习个体标注者模型和潜在真实标签分布，仅使用嘈杂的观察结果。每个标注者都由一个混淆矩阵建模，该混淆矩阵与分类器预测一起联合估计。我们建议向损失函数添加一个正则化项，以鼓励收敛到真实的标注者混淆矩阵。我们提供了一个理论论据，说明了正则化对我们的方法的重要性，无论是对于单个标注者还是多个标注者的情况。尽管这个想法很简单，但在模拟和真实标签的图像分类任务上进行的实验表明，我们的方法要么优于现有方法，要么与之相当，并且能够估计标注者的技能，即使每幅图像只有一个标签可用。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Tanno_Learning_From_Noisy_Labels_by_Regularized_Estimation_of_Annotator_Confusion_CVPR_2019_paper.pdf) |
| 2019 | Attention Based Glaucoma Detection: A Large-Scale Database and CNN Model | Liu Li,  Mai Xu,  Xiaofei Wang,  Lai Jiang,  Hanruo Liu | Recently, the attention mechanism has been successfully applied in convolutional neural networks (CNNs), significantly boosting the performance of many computer vision tasks. Unfortunately, few medical image recognition approaches incorporate the attention mechanism in the CNNs. In particular, there exists high redundancy in fundus images for glaucoma detection, such that the attention mechanism has potential in improving the performance of CNN-based glaucoma detection. This paper proposes an attention-based CNN for glaucoma detection (AG-CNN). Specifically, we first establish a large-scale attention based glaucoma (LAG) database, which includes 5,824 fundus images labeled with either positive glaucoma (2,392) or negative glaucoma (3,432). The attention maps of the ophthalmologists are also collected in LAG database through a simulated eye-tracking experiment. Then, a new structure of AG-CNN is designed, including an attention prediction subnet, a pathological area localization subnet and a glaucoma classification subnet. Different from other attention-based CNN methods, the features are also visualized as the localized pathological area, which can advance the performance of glaucoma detection. Finally, the experiment results show that the proposed AG-CNN approach significantly advances state-of-the-art glaucoma detection. | 最近，注意力机制已成功应用于卷积神经网络（CNN），显著提升了许多计算机视觉任务的性能。不幸的是，很少有医学图像识别方法在CNN中引入了注意力机制。特别是，在青光眼检测的眼底图像中存在较高的冗余性，因此注意力机制有潜力提高基于CNN的青光眼检测性能。本文提出了一种基于注意力的CNN用于青光眼检测（AG-CNN）。具体来说，我们首先建立了一个大规模的基于注意力的青光眼（LAG）数据库，其中包括5,824张标记为阳性青光眼（2,392张）或阴性青光眼（3,432张）的眼底图像。通过模拟眼动实验还收集了LAG数据库中眼科医生的注意力地图。然后，设计了一个新的AG-CNN结构，包括一个注意力预测子网络，一个病理区域定位子网络和一个青光眼分类子网络。与其他基于注意力的CNN方法不同，这些特征还可视化为定位的病理区域，这可以提高青光眼检测的性能。最后，实验结果表明，所提出的AG-CNN方法显著提高了最先进的青光眼检测水平。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Attention_Based_Glaucoma_Detection_A_Large-Scale_Database_and_CNN_Model_CVPR_2019_paper.pdf) |
| 2019 | DuDoNet: Dual Domain Network for CT Metal Artifact Reduction | Wei-An Lin,  Haofu Liao,  Cheng Peng,  Xiaohang Sun,  Jingdan Zhang,  Jiebo Luo,  Rama Chellappa,  Shaohua Kevin Zhou | Computed tomography (CT) is an imaging modality widely used for medical diagnosis and treatment. CT images are often corrupted by undesirable artifacts when metallic implants are carried by patients, which creates the problem of metal artifact reduction (MAR). Existing methods for reducing the artifacts due to metallic implants are inadequate for two main reasons. First, metal artifacts are structured and non-local so that simple image domain enhancement approaches would not suffice. Second, the MAR approaches which attempt to reduce metal artifacts in the X-ray projection (sinogram) domain inevitably lead to severe secondary artifact due to sinogram inconsistency. To overcome these difficulties, we propose an end-to-end trainable Dual Domain Network (DuDoNet) to simultaneously restore sinogram consistency and enhance CT images. The linkage between the sigogram and image domains is a novel Radon inversion layer that allows the gradients to back-propagate from the image domain to the sinogram domain during training. Extensive experiments show that our method achieves significant improvements over other single domain MAR approaches. To the best of our knowledge, it is the first end-to-end dual-domain network for MAR. | 计算机断层扫描（CT）是一种广泛应用于医学诊断和治疗的成像技术。当患者携带金属植入物时，CT图像经常会受到不良伪影的影响，从而产生金属伪影减少（MAR）的问题。现有的减少金属植入物造成的伪影的方法存在两个主要缺点。首先，金属伪影是结构化的和非局部的，因此简单的图像域增强方法是不够的。其次，试图在X射线投影（正弦图）域中减少金属伪影的MAR方法不可避免地会导致由于正弦图不一致而产生严重的次生伪影。为了克服这些困难，我们提出了一个端到端可训练的双域网络（DuDoNet），以同时恢复正弦图的一致性并增强CT图像。正弦图和图像域之间的连接是一个新颖的Radon反演层，允许梯度在训练期间从图像域向正弦图域反向传播。大量实验证明，我们的方法在其他单域MAR方法上取得了显著的改进。据我们所知，这是第一个用于MAR的端到端双域网络。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Lin_DuDoNet_Dual_Domain_Network_for_CT_Metal_Artifact_Reduction_CVPR_2019_paper.pdf) |
| 2019 | Learning Multi-Class Segmentations From Single-Class Datasets | Konstantin Dmitriev,  Arie E. Kaufman | Multi-class segmentation has recently achieved significant performance in natural images and videos. This achievement is due primarily to the public availability of large multi-class datasets. However, there are certain domains, such as biomedical images, where obtaining sufficient multi-class annotations is a laborious and often impossible task and only single-class datasets are available. While existing segmentation research in such domains use private multi-class datasets or focus on single-class segmentations, we propose a unified highly efficient framework for robust simultaneous learning of multi-class segmentations by combining single-class datasets and utilizing a novel way of conditioning a convolutional network for the purpose of segmentation. We demonstrate various ways of incorporating the conditional information, perform an extensive evaluation, and show compelling multi-class segmentation performance on biomedical images, which outperforms current state-of-the-art solutions (up to 2.7%). Unlike current solutions, which are meticulously tailored for particular single-class datasets, we utilize datasets from a variety of sources. Furthermore, we show the applicability of our method also to natural images and evaluate it on the Cityscapes dataset. We further discuss other possible applications of our proposed framework. | 最近，多类分割在自然图像和视频中取得了显著的表现。这一成就主要归功于大型多类数据集的公开可用性。然而，在某些领域，如生物医学图像，获取足够的多类注释是一项费时且常常不可能完成的任务，仅有单类数据集可用。虽然目前在这些领域的分割研究使用私有多类数据集或专注于单类分割，我们提出了一个统一高效的框架，通过结合单类数据集并利用一种新颖的方式对卷积网络进行条件化，实现了多类分割的稳健同时学习。我们展示了各种整合条件信息的方式，进行了广泛的评估，并展示了在生物医学图像上引人注目的多类分割性能，超过了当前最先进的解决方案（高达2.7%）。与目前为特定单类数据集精心定制的解决方案不同，我们利用了各种来源的数据集。此外，我们还展示了我们的方法对自然图像的适用性，并在Cityscapes数据集上进行了评估。我们进一步讨论了我们提出的框架的其他可能应用。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Dmitriev_Learning_Multi-Class_Segmentations_From_Single-Class_Datasets_CVPR_2019_paper.pdf) |
| 2019 | Collaborative Global-Local Networks for Memory-Efficient Segmentation of Ultra-High Resolution Images | Wuyang Chen,  Ziyu Jiang,  Zhangyang Wang,  Kexin Cui,  Xiaoning Qian | Segmentation of ultra-high resolution images is increasingly demanded, yet poses significant challenges for algorithm efficiency, in particular considering the (GPU) memory limits. Current approaches either downsample an ultra-high resolution image, or crop it into small patches for separate processing. In either way, the loss of local fine details or global contextual information results in limited segmentation accuracy. We propose collaborative Global-Local Networks (GLNet) to effectively preserve both global and local information in a highly memory-efficient manner. GLNet is composed of a global branch and a local branch, taking the downsampled entire image and its cropped local patches as respective inputs. For segmentation, GLNet deeply fuses feature maps from two branches, capturing both the high-resolution fine structures from  zoomed-in local patches and the contextual dependency from the  downsampled input. To further resolve the potential class imbalance problem between background and foreground regions, we present a coarse-to-fine variant of GLNet, also being memory-efficient. Extensive experiments and analyses have been performed on three real-world ultra-high aerial and medical image datasets (resolution up to 30 million pixels). With only one single 1080Ti GPU and less than 2GB memory used, our GLNet yields high-quality segmentation results, and achieves much more competitive accuracy-memory usage trade-offs compared to state-of-the-arts. | 超高分辨率图像的分割需求日益增加，但在算法效率方面存在重大挑战，特别是考虑到（GPU）内存限制。当前的方法要么对超高分辨率图像进行降采样，要么将其裁剪成小块进行分开处理。无论哪种方式，都会导致局部细节或全局背景信息的丢失，从而限制了分割的准确性。我们提出了协作全局-局部网络（GLNet），以高度节省内存的方式有效保留全局和局部信息。GLNet由全局分支和局部分支组成，分别将降采样的整个图像和裁剪的局部块作为输入。对于分割，GLNet深度融合了两个分支的特征图，捕捉了来自放大局部块的高分辨率细微结构和来自降采样输入的上下文依赖性。为了进一步解决背景和前景区域之间潜在的类别不平衡问题，我们提出了GLNet的粗到精变体，同样具有高效的内存利用率。我们在三个真实世界的超高空中和医学图像数据集上进行了大量实验和分析（分辨率高达3000万像素）。仅使用一块1080Ti GPU和少于2GB内存，我们的GLNet产生了高质量的分割结果，并与当前技术相比取得了更有竞争力的准确性-内存使用权衡结果。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Collaborative_Global-Local_Networks_for_Memory-Efficient_Segmentation_of_Ultra-High_Resolution_Images_CVPR_2019_paper.pdf) |
| 2019 | Data Augmentation Using Learned Transformations for One-Shot Medical Image Segmentation | Amy Zhao,  Guha Balakrishnan,  Fredo Durand,  John V. Guttag,  Adrian V. Dalca | Image segmentation is an important task in many medical applications. Methods based on convolutional neural networks attain state-of-the-art accuracy; however, they typically rely on supervised training with large labeled datasets. Labeling medical images requires significant expertise and time, and typical hand-tuned approaches for data augmentation fail to capture the complex variations in such images.   We present an automated data augmentation method for synthesizing labeled medical images. We demonstrate our method on the task of segmenting magnetic resonance imaging (MRI) brain scans. Our method requires only a single segmented scan, and leverages other unlabeled scans in a semi-supervised approach. We learn a model of transformations from the images, and use the model along with the labeled example to synthesize additional labeled examples. Each transformation is comprised of a spatial deformation field and an intensity change, enabling the synthesis of complex effects such as variations in anatomy and image acquisition procedures. We show that training a supervised segmenter with these new examples provides significant improvements over state-of-the-art methods for one-shot biomedical image segmentation. | 图像分割是许多医学应用中的重要任务。基于卷积神经网络的方法实现了最先进的准确性; 但是，它们通常依赖于具有大型标记数据集的监督训练。标记医学图像需要重要的专业知识和时间，并且典型的手工调整方法用于数据增强无法捕捉这些图像中的复杂变化。我们提出了一种用于合成标记医学图像的自动数据增强方法。我们在分割磁共振成像（MRI）脑扫描的任务上展示了我们的方法。我们的方法仅需要一个分割扫描，并利用半监督方法中的其他未标记扫描。我们从图像中学习变换模型，并使用该模型以及标记示例来合成额外的标记示例。每个转换由空间变形场和强度变化组成，从而实现解剖学和图像获取过程中的变化等复杂效果的合成。我们展示了使用这些新示例训练监督分段器相比于一次性生物医学图像分割的最先进方法提供了显著的改进。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_Data_Augmentation_Using_Learned_Transformations_for_One-Shot_Medical_Image_Segmentation_CVPR_2019_paper.pdf) |
| 2019 | Holistic and Comprehensive Annotation of Clinically Significant Findings on Diverse CT Images: Learning From Radiology Reports and Label Ontology | Ke Yan,  Yifan Peng,  Veit Sandfort,  Mohammadhadi Bagheri,  Zhiyong Lu,  Ronald M. Summers | In radiologists' routine work, one major task is to read a medical image, e.g., a CT scan, find significant lesions, and describe them in the radiology report. In this paper, we study the lesion description or annotation problem. Given a lesion image, our aim is to predict a comprehensive set of relevant labels, such as the lesion's body part, type, and attributes, which may assist downstream fine-grained diagnosis. To address this task, we first design a deep learning module to extract relevant semantic labels from the radiology reports associated with the lesion images. With the images and text-mined labels, we propose a lesion annotation network (LesaNet) based on a multilabel convolutional neural network (CNN) to learn all labels holistically. Hierarchical relations and mutually exclusive relations between the labels are leveraged to improve the label prediction accuracy. The relations are utilized in a label expansion strategy and a reliable hard example mining algorithm. We also attach a simple score propagation layer on LesaNet to enhance recall and explore implicit relation between labels. Multilabel metric learning is combined with classification to enable interpretable prediction. We evaluated LesaNet on the public DeepLesion dataset, which contains over 32K diverse lesion images. Experiments show that LesaNet can precisely annotate the lesions using an ontology of 171 fine-grained labels with an average AUC of 0.9344. | 在放射科医生的日常工作中，一个主要任务是阅读医学影像，例如CT扫描，找到重要的病变，并在放射学报告中描述它们。本文研究了病变描述或注释问题。给定一个病变图像，我们的目标是预测一组相关标签，例如病变的身体部位、类型和属性，这些标签可能有助于下游细粒度诊断。为了解决这个任务，我们首先设计了一个深度学习模块，从与病变图像相关的放射学报告中提取相关的语义标签。通过图像和文本挖掘的标签，我们提出了一种基于多标签卷积神经网络（CNN）的病变注释网络（LesaNet），以全面学习所有标签。层次关系和标签之间的相互排斥关系被利用来提高标签预测的准确性。这些关系被用于标签扩展策略和可靠的难例挖掘算法。我们还在LesaNet上附加了一个简单的分数传播层，以增强召回率并探索标签之间的隐含关系。多标签度量学习与分类相结合，实现可解释的预测。我们在包含超过32K多样化病变图像的公共DeepLesion数据集上评估了LesaNet。实验表明，LesaNet可以使用171个精细标签的本体精确地注释病变，平均AUC为0.9344。 | [link](https://openaccess.thecvf.com/content_CVPR_2019/papers/Yan_Holistic_and_Comprehensive_Annotation_of_Clinically_Significant_Findings_on_Diverse_CVPR_2019_paper.pdf) |
