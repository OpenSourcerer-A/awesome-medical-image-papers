| 年份 | 题目 | 作者 | 摘要 | 中文摘要 | link |
| --- | --- | --- | --- | --- | --- |
| 2023 | TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation | Devavrat Tomar, Guillaume Vray, Behzad Bozorgtabar, Jean-Philippe Thiran | Most recent test-time adaptation methods focus on only classification tasks, use specialized network architectures, destroy model calibration or rely on lightweight information from the source domain. To tackle these issues, this paper proposes a novel Test-time Self-Learning method with automatic Adversarial augmentation dubbed TeSLA for adapting a pre-trained source model to the unlabeled streaming test data. In contrast to conventional self-learning methods based on cross-entropy, we introduce a new test-time loss function through an implicitly tight connection with the mutual information and online knowledge distillation. Furthermore, we propose a learnable efficient adversarial augmentation module that further enhances online knowledge distillation by simulating high entropy augmented images. Our method achieves state-of-the-art classification and segmentation results on several benchmarks and types of domain shifts, particularly on challenging measurement shifts of medical images. TeSLA also benefits from several desirable properties compared to competing methods in terms of calibration, uncertainty metrics, insensitivity to model architectures, and source training strategies, all supported by extensive ablations. Our code and models are available at https://github.com/devavratTomar/TeSLA. | 最近的测试时间适应方法主要关注于分类任务，使用专门的网络架构，破坏模型校准或依赖于源域的轻量级信息。为了解决这些问题，本文提出了一种名为TeSLA的新颖的测试时间自学习方法，其中包括自动对抗增强，用于将预训练的源模型调整到未标记的流式测试数据。与基于交叉熵的传统自学习方法相比，我们通过与互信息和在线知识蒸馏的隐式紧密连接引入了一种新的测试时间损失函数。此外，我们提出了一个可学习的高效对抗增强模块，通过模拟高熵增强图像进一步增强了在线知识蒸馏。我们的方法在几个基准和域转移类型上实现了最先进的分类和分割结果，特别是在医学图像的挑战性测量转移上表现出色。与竞争方法相比，TeSLA在校准、不确定性指标、对模型架构的不敏感性和源训练策略等方面具有几个理想的特性，所有这些都得到了广泛的实验证明。我们的代码和模型可在https://github.com/devavratTomar/TeSLA 上获得。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Tomar_TeSLA_Test-Time_Self-Learning_With_Automatic_Adversarial_Augmentation_CVPR_2023_paper.pdf) |
| 2023 | Pseudo-Label Guided Contrastive Learning for Semi-Supervised Medical Image Segmentation | Hritam Basak, Zhaozheng Yin | Although recent works in semi-supervised learning (SemiSL) have accomplished significant success in natural image segmentation, the task of learning discriminative representations from limited annotations has been an open problem in medical images. Contrastive Learning (CL) frameworks use the notion of similarity measure which is useful for classification problems, however, they fail to transfer these quality representations for accurate pixel-level segmentation. To this end, we propose a novel semi-supervised patch-based CL framework for medical image segmentation without using any explicit pretext task. We harness the power of both CL and SemiSL, where the pseudo-labels generated from SemiSL aid CL by providing additional guidance, whereas discriminative class information learned in CL leads to accurate multi-class segmentation. Additionally, we formulate a novel loss that synergistically encourages inter-class separability and intra-class compactness among the learned representations. A new inter-patch semantic disparity mapping using average patch entropy is employed for a guided sampling of positives and negatives in the proposed CL framework. Experimental analysis on three publicly available datasets of multiple modalities reveals the superiority of our proposed method as compared to the state-of-the-art methods. Code is available at: https://github.com/hritam-98/PatchCL-MedSeg. | 近年来，在半监督学习（SemiSL）领域取得了显著成功，特别是在自然图像分割方面，然而，在医学图像中，从有限注释中学习具有区分性表示的任务一直是一个悬而未决的问题。对比学习（CL）框架使用相似性度量的概念对分类问题很有用，然而，它们未能将这些高质量表示转移到准确的像素级分割中。为此，我们提出了一种新颖的基于补丁的半监督对比学习框架，用于医学图像分割，无需使用任何显式的预训练任务。我们充分利用CL和SemiSL的力量，其中由SemiSL生成的伪标签通过提供额外的指导来辅助CL，而CL中学习的区分性类信息则导致准确的多类分割。此外，我们制定了一种新型损失，协同促进所学表示之间的类间可分性和类内紧凑性。在所提出的CL框架中，通过使用平均补丁熵进行新的补丁间语义差异映射，以引导正负样本的采样。对三个公开可用的多模态数据集进行的实验分析表明，与最先进的方法相比，我们提出的方法具有更高的优越性。代码可在https://github.com/hritam-98/PatchCL-MedSeg中找到。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Basak_Pseudo-Label_Guided_Contrastive_Learning_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf) |
| 2023 | Feature Alignment and Uniformity for Test Time Adaptation | Shuai Wang, Daoan Zhang, Zipei Yan, Jianguo Zhang, Rui Li | Test time adaptation (TTA) aims to adapt deep neural networks when receiving out of distribution test domain samples. In this setting, the model can only access online unlabeled test samples and pre-trained models on the training domains. We first address TTA as a feature revision problem due to the domain gap between source domains and target domains. After that, we follow the two measurements alignment and uniformity to discuss the test time feature revision. For test time feature uniformity, we propose a test time self-distillation strategy to guarantee the consistency of uniformity between representations of the current batch and all the previous batches. For test time feature alignment, we propose a memorized spatial local clustering strategy to align the representations among the neighborhood samples for the upcoming batch. To deal with the common noisy label problem, we propound the entropy and consistency filters to select and drop the possible noisy labels. To prove the scalability and efficacy of our method, we conduct experiments on four domain generalization benchmarks and four medical image segmentation tasks with various backbones. Experiment results show that our method not only improves baseline stably but also outperforms existing state-of-the-art test time adaptation methods. | 测试时间适应（TTA）旨在在接收到分布测试域样本时调整深度神经网络。在这种情况下，模型只能访问在线未标记的测试样本和在训练域上预训练的模型。我们首先将TTA视为特征修订问题，因为源域和目标域之间存在领域差异。之后，我们遵循两个衡量标准：对齐和一致性，讨论测试时间特征修订。对于测试时间特征一致性，我们提出了一种测试时间自我蒸馏策略，以确保当前批次和所有先前批次的表示之间的一致性。对于测试时间特征对齐，我们提出了一种记忆空间局部聚类策略，以对齐邻域样本之间的表示，以应对即将到来的批次。为了处理常见的嘈杂标签问题，我们提出了熵和一致性过滤器来选择和删除可能的嘈杂标签。为了证明我们方法的可扩展性和有效性，我们在四个领域泛化基准测试和四个医学图像分割任务上进行实验，使用各种主干网络。实验结果表明，我们的方法不仅稳定提高了基线，而且优于现有的最先进的测试时间适应方法。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Feature_Alignment_and_Uniformity_for_Test_Time_Adaptation_CVPR_2023_paper.pdf) |
| 2023 | 3D-Aware Facial Landmark Detection via Multi-View Consistent Training on Synthetic Data | Libing Zeng, Lele Chen, Wentao Bao, Zhong Li, Yi Xu, Junsong Yuan, Nima Khademi Kalantari | Accurate facial landmark detection on wild images plays an essential role in human-computer interaction, entertainment, and medical applications. Existing approaches have limitations in enforcing 3D consistency while detecting 3D/2D facial landmarks due to the lack of multi-view in-the-wild training data. Fortunately, with the recent advances in generative visual models and neural rendering, we have witnessed rapid progress towards high quality 3D image synthesis. In this work, we leverage such approaches to construct a synthetic dataset and propose a novel multi-view consistent learning strategy to improve 3D facial landmark detection accuracy on in-the-wild images. The proposed 3D-aware module can be plugged into any learning-based landmark detection algorithm to enhance its accuracy. We demonstrate the superiority of the proposed plug-in module with extensive comparison against state-of-the-art methods on several real and synthetic datasets. | 野外图像上准确的面部关键点检测在人机交互、娱乐和医疗应用中起着至关重要的作用。现有方法在检测3D/2D面部关键点时存在一定局限性，原因是缺乏多视角野外训练数据以强化3D一致性。幸运的是，随着生成视觉模型和神经渲染技术的最新进展，我们见证了向高质量3D图像合成的快速进展。在这项工作中，我们利用这些方法构建了一个合成数据集，并提出了一种新颖的多视角一致学习策略，以提高野外图像上的3D面部关键点检测准确性。提出的3D感知模块可以插入任何基于学习的关键点检测算法，以提升其准确性。我们通过与几个真实和合成数据集上最先进方法的广泛比较，展示了所提出的插件模块的优越性。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_3D-Aware_Facial_Landmark_Detection_via_Multi-View_Consistent_Training_on_Synthetic_CVPR_2023_paper.pdf) |
| 2023 | Fair Federated Medical Image Segmentation via Client Contribution Estimation | Meirui Jiang, Holger R. Roth, Wenqi Li, Dong Yang, Can Zhao, Vishwesh Nath, Daguang Xu, Qi Dou, Ziyue Xu | How to ensure fairness is an important topic in federated learning (FL). Recent studies have investigated how to reward clients based on their contribution (collaboration fairness), and how to achieve uniformity of performance across clients (performance fairness). Despite achieving progress on either one, we argue that it is critical to consider them together, in order to engage and motivate more diverse clients joining FL to derive a high-quality global model. In this work, we propose a novel method to optimize both types of fairness simultaneously. Specifically, we propose to estimate client contribution in gradient and data space. In gradient space, we monitor the gradient direction differences of each client with respect to others. And in data space, we measure the prediction error on client data using an auxiliary model. Based on this contribution estimation, we propose a FL method, federated training via contribution estimation (FedCE), i.e., using estimation as global model aggregation weights. We have theoretically analyzed our method and empirically evaluated it on two real-world medical datasets. The effectiveness of our approach has been validated with significant performance improvements, better collaboration fairness, better performance fairness, and comprehensive analytical studies. | 在联邦学习（FL）中如何确保公平性是一个重要课题。最近的研究探讨了如何根据客户的贡献来奖励他们（协作公平性），以及如何在客户之间实现性能均衡（性能公平性）。尽管在其中一个方面取得了进展，但我们认为考虑二者一起是至关重要的，以便吸引和激励更多不同类型的客户加入FL，从而得到高质量的全局模型。在这项工作中，我们提出了一种新的方法，同时优化两种类型的公平性。具体来说，我们提出在梯度和数据空间中估计客户的贡献。在梯度空间中，我们监测每个客户相对于其他客户的梯度方向差异。而在数据空间中，我们使用辅助模型来衡量客户数据上的预测误差。基于这种贡献估计，我们提出了一种FL方法，即通过贡献估计进行联邦训练（FedCE），即使用估计作为全局模型聚合权重。我们已从理论上分析了我们的方法，并在两个真实世界的医疗数据集上进行了实证评估。我们的方法的有效性已通过显著的性能改进、更好的协作公平性、更好的性能公平性以及全面的分析研究得到验证。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Fair_Federated_Medical_Image_Segmentation_via_Client_Contribution_Estimation_CVPR_2023_paper.pdf) |
| 2023 | Directional Connectivity-Based Segmentation of Medical Images | Ziyun Yang, Sina Farsiu | Anatomical consistency in biomarker segmentation is crucial for many medical image analysis tasks. A promising paradigm for achieving anatomically consistent segmentation via deep networks is incorporating pixel connectivity, a basic concept in digital topology, to model inter-pixel relationships. However, previous works on connectivity modeling have ignored the rich channel-wise directional information in the latent space. In this work, we demonstrate that effective disentanglement of directional sub-space from the shared latent space can significantly enhance the feature representation in the connectivity-based network. To this end, we propose a directional connectivity modeling scheme for segmentation that decouples, tracks, and utilizes the directional information across the network. Experiments on various public medical image segmentation benchmarks show the effectiveness of our model as compared to the state-of-the-art methods. Code is available at https://github.com/Zyun-Y/DconnNet. | 生物标记物分割中的解剖一致性对于许多医学图像分析任务至关重要。通过深度网络实现解剖一致分割的一种有前途的范例是整合像素连接性，这是数字拓扑中的基本概念，用于建模像素之间的关系。然而，先前关于连接性建模的研究忽略了潜在空间中丰富的通道方向信息。在这项工作中，我们证明了从共享潜在空间中有效解开方向子空间可以显著增强基于连接性的网络中的特征表示。为此，我们提出了一种用于分割的方向连接性建模方案，该方案解耦、跟踪和利用整个网络中的方向信息。在各种公共医学图像分割基准测试上的实验显示，我们的模型相对于最先进的方法的有效性。代码可在https://github.com/Zyun-Y/DconnNet 上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Directional_Connectivity-Based_Segmentation_of_Medical_Images_CVPR_2023_paper.pdf) |
| 2023 | Geometric Visual Similarity Learning in 3D Medical Image Self-Supervised Pre-Training | Yuting He, Guanyu Yang, Rongjun Ge, Yang Chen, Jean-Louis Coatrieux, Boyu Wang, Shuo Li | Learning inter-image similarity is crucial for 3D medical images self-supervised pre-training, due to their sharing of numerous same semantic regions. However, the lack of the semantic prior in metrics and the semantic-independent variation in 3D medical images make it challenging to get a reliable measurement for the inter-image similarity, hindering the learning of consistent representation for same semantics. We investigate the challenging problem of this task, i.e., learning a consistent representation between images for a clustering effect of same semantic features. We propose a novel visual similarity learning paradigm, Geometric Visual Similarity Learning, which embeds the prior of topological invariance into the measurement of the inter-image similarity for consistent representation of semantic regions. To drive this paradigm, we further construct a novel geometric matching head, the Z-matching head, to collaboratively learn the global and local similarity of semantic regions, guiding the efficient representation learning for different scale-level inter-image semantic features. Our experiments demonstrate that the pre-training with our learning of inter-image similarity yields more powerful inner-scene, inter-scene, and global-local transferring ability on four challenging 3D medical image tasks. Our codes and pre-trained models will be publicly available in https://github.com/YutingHe-list/GVSL. | 学习图像间相似度对于3D医学图像的自监督预训练至关重要，因为它们共享许多相同的语义区域。然而，在度量中缺乏语义先验和3D医学图像中的语义独立变化使得获得可靠的图像间相似度衡量变得具有挑战性，从而阻碍了相同语义的一致表示的学习。我们研究了这一任务的困难问题，即学习图像之间的一致表示以实现相同语义特征的聚类效果。我们提出了一种新颖的视觉相似度学习范式，即几何视觉相似度学习，将拓扑不变性的先验嵌入到图像间相似度的衡量中，以实现语义区域的一致表示。为推动这一范式，我们进一步构建了一个新颖的几何匹配头，Z匹配头，以协同学习语义区域的全局和局部相似度，指导不同尺度级别的图像间语义特征的有效表示学习。我们的实验证明，通过我们学习图像间相似度的预训练，在四个具有挑战性的3D医学图像任务上具有更强大的内部场景、场景间和全局-局部转移能力。我们的代码和预训练模型将在https://github.com/YutingHe-list/GVSL 上公开提供。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Geometric_Visual_Similarity_Learning_in_3D_Medical_Image_Self-Supervised_Pre-Training_CVPR_2023_paper.pdf) |
| 2023 | Learning To Exploit Temporal Structure for Biomedical Vision-Language Processing | Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando PÃ©rez-GarcÃ­a, Maximilian Ilse, Daniel C. Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, Anton Schwaighofer, Maria Wetscherek, Matthew P. Lungren, Aditya Nori, Javier Alvarez-Valle, Ozan Oktay | Self-supervised learning in vision--language processing (VLP) exploits semantic alignment between imaging and text modalities. Prior work in biomedical VLP has mostly relied on the alignment of single image and report pairs even though clinical notes commonly refer to prior images. This does not only introduce poor alignment between the modalities but also a missed opportunity to exploit rich self-supervision through existing temporal content in the data. In this work, we explicitly account for prior images and reports when available during both training and fine-tuning. Our approach, named BioViL-T, uses a CNN--Transformer hybrid multi-image encoder trained jointly with a text model. It is designed to be versatile to arising challenges such as pose variations and missing input images across time. The resulting model excels on downstream tasks both in single- and multi-image setups, achieving state-of-the-art (SOTA) performance on (I) progression classification, (II) phrase grounding, and (III) report generation, whilst offering consistent improvements on disease classification and sentence-similarity tasks. We release a novel multi-modal temporal benchmark dataset, CXR-T, to quantify the quality of vision--language representations in terms of temporal semantics. Our experimental results show the significant advantages of incorporating prior images and reports to make most use of the data. | 自监督学习在视觉-语言处理（VLP）中利用图像和文本模态之间的语义对齐。先前在生物医学VLP领域的工作主要依赖于单个图像和报告对之间的对齐，即使临床笔记通常会提到先前的图像。这不仅会导致模态之间的对齐不佳，还会错失利用数据中现有时间内容的丰富自监督机会。在这项工作中，我们在训练和微调过程中明确考虑了先前的图像和报告。我们的方法，命名为BioViL-T，使用了一个CNN-Transformer混合多图像编码器，与文本模型一起进行联合训练。它旨在应对不同挑战，如姿态变化和时间跨度内缺失输入图像。得到的模型在单一和多图像设置下都表现出色，在（I）进展分类，（II）短语定位和（III）报告生成方面实现了最先进的性能，同时在疾病分类和句子相似性任务上提供了一致的改进。我们发布了一个新颖的多模态时间基准数据集，CXR-T，以量化视觉-语言表示的质量，涉及时间语义。我们的实验结果显示了将先前的图像和报告纳入以充分利用数据的重要优势。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Bannur_Learning_To_Exploit_Temporal_Structure_for_Biomedical_Vision-Language_Processing_CVPR_2023_paper.pdf) |
| 2023 | Multi-Modal Learning With Missing Modality via Shared-Specific Feature Modelling | Hu Wang, Yuanhong Chen, Congbo Ma, Jodie Avery, Louise Hull, Gustavo Carneiro | The missing modality issue is critical but non-trivial to be solved by multi-modal models. Current methods aiming to handle the missing modality problem in multi-modal tasks, either deal with missing modalities only during evaluation or train separate models to handle specific missing modality settings. In addition, these models are designed for specific tasks, so for example, classification models are not easily adapted to segmentation tasks and vice versa. In this paper, we propose the Shared-Specific Feature Modelling (ShaSpec) method that is considerably simpler and more effective than competing approaches that address the issues above. ShaSpec is designed to take advantage of all available input modalities during training and evaluation by learning shared and specific features to better represent the input data. This is achieved from a strategy that relies on auxiliary tasks based on distribution alignment and domain classification, in addition to a residual feature fusion procedure. Also, the design simplicity of ShaSpec enables its easy adaptation to multiple tasks, such as classification and segmentation. Experiments are conducted on both medical image segmentation and computer vision classification, with results indicating that ShaSpec outperforms competing methods by a large margin. For instance, on BraTS2018, ShaSpec improves the SOTA by more than 3% for enhancing tumour, 5% for tumour core and 3% for whole tumour. | 缺失模态问题在多模态模型中是至关重要但非常重要的，目前的方法旨在处理多模态任务中的缺失模态问题，要么仅在评估过程中处理缺失模态，要么训练单独的模型来处理特定的缺失模态设置。此外，这些模型是针对特定任务设计的，例如，分类模型不容易适应分割任务，反之亦然。本文提出了“共享特定特征建模”(ShaSpec)方法，该方法比解决上述问题的竞争方法更简单、更有效。ShaSpec旨在利用训练和评估期间的所有可用输入模态，通过学习共享和特定特征来更好地表示输入数据。这是通过依赖于基于分布对齐和域分类的辅助任务以及残差特征融合过程的策略实现的。此外，ShaSpec的设计简单性使其易于适应多个任务，例如分类和分割。我们在医学图像分割和计算机视觉分类上进行了实验，结果表明ShaSpec在很大程度上优于竞争方法。例如，在BraTS2018上，ShaSpec对于增强肿瘤提高了超过3%，对于肿瘤核心提高了5%，对于整个肿瘤提高了3%。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multi-Modal_Learning_With_Missing_Modality_via_Shared-Specific_Feature_Modelling_CVPR_2023_paper.pdf) |
| 2023 | Ambiguous Medical Image Segmentation Using Diffusion Models | Aimon Rahman, Jeya Maria Jose Valanarasu, Ilker Hacihaliloglu, Vishal M. Patel | Collective insights from a group of experts have always proven to outperform an individual's best diagnostic for clinical tasks. For the task of medical image segmentation, existing research on AI-based alternatives focuses more on developing models that can imitate the best individual rather than harnessing the power of expert groups. In this paper, we introduce a single diffusion model-based approach that produces multiple plausible outputs by learning a distribution over group insights. Our proposed model generates a distribution of segmentation masks by leveraging the inherent stochastic sampling process of diffusion using only minimal additional learning. We demonstrate on three different medical image modalities- CT, ultrasound, and MRI that our model is capable of producing several possible variants while capturing the frequencies of their occurrences. Comprehensive results show that our proposed approach outperforms existing state-of-the-art ambiguous segmentation networks in terms of accuracy while preserving naturally occurring variation. We also propose a new metric to evaluate the diversity as well as the accuracy of segmentation predictions that aligns with the interest of clinical practice of collective insights. Implementation code will be released publicly after the review process. | 专家组的集体见解一直被证明在临床任务的诊断中胜过个人最佳诊断。对于医学图像分割任务，现有基于人工智能的替代方案的研究更多地侧重于开发能够模仿最佳个人的模型，而不是利用专家组的力量。在本文中，我们介绍了一种基于单一扩散模型的方法，通过学习关于专家组见解的分布，产生多个合理的输出。我们提出的模型利用扩散的固有随机抽样过程生成分割掩膜的分布，仅需进行最少的额外学习。我们在三种不同的医学图像模态 - CT、超声波和MRI上展示了我们的模型能够产生多个可能的变体，同时捕捉它们出现的频率。全面的结果表明，我们提出的方法在准确性方面优于现有的最先进的模糊分割网络，同时保留自然发生的变化。我们还提出了一种新的评估分割预测的多样性和准确性的指标，与临床实践的集体见解的兴趣相一致。在审查过程结束后，实现代码将公开发布。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Rahman_Ambiguous_Medical_Image_Segmentation_Using_Diffusion_Models_CVPR_2023_paper.pdf) |
| 2023 | Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation | Yunhao Bai, Duowen Chen, Qingli Li, Wei Shen, Yan Wang | In semi-supervised medical image segmentation, there exist empirical mismatch problems between labeled and unlabeled data distribution. The knowledge learned from the labeled data may be largely discarded if treating labeled and unlabeled data separately or training labeled and unlabeled data in an inconsistent manner. We propose a straightforward method for alleviating the problem -- copy-pasting labeled and unlabeled data bidirectionally, in a simple Mean Teacher architecture. The method encourages unlabeled data to learn comprehensive common semantics from the labeled data in both inward and outward directions. More importantly, the consistent learning procedure for labeled and unlabeled data can largely reduce the empirical distribution gap. In detail, we copy-paste a random crop from a labeled image (foreground) onto an unlabeled image (background) and an unlabeled image (foreground) onto a labeled image (background), respectively. The two mixed images are fed into a Student network. It is trained by the generated supervisory signal via bidirectional copy-pasting between the predictions of the unlabeled images from the Teacher and the label maps of the labeled images. We explore several design choices of how to copy-paste to make it more effective for minimizing empirical distribution gaps between labeled and unlabeled data. We reveal that the simple mechanism of copy-pasting bidirectionally between labeled and unlabeled data is good enough and the experiments show solid gains (e.g., over 21% Dice improvement on ACDC dataset with 5% labeled data) compared with other state-of-the-arts on various semi-supervised medical image segmentation datasets. | 在半监督医学图像分割中，标记和未标记数据分布之间存在经验不匹配问题。如果将标记和未标记数据分开处理或以不一致的方式训练标记和未标记数据，则从标记数据中学到的知识可能会被大量丢弃。我们提出了一种简单的方法来缓解这个问题--在简单的Mean Teacher架构中双向复制粘贴标记和未标记数据。该方法鼓励未标记数据从标记数据中双向学习全面的共同语义。更重要的是，标记和未标记数据的一致学习过程可以大大减少经验分布差距。具体而言，我们分别从标记图像（前景）中随机裁剪一部分粘贴到未标记图像（背景）上，然后从未标记图像（前景）中随机裁剪一部分粘贴到标记图像（背景）上。这两个混合图像被输入到一个学生网络中。通过在Teacher的未标记图像预测和标记图像的标签映射之间进行双向复制粘贴生成监督信号来训练它。我们探讨了几种复制粘贴的设计选择，使其更有效地减少标记和未标记数据之间的经验分布差距。我们发现，双向复制粘贴在标记和未标记数据之间是足够好的简单机制，实验证明与其他半监督医学图像分割数据集上的各种最新技术相比，取得了可观的进展（例如，在具有5%标记数据的ACDC数据集上，Dice指标提高了超过21%）。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_Bidirectional_Copy-Paste_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf) |
| 2023 | PEFAT: Boosting Semi-Supervised Medical Image Classification via Pseudo-Loss Estimation and Feature Adversarial Training | Qingjie Zeng, Yutong Xie, Zilin Lu, Yong Xia | Pseudo-labeling approaches have been proven beneficial for semi-supervised learning (SSL) schemes in computer vision and medical imaging. Most works are dedicated to finding samples with high-confidence pseudo-labels from the perspective of model predicted probability. Whereas this way may lead to the inclusion of incorrectly pseudo-labeled data if the threshold is not carefully adjusted. In addition, low-confidence probability samples are frequently disregarded and not employed to their full potential. In this paper, we propose a novel Pseudo-loss Estimation and Feature Adversarial Training semi-supervised framework, termed as PEFAT, to boost the performance of multi-class and multi-label medical image classification from the point of loss distribution modeling and adversarial training. Specifically, we develop a trustworthy data selection scheme to split a high-quality pseudo-labeled set, inspired by the dividable pseudo-loss assumption that clean data tend to show lower loss while noise data is the opposite. Instead of directly discarding these samples with low-quality pseudo-labels, we present a novel regularization approach to learn discriminate information from them via injecting adversarial noises at the feature-level to smooth the decision boundary. Experimental results on three medical and two natural image benchmarks validate that our PEFAT can achieve a promising performance and surpass other state-of-the-art methods. The code is available at https://github.com/maxwell0027/PEFAT. | 伪标签方法已被证明对计算机视觉和医学影像的半监督学习方案有益。大多数研究致力于从模型预测概率的角度找到具有高置信度伪标签的样本。然而，如果不仔细调整阈值，这种方法可能导致错误伪标记数据的包含。此外，低置信度概率样本经常被忽视，未能充分利用。在本文中，我们提出了一种新颖的伪损失估计和特征对抗训练半监督框架，称为PEFAT，旨在从损失分布建模和对抗训练的角度提升多类别和多标签医学图像分类的性能。具体来说，我们开发了一种可靠的数据选择方案，从高质量伪标记集中分离出来，灵感来自可分的伪损失假设，即干净数据往往显示较低损失，而噪声数据则相反。我们提出了一种新颖的正则化方法，而不是直接丢弃这些具有低质量伪标签的样本，通过在特征级别注入对抗性噪声来学习区分信息，以平滑决策边界。在三个医学和两个自然图像基准上的实验结果验证了我们的PEFAT可以取得令人满意的性能，并超越其他最先进的方法。代码可在https://github.com/maxwell0027/PEFAT 上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_PEFAT_Boosting_Semi-Supervised_Medical_Image_Classification_via_Pseudo-Loss_Estimation_and_CVPR_2023_paper.pdf) |
| 2023 | MCF: Mutual Correction Framework for Semi-Supervised Medical Image Segmentation | Yongchao Wang, Bin Xiao, Xiuli Bi, Weisheng Li, Xinbo Gao | Semi-supervised learning is a promising method for medical image segmentation under limited annotation. However, the model cognitive bias impairs the segmentation performance, especially for edge regions. Furthermore, current mainstream semi-supervised medical image segmentation (SSMIS) methods lack designs to handle model bias. The neural network has a strong learning ability, but the cognitive bias will gradually deepen during the training, and it is difficult to correct itself. We propose a novel mutual correction framework (MCF) to explore network bias correction and improve the performance of SSMIS. Inspired by the plain contrast idea, MCF introduces two different subnets to explore and utilize the discrepancies between subnets to correct cognitive bias of the model. More concretely, a contrastive difference review (CDR) module is proposed to find out inconsistent prediction regions and perform a review training. Additionally, a dynamic competitive pseudo-label generation (DCPLG) module is proposed to evaluate the performance of subnets in real-time, dynamically selecting more reliable pseudo-labels. Experimental results on two medical image databases with different modalities (CT and MRI) show that our method achieves superior performance compared to several state-of-the-art methods. The code will be available at https://github.com/WYC-321/MCF. | 半监督学习是一种在有限注释下进行医学图像分割的有前途的方法。然而，模型的认知偏见会损害分割性能，特别是在边缘区域。此外，当前主流的半监督医学图像分割（SSMIS）方法缺乏处理模型偏见的设计。神经网络具有强大的学习能力，但认知偏见在训练过程中会逐渐加深，难以自我纠正。我们提出了一种新颖的相互校正框架（MCF）来探索网络偏见校正并改善SSMIS的性能。受简单对比思想的启发，MCF引入了两个不同的子网络来探索和利用子网络之间的差异，以校正模型的认知偏见。更具体地说，提出了一个对比差异审查（CDR）模块，用于找出不一致的预测区域并进行审查训练。此外，提出了一个动态竞争伪标签生成（DCPLG）模块，实时评估子网络的性能，动态选择更可靠的伪标签。在两个具有不同模态（CT和MRI）的医学图像数据库上的实验结果表明，我们的方法相对于几种最先进的方法实现了更优越的性能。代码将可在https://github.com/WYC-321/MCF 上获得。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MCF_Mutual_Correction_Framework_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf) |
| 2023 | Decoupled Semantic Prototypes Enable Learning From Diverse Annotation Types for Semi-Weakly Segmentation in Expert-Driven Domains | Simon ReiÃ, Constantin Seibold, Alexander Freytag, Erik Rodner, Rainer Stiefelhagen | A vast amount of images and pixel-wise annotations allowed our community to build scalable segmentation solutions for natural domains. However, the transfer to expert-driven domains like microscopy applications or medical healthcare remains difficult as domain experts are a critical factor due to their limited availability for providing pixel-wise annotations. To enable affordable segmentation solutions for such domains, we need training strategies which can simultaneously handle diverse annotation types and are not bound to costly pixel-wise annotations. In this work, we analyze existing training algorithms towards their flexibility for different annotation types and scalability to small annotation regimes. We conduct an extensive evaluation in the challenging domain of organelle segmentation and find that existing semi- and semi-weakly supervised training algorithms are not able to fully exploit diverse annotation types. Driven by our findings, we introduce Decoupled Semantic Prototypes (DSP) as a training method for semantic segmentation which enables learning from annotation types as diverse as image-level-, point-, bounding box-, and pixel-wise annotations and which leads to remarkable accuracy gains over existing solutions for semi-weakly segmentation. | 大量图像和像素级注释使我们的社区能够为自然领域构建可扩展的分割解决方案。然而，将其转移到像显微镜应用或医疗保健等专家驱动的领域仍然困难，因为领域专家是关键因素，由于他们提供像素级注释的可用性有限。为了实现这些领域的可承受分割解决方案，我们需要能够同时处理不同注释类型并且不受昂贵像素级注释限制的训练策略。在这项工作中，我们分析了现有的训练算法，以确定它们对不同注释类型的灵活性和对小型注释规则的可扩展性。我们在器官分割这一具有挑战性的领域进行了广泛评估，并发现现有的半监督和半弱监督训练算法无法充分利用多样的注释类型。在这些发现的基础上，我们引入了解耦语义原型（DSP）作为一种语义分割的训练方法，它能够学习来自不同注释类型（如图像级、点、包围框和像素级注释），并且相较于现有的半弱监督分割解决方案实现了显著的准确率提高。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Reiss_Decoupled_Semantic_Prototypes_Enable_Learning_From_Diverse_Annotation_Types_for_CVPR_2023_paper.pdf) |
| 2023 | Private Image Generation With Dual-Purpose Auxiliary Classifier | Chen Chen, Daochang Liu, Siqi Ma, Surya Nepal, Chang Xu | Privacy-preserving image generation has been important for segments such as medical domains that have sensitive and limited data. The benefits of guaranteed privacy come at the costs of generated images' quality and utility due to the privacy budget constraints. The utility is currently measured by the gen2real accuracy (g2r%), i.e., the accuracy on real data of a downstream classifier trained using generated data. However, apart from this standard utility, we identify the "reversed utility" as another crucial aspect, which computes the accuracy on generated data of a classifier trained using real data, dubbed as real2gen accuracy (r2g%). Jointly considering these two views of utility, the standard and the reversed, could help the generation model better improve transferability between fake and real data. Therefore, we propose a novel private image generation method that incorporates a dual-purpose auxiliary classifier, which alternates between learning from real data and fake data, into the training of differentially private GANs. Additionally, our deliberate training strategies such as sequential training contributes to accelerating the generator's convergence and further boosting the performance upon exhausting the privacy budget. Our results achieve new state-of-the-arts over all metrics on three benchmarks: MNIST, Fashion-MNIST, and CelebA. | 隐私保护图像生成对于具有敏感和有限数据的领域（如医疗领域）至关重要。保证隐私的好处是生成图像的质量和效用会受到隐私预算约束的影响。目前，效用通常由gen2real准确率（g2r%）来衡量，即使用生成数据训练的下游分类器在真实数据上的准确性。然而，除了这种标准效用之外，我们还确定了另一个关键方面的“逆向效用”，即使用真实数据训练的分类器在生成数据上的准确性，被称为real2gen准确率（r2g%）。同时考虑这两种效用的标准和逆向视角，可以帮助生成模型更好地提高虚假数据和真实数据之间的可转移性。因此，我们提出了一种新颖的私密图像生成方法，将一个双重用途的辅助分类器，交替地从真实数据和虚假数据中学习，纳入到差分隐私GANs的训练中。此外，我们精心设计的训练策略，如顺序训练，有助于加速生成器的收敛，并在耗尽隐私预算后进一步提升性能。我们的结果在三个基准测试中（MNIST、Fashion-MNIST和CelebA）的所有指标上均达到了新的最高水平。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Private_Image_Generation_With_Dual-Purpose_Auxiliary_Classifier_CVPR_2023_paper.pdf) |
| 2023 | Why Is the Winner the Best? | Matthias Eisenmann, Annika Reinke, Vivienn Weru, Minu D. Tizabi, Fabian Isensee, Tim J. Adler, Sharib Ali, Vincent Andrearczyk, Marc Aubreville, Ujjwal Baid, Spyridon Bakas, Niranjan Balu, Sophia Bano, Jorge Bernal, Sebastian Bodenstedt, Alessandro Casella, Veronika Cheplygina, Marie Daum, Marleen de Bruijne, Adrien Depeursinge, Reuben Dorent, Jan Egger, David G. Ellis, Sandy Engelhardt, Melanie Ganz, Noha Ghatwary, Gabriel Girard, Patrick Godau, Anubha Gupta, Lasse Hansen, Kanako Harada, Mattias P. Heinrich, Nicholas Heller, Alessa Hering, Arnaud HuaulmÃ©, Pierre Jannin, Ali Emre Kavur, OldÅich Kodym, Michal Kozubek, Jianning Li, Hongwei Li, Jun Ma, Carlos MartÃ­n-Isla, Bjoern Menze, Alison Noble, Valentin Oreiller, Nicolas Padoy, Sarthak Pati, Kelly Payette, Tim RÃ¤dsch, Jonathan Rafael-PatiÃ±o, Vivek Singh Bawa, Stefanie Speidel, Carole H. Sudre, Kimberlin van Wijnen, Martin Wagner, Donglai Wei, Amine Yamlahi, Moi Hoon Yap, Chun Yuan, Maximilian Zenk, Aneeq Zia, David Zimmerer, Dogu Baran Aydogan, Binod Bhattarai, Louise Bloch, Raphael BrÃ¼ngel, Jihoon Cho, Chanyeol Choi, Qi Dou, Ivan Ezhov, Christoph M. Friedrich, Clifton D. Fuller, Rebati Raman Gaire, Adrian Galdran, Ãlvaro GarcÃ­a Faura, Maria Grammatikopoulou, SeulGi Hong, Mostafa Jahanifar, Ikbeom Jang, Abdolrahim Kadkhodamohammadi, Inha Kang, Florian Kofler, Satoshi Kondo, Hugo Kuijf, Mingxing Li, Minh Luu, TomaÅ¾ MartinÄiÄ, Pedro Morais, Mohamed A. Naser, Bruno Oliveira, David Owen, Subeen Pang, Jinah Park, Sung-Hong Park, Szymon Plotka, Elodie Puybareau, Nasir Rajpoot, Kanghyun Ryu, Numan Saeed, Adam Shephard, Pengcheng Shi, Dejan Å tepec, Ronast Subedi, Guillaume Tochon, Helena R. Torres, Helene Urien, JoÃ£o L. VilaÃ§a, Kareem A. Wahid, Haojie Wang, Jiacheng Wang, Liansheng Wang, Xiyue Wang, Benedikt Wiestler, Marek Wodzinski, Fangfang Xia, Juanying Xie, Zhiwei Xiong, Sen Yang, Yanwu Yang, Zixuan Zhao, Klaus Maier-Hein, Paul F. JÃ¤ger, Annette Kopp-Schneider, Lena Maier-Hein | International benchmarking competitions have become fundamental for the comparative performance assessment of image analysis methods. However, little attention has been given to investigating what can be learnt from these competitions. Do they really generate scientific progress? What are common and successful participation strategies? What makes a solution superior to a competing method? To address this gap in the literature, we performed a multi-center study with all 80 competitions that were conducted in the scope of IEEE ISBI 2021 and MICCAI 2021. Statistical analyses performed based on comprehensive descriptions of the submitted algorithms linked to their rank as well as the underlying participation strategies revealed common characteristics of winning solutions. These typically include the use of multi-task learning (63%) and/or multi-stage pipelines (61%), and a focus on augmentation (100%), image preprocessing (97%), data curation (79%), and postprocessing (66%). The "typical" lead of a winning team is a computer scientist with a doctoral degree, five years of experience in biomedical image analysis, and four years of experience in deep learning. Two core general development strategies stood out for highly-ranked teams: the reflection of the metrics in the method design and the focus on analyzing and handling failure cases. According to the organizers, 43% of the winning algorithms exceeded the state of the art but only 11% completely solved the respective domain problem. The insights of our study could help researchers (1) improve algorithm development strategies when approaching new problems, and (2) focus on open research questions revealed by this work. | 国际基准竞赛已成为比较性能评估图像分析方法的基础。然而，对于这些竞赛能够学到什么却付出了很少的关注。它们真的能够产生科学进步吗？哪些是常见和成功的参与策略？什么使一种解决方案优于竞争方法？为了填补文献中的这一空白，我们进行了一项多中心研究，涵盖了在IEEE ISBI 2021和MICCAI 2021范围内举行的所有80个竞赛。基于提交的算法的全面描述以及其排名和底层参与策略，进行的统计分析揭示了获胜解决方案的常见特征。这些通常包括使用多任务学习（63%）和/或多阶段管线（61%），以及对增强（100%）、图像预处理（97%）、数据筛选（79%）和后处理（66%）的关注。获胜团队的“典型”领先者是一名拥有博士学位的计算机科学家，在生物医学图像分析方面有五年经验，并且在深度学习方面有四年经验。对于排名较高的团队，两种核心开发策略脱颖而出：在方法设计中反映指标，以及专注于分析和处理失败案例。据组织者称，43%的获胜算法超过了现有技术水平，但只有11%完全解决了相应的领域问题。我们研究的见解可以帮助研究人员（1）在解决新问题时改进算法开发策略，以及（2）专注于本次工作揭示的开放性研究问题。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Eisenmann_Why_Is_the_Winner_the_Best_CVPR_2023_paper.pdf) |
| 2023 | Hierarchical Discriminative Learning Improves Visual Representations of Biomedical Microscopy | Cheng Jiang, Xinhai Hou, Akhil Kondepudi, Asadur Chowdury, Christian W. Freudiger, Daniel A. Orringer, Honglak Lee, Todd C. Hollon | Learning high-quality, self-supervised, visual representations is essential to advance the role of computer vision in biomedical microscopy and clinical medicine. Previous work has focused on self-supervised representation learning (SSL) methods developed for instance discrimination and applied them directly to image patches, or fields-of-view, sampled from gigapixel whole-slide images (WSIs) used for cancer diagnosis. However, this strategy is limited because it (1) assumes patches from the same patient are independent, (2) neglects the patient-slide-patch hierarchy of clinical biomedical microscopy, and (3) requires strong data augmentations that can degrade downstream performance. Importantly, sampled patches from WSIs of a patient's tumor are a diverse set of image examples that capture the same underlying cancer diagnosis. This motivated HiDisc, a data-driven method that leverages the inherent patient-slide-patch hierarchy of clinical biomedical microscopy to define a hierarchical discriminative learning task that implicitly learns features of the underlying diagnosis. HiDisc uses a self-supervised contrastive learning framework in which positive patch pairs are defined based on a common ancestry in the data hierarchy, and a unified patch, slide, and patient discriminative learning objective is used for visual SSL. We benchmark HiDisc visual representations on two vision tasks using two biomedical microscopy datasets, and demonstrate that (1) HiDisc pretraining outperforms current state-of-the-art self-supervised pretraining methods for cancer diagnosis and genetic mutation prediction, and (2) HiDisc learns high-quality visual representations using natural patch diversity without strong data augmentations. | 学习高质量、自监督的视觉表示对于推动计算机视觉在生物医学显微镜和临床医学中的作用至关重要。先前的工作集中在为实例识别开发的自监督表示学习（SSL）方法上，并直接将其应用于从用于癌症诊断的千兆像素全切片图像（WSI）中采样的图像补丁或视野。然而，这种策略存在局限性，因为它（1）假设来自同一患者的补丁是独立的，（2）忽视了临床生物医学显微镜的患者-切片-补丁层次结构，（3）需要强大的数据增强，可能会降低下游性能。重要的是，来自患者肿瘤WSI的采样补丁是捕获相同潜在癌症诊断的多样化图像示例集。这促使HiDisc的发展，这是一种利用临床生物医学显微镜的固有患者-切片-补丁层次结构来定义一个层次判别学习任务的数据驱动方法，该方法隐式地学习潜在诊断的特征。HiDisc使用自监督对比学习框架，其中基于数据层次结构中的共同祖先定义正补丁对，并使用统一的补丁、切片和患者判别学习目标进行视觉SSL。我们对HiDisc视觉表示进行了两项视觉任务的基准测试，使用了两个生物医学显微镜数据集，并证明了（1）HiDisc预训练优于当前癌症诊断和遗传突变预测的最新自监督预训练方法，（2）HiDisc使用自然补丁多样性学习高质量的视觉表示，而无需强大的数据增强。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Hierarchical_Discriminative_Learning_Improves_Visual_Representations_of_Biomedical_Microscopy_CVPR_2023_paper.pdf) |
| 2023 | Solving 3D Inverse Problems Using Pre-Trained 2D Diffusion Models | Hyungjin Chung, Dohoon Ryu, Michael T. McCann, Marc L. Klasky, Jong Chul Ye | Diffusion models have emerged as the new state-of-the-art generative model with high quality samples, with intriguing properties such as mode coverage and high flexibility. They have also been shown to be effective inverse problem solvers, acting as the prior of the distribution, while the information of the forward model can be granted at the sampling stage. Nonetheless, as the generative process remains in the same high dimensional (i.e. identical to data dimension) space, the models have not been extended to 3D inverse problems due to the extremely high memory and computational cost. In this paper, we combine the ideas from the conventional model-based iterative reconstruction with the modern diffusion models, which leads to a highly effective method for solving 3D medical image reconstruction tasks such as sparse-view tomography, limited angle tomography, compressed sensing MRI from pre-trained 2D diffusion models. In essence, we propose to augment the 2D diffusion prior with a model-based prior in the remaining direction at test time, such that one can achieve coherent reconstructions across all dimensions. Our method can be run in a single commodity GPU, and establishes the new state-of-the-art, showing that the proposed method can perform reconstructions of high fidelity and accuracy even in the most extreme cases (e.g. 2-view 3D tomography). We further reveal that the generalization capacity of the proposed method is surprisingly high, and can be used to reconstruct volumes that are entirely different from the training dataset. Code available: https://github.com/HJ-harry/DiffusionMBIR | 扩散模型已成为新一代具有高质量样本的生成模型，具有诸如模态覆盖和高灵活性等有趣的特性。它们还被证明是有效的逆问题求解器，作为分布的先验，而前向模型的信息可以在采样阶段获得。然而，由于生成过程仍然保持在相同的高维空间（即与数据维度相同），这些模型尚未扩展到三维逆问题，因为极高的内存和计算成本。在本文中，我们将传统基于模型的迭代重建方法与现代扩散模型相结合，从而提出了一种高效解决三维医学图像重建任务的方法，例如稀疏视图断层摄影术、有限角度断层摄影术、压缩感知MRI，从预训练的2D扩散模型中进行重建。本质上，我们提出在测试时将2D扩散先验与基于模型的先验在剩余方向上进行增强，从而可以实现所有维度上的连贯重建。我们的方法可以在单个通用GPU上运行，并建立了新的最先进技术，表明所提出的方法甚至可以在最极端情况下（例如2视图3D断层摄影术）进行高保真度和准确度的重建。我们进一步揭示了所提出方法的泛化能力出奇地高，可以用于重建与训练数据集完全不同的体积。代码可在https://github.com/HJ-harry/DiffusionMBIR 获取。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Chung_Solving_3D_Inverse_Problems_Using_Pre-Trained_2D_Diffusion_Models_CVPR_2023_paper.pdf) |
| 2023 | GradICON: Approximate Diffeomorphisms via Gradient Inverse Consistency | Lin Tian, Hastings Greer, FranÃ§ois-Xavier Vialard, Roland Kwitt, RaÃºl San JosÃ© EstÃ©par, Richard Jarrett Rushmore, Nikolaos Makris, Sylvain Bouix, Marc Niethammer | We present an approach to learning regular spatial transformations between image pairs in the context of medical image registration. Contrary to optimization-based registration techniques and many modern learning-based methods, we do not directly penalize transformation irregularities but instead promote transformation regularity via an inverse consistency penalty. We use a neural network to predict a map between a source and a target image as well as the map when swapping the source and target images. Different from existing approaches, we compose these two resulting maps and regularize deviations of the Jacobian of this composition from the identity matrix. This regularizer -- GradICON -- results in much better convergence when training registration models compared to promoting inverse consistency of the composition of maps directly while retaining the desirable implicit regularization effects of the latter. We achieve state-of-the-art registration performance on a variety of real-world medical image datasets using a single set of hyperparameters and a single non-dataset-specific training protocol. The code is available at https://github.com/uncbiag/ICON. | 我们提出了一种学习医学图像注册中图像对之间常规空间变换的方法。与基于优化的注册技术和许多现代基于学习的方法相反，我们不直接惩罚变换的不规则性，而是通过逆一致性惩罚来促进变换的规则性。我们使用神经网络来预测源图像和目标图像之间的映射，以及交换源和目标图像时的映射。与现有方法不同，我们组合这两个结果映射，并规范这个组合的Jacobian矩阵与单位矩阵之间的偏差。这种正则化器--GradICON--在训练注册模型时比直接促进映射组合的逆一致性时实现了更好的收敛性，同时保留了后者的良好隐式正则化效果。我们使用一组超参数和一种非数据集特定的训练协议，在各种真实世界医学图像数据集上实现了最先进的注册性能。该代码可在https://github.com/uncbiag/ICON获得。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_GradICON_Approximate_Diffeomorphisms_via_Gradient_Inverse_Consistency_CVPR_2023_paper.pdf) |
| 2023 | KiUT: Knowledge-Injected U-Transformer for Radiology Report Generation | Zhongzhen Huang, Xiaofan Zhang, Shaoting Zhang | Radiology report generation aims to automatically generate a clinically accurate and coherent paragraph from the X-ray image, which could relieve radiologists from the heavy burden of report writing. Although various image caption methods have shown remarkable performance in the natural image field, generating accurate reports for medical images requires knowledge of multiple modalities, including vision, language, and medical terminology. We propose a Knowledge-injected U-Transformer (KiUT) to learn multi-level visual representation and adaptively distill the information with contextual and clinical knowledge for word prediction. In detail, a U-connection schema between the encoder and decoder is designed to model interactions between different modalities. And a symptom graph and an injected knowledge distiller are developed to assist the report generation. Experimentally, we outperform state-of-the-art methods on two widely used benchmark datasets: IU-Xray and MIMIC-CXR. Further experimental results prove the advantages of our architecture and the complementary benefits of the injected knowledge. | 放射学报告生成旨在从X射线图像中自动生成临床准确且连贯的段落，从而可以减轻放射科医师撰写报告的沉重负担。尽管各种图像标题方法在自然图像领域表现出色，但为医学图像生成准确的报告需要涉及多种模态的知识，包括视觉、语言和医学术语。我们提出了一种注入知识的U-Transformer（KiUT），以学习多层次的视觉表示，并通过上下文和临床知识自适应地提炼信息以进行单词预测。具体而言，我们设计了编码器和解码器之间的U连接图谱，以建模不同模态之间的交互作用。同时，我们开发了一种症状图谱和注入知识提炼器来辅助报告生成。在实验中，我们在两个广泛使用的基准数据集IU-Xray和MIMIC-CXR上超越了现有技术。进一步的实验结果证明了我们的架构的优势以及注入知识的互补好处。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_KiUT_Knowledge-Injected_U-Transformer_for_Radiology_Report_Generation_CVPR_2023_paper.pdf) |
| 2023 | MagicNet: Semi-Supervised Multi-Organ Segmentation via Magic-Cube Partition and Recovery | Duowen Chen, Yunhao Bai, Wei Shen, Qingli Li, Lequan Yu, Yan Wang | We propose a novel teacher-student model for semi-supervised multi-organ segmentation. In the teacher-student model, data augmentation is usually adopted on unlabeled data to regularize the consistent training between teacher and student. We start from a key perspective that fixed relative locations and variable sizes of different organs can provide distribution information where a multi-organ CT scan is drawn. Thus, we treat the prior anatomy as a strong tool to guide the data augmentation and reduce the mismatch between labeled and unlabeled images for semi-supervised learning. More specifically, we propose a data augmentation strategy based on partition-and-recovery N^3 cubes cross- and within- labeled and unlabeled images. Our strategy encourages unlabeled images to learn organ semantics in relative locations from the labeled images (cross-branch) and enhances the learning ability for small organs (within-branch). For within-branch, we further propose to refine the quality of pseudo labels by blending the learned representations from small cubes to incorporate local attributes. Our method is termed as MagicNet, since it treats the CT volume as a magic-cube and N^3-cube partition-and-recovery process matches with the rule of playing a magic-cube. Extensive experiments on two public CT multi-organ datasets demonstrate the effectiveness of MagicNet, and noticeably outperforms state-of-the-art semi-supervised medical image segmentation approaches, with +7% DSC improvement on MACT dataset with 10% labeled images. | 我们提出了一种新颖的师生模型用于半监督多器官分割。在师生模型中，通常采用数据增强来规范未标记数据之间师生之间的一致训练。我们从一个关键的角度出发，即不同器官的固定相对位置和可变大小可以提供一个多器官CT扫描中绘制的分布信息。因此，我们将先前的解剖结构视为引导数据增强的强大工具，以减少半监督学习中标记和未标记图像之间的不匹配。更具体地说，我们提出了一种基于分区和恢复N^3立方体跨标记和未标记图像的数据增强策略。我们的策略鼓励未标记图像从标记图像中学习器官语义在相对位置上（跨分支），并增强小器官的学习能力（内部分支）。对于内部分支，我们进一步提出通过混合从小立方体中学到的表示来改进伪标签的质量，以融合局部属性。我们的方法被称为MagicNet，因为它将CT体积视为一个魔方，并且N^3立方体的分区和恢复过程符合玩魔方的规则。对两个公共CT多器官数据集的广泛实验表明了MagicNet的有效性，并且在MACT数据集上比现有半监督医学图像分割方法表现出色，标记图像占10%，DSC提高了7%。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_MagicNet_Semi-Supervised_Multi-Organ_Segmentation_via_Magic-Cube_Partition_and_Recovery_CVPR_2023_paper.pdf) |
| 2023 | Devil Is in the Queries: Advancing Mask Transformers for Real-World Medical Image Segmentation and Out-of-Distribution Localization | Mingze Yuan, Yingda Xia, Hexin Dong, Zifan Chen, Jiawen Yao, Mingyan Qiu, Ke Yan, Xiaoli Yin, Yu Shi, Xin Chen, Zaiyi Liu, Bin Dong, Jingren Zhou, Le Lu, Ling Zhang, Li Zhang | Real-world medical image segmentation has tremendous long-tailed complexity of objects, among which tail conditions correlate with relatively rare diseases and are clinically significant. A trustworthy medical AI algorithm should demonstrate its effectiveness on tail conditions to avoid clinically dangerous damage in these out-of-distribution (OOD) cases. In this paper, we adopt the concept of object queries in Mask transformers to formulate semantic segmentation as a soft cluster assignment. The queries fit the feature-level cluster centers of inliers during training. Therefore, when performing inference on a medical image in real-world scenarios, the similarity between pixels and the queries detects and localizes OOD regions. We term this OOD localization as MaxQuery. Furthermore, the foregrounds of real-world medical images, whether OOD objects or inliers, are lesions. The difference between them is obviously less than that between the foreground and background, resulting in the object queries may focus redundantly on the background. Thus, we propose a query-distribution (QD) loss to enforce clear boundaries between segmentation targets and other regions at the query level, improving the inlier segmentation and OOD indication. Our proposed framework is tested on two real-world segmentation tasks, i.e., segmentation of pancreatic and liver tumors, outperforming previous leading algorithms by an average of 7.39% on AUROC, 14.69% on AUPR, and 13.79% on FPR95 for OOD localization. On the other hand, our framework improves the performance of inlier segmentation by an average of 5.27% DSC compared with nnUNet. | 现实世界的医学图像分割具有极其复杂的长尾对象，其中尾部情况与相对罕见的疾病相关，并且在临床上具有重要意义。一个值得信赖的医学人工智能算法应该在尾部情况上展示其有效性，以避免在这些分布之外的情况下造成临床上危险的损害。在本文中，我们采用Mask transformers中的对象查询的概念，将语义分割定义为软聚类分配。查询在训练过程中适配内群体的特征级聚类中心。因此，在现实世界情景中对医学图像进行推断时，像素与查询之间的相似性可以检测和定位分布之外区域。我们将这种分布之外定位称为MaxQuery。此外，现实世界医学图像的前景，无论是分布之外的对象还是内群体，都是病变。它们之间的差异显然比前景和背景之间的差异小，导致对象查询可能会过多地关注背景。因此，我们提出了一个查询分布（QD）损失，以在查询级别上强化分割目标与其他区域之间的清晰边界，提高内群体分割和分布之外指示。我们提出的框架在两个真实世界的分割任务上进行了测试，即胰腺和肝脏肿瘤的分割，在OOD定位方面的性能优于以前领先算法，平均提高了7.39%的AUROC，14.69%的AUPR和13.79%的FPR95。另一方面，与nnUNet相比，我们的框架将内群体分割的性能平均提高了5.27%的DSC。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Yuan_Devil_Is_in_the_Queries_Advancing_Mask_Transformers_for_Real-World_CVPR_2023_paper.pdf) |
| 2023 | OCELOT: Overlapped Cell on Tissue Dataset for Histopathology | Jeongun Ryu, Aaron Valero Puche, JaeWoong Shin, Seonwook Park, Biagio Brattoli, Jinhee Lee, Wonkyung Jung, Soo Ick Cho, Kyunghyun Paeng, Chan-Young Ock, Donggeun Yoo, SÃ©rgio Pereira | Cell detection is a fundamental task in computational pathology that can be used for extracting high-level medical information from whole-slide images. For accurate cell detection, pathologists often zoom out to understand the tissue-level structures and zoom in to classify cells based on their morphology and the surrounding context. However, there is a lack of efforts to reflect such behaviors by pathologists in the cell detection models, mainly due to the lack of datasets containing both cell and tissue annotations with overlapping regions. To overcome this limitation, we propose and publicly release OCELOT, a dataset purposely dedicated to the study of cell-tissue relationships for cell detection in histopathology. OCELOT provides overlapping cell and tissue annotations on images acquired from multiple organs. Within this setting, we also propose multi-task learning approaches that benefit from learning both cell and tissue tasks simultaneously. When compared against a model trained only for the cell detection task, our proposed approaches improve cell detection performance on 3 datasets: proposed OCELOT, public TIGER, and internal CARP datasets. On the OCELOT test set in particular, we show up to 6.79 improvement in F1-score. We believe the contributions of this paper, including the release of the OCELOT dataset at https://lunit-io.github.io/research/publications/ocelot are a crucial starting point toward the important research direction of incorporating cell-tissue relationships in computation pathology. | 细胞检测是计算病理学中的一项基本任务，可用于从全幻灯片图像中提取高级医学信息。为了准确检测细胞，病理学家经常放大以了解组织水平结构，并放大以基于形态学和周围环境对细胞进行分类。然而，由于缺乏包含细胞和组织标注重叠区域的数据集，目前缺乏努力来反映病理学家的这种行为在细胞检测模型中。为了克服这一局限性，我们提出并公开发布了OCELOT，这是一个专门用于研究组织细胞关系的数据集，用于组织学中的细胞检测。OCELOT在从多个器官获取的图像上提供重叠的细胞和组织标注。在这种设置下，我们还提出了多任务学习方法，可以同时学习细胞和组织任务。与仅针对细胞检测任务训练的模型相比，我们提出的方法在3个数据集上改善了细胞检测性能：提出的OCELOT、公共TIGER和内部CARP数据集。特别是在OCELOT测试集上，我们展示了F1得分高达6.79的改进。我们相信本文的贡献，包括在https://lunit-io.github.io/research/publications/ocelot发布OCELOT数据集，是将细胞组织关系纳入计算病理学重要研究方向的关键起点。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Ryu_OCELOT_Overlapped_Cell_on_Tissue_Dataset_for_Histopathology_CVPR_2023_paper.pdf) |
| 2023 | Image Quality-Aware Diagnosis via Meta-Knowledge Co-Embedding | Haoxuan Che, Siyu Chen, Hao Chen | Medical images usually suffer from image degradation in clinical practice, leading to decreased performance of deep learning-based models. To resolve this problem, most previous works have focused on filtering out degradation-causing low-quality images while ignoring their potential value for models. Through effectively learning and leveraging the knowledge of degradations, models can better resist their adverse effects and avoid misdiagnosis. In this paper, we raise the problem of image quality-aware diagnosis, which aims to take advantage of low-quality images and image quality labels to achieve a more accurate and robust diagnosis. However, the diversity of degradations and superficially unrelated targets between image quality assessment and disease diagnosis makes it still quite challenging to effectively leverage quality labels to assist diagnosis. Thus, to tackle these issues, we propose a novel meta-knowledge co-embedding network, consisting of two subnets: Task Net and Meta Learner. Task Net constructs an explicit quality information utilization mechanism to enhance diagnosis via knowledge co-embedding features, while Meta Learner ensures the effectiveness and constrains the semantics of these features via meta-learning and joint-encoding masking. Superior performance on five datasets with four widely-used medical imaging modalities demonstrates the effectiveness and generalizability of our method. | 在临床实践中，医学图像通常会受到图像退化的影响，导致基于深度学习模型的性能下降。为了解决这个问题，大多数先前的研究集中在过滤掉导致退化的低质量图像，而忽略了它们对模型的潜在价值。通过有效地学习和利用退化的知识，模型可以更好地抵抗其不良影响，避免误诊。在本文中，我们提出了图像质量感知诊断的问题，旨在利用低质量图像和图像质量标签来实现更准确和更稳健的诊断。然而，退化的多样性以及图像质量评估和疾病诊断之间表面上无关的目标，使得有效利用质量标签来辅助诊断仍然非常具有挑战性。因此，为了解决这些问题，我们提出了一种新颖的元知识共嵌入网络，由两个子网络组成：任务网络和元学习器。任务网络构建了一个显式的质量信息利用机制，通过知识共嵌入特征来增强诊断，而元学习器通过元学习和联合编码掩模确保了这些特征的有效性并限制了语义。在五个数据集上使用四种广泛使用的医学成像模式表现出卓越的性能，证明了我们方法的有效性和泛化能力。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Che_Image_Quality-Aware_Diagnosis_via_Meta-Knowledge_Co-Embedding_CVPR_2023_paper.pdf) |
| 2023 | Guided Depth Super-Resolution by Deep Anisotropic Diffusion | Nando Metzger, Rodrigo Caye Daudt, Konrad Schindler | Performing super-resolution of a depth image using the guidance from an RGB image is a problem that concerns several fields, such as robotics, medical imaging, and remote sensing. While deep learning methods have achieved good results in this problem, recent work highlighted the value of combining modern methods with more formal frameworks. In this work we propose a novel approach which combines guided anisotropic diffusion with a deep convolutional network and advances the state of the art for guided depth super-resolution. The edge transferring/enhancing properties of the diffusion are boosted by the contextual reasoning capabilities of modern networks, and a strict adjustment step guarantees perfect adherence to the source image. We achieve unprecedented results in three commonly used benchmarks for guided depth super resolution. The performance gain compared to other methods is the largest at larger scales, such as x32 scaling. Code for the proposed method will be made available to promote reproducibility of our results. | 利用RGB图像的指导对深度图像进行超分辨率处理是涉及到多个领域的问题，如机器人技术、医学成像和遥感。虽然深度学习方法在这个问题上取得了良好的结果，但最近的研究强调了将现代方法与更正式的框架相结合的价值。在本文中，我们提出了一种新颖的方法，将引导各向异性扩散与深度卷积网络相结合，推动了引导深度超分辨率的最新技术。扩散的边缘传递/增强特性通过现代网络的上下文推理能力得到增强，严格的调整步骤确保完美地遵循源图像。我们在三个常用的引导深度超分辨率基准测试中取得了空前的成果。与其他方法相比，性能提升在较大比例（如x32倍）时最为显著。为了促进我们结果的再现性，我们将提供所提出方法的代码。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Metzger_Guided_Depth_Super-Resolution_by_Deep_Anisotropic_Diffusion_CVPR_2023_paper.pdf) |
| 2023 | Best of Both Worlds: Multimodal Contrastive Learning With Tabular and Imaging Data | Paul Hager, Martin J. Menten, Daniel Rueckert | Medical datasets and especially biobanks, often contain extensive tabular data with rich clinical information in addition to images. In practice, clinicians typically have less data, both in terms of diversity and scale, but still wish to deploy deep learning solutions. Combined with increasing medical dataset sizes and expensive annotation costs, the necessity for unsupervised methods that can pretrain multimodally and predict unimodally has risen. To address these needs, we propose the first self-supervised contrastive learning framework that takes advantage of images and tabular data to train unimodal encoders. Our solution combines SimCLR and SCARF, two leading contrastive learning strategies, and is simple and effective. In our experiments, we demonstrate the strength of our framework by predicting risks of myocardial infarction and coronary artery disease (CAD) using cardiac MR images and 120 clinical features from 40,000 UK Biobank subjects. Furthermore, we show the generalizability of our approach to natural images using the DVM car advertisement dataset. We take advantage of the high interpretability of tabular data and through attribution and ablation experiments find that morphometric tabular features, describing size and shape, have outsized importance during the contrastive learning process and improve the quality of the learned embeddings. Finally, we introduce a novel form of supervised contrastive learning, label as a feature (LaaF), by appending the ground truth label as a tabular feature during multimodal pretraining, outperforming all supervised contrastive baselines. | 医学数据集，尤其是生物库，通常包含丰富的临床信息以及图像等大量表格数据。在实践中，临床医生通常拥有更少的数据，无论是多样性还是规模，但仍希望部署深度学习解决方案。随着医学数据集规模的增加和昂贵的注释成本，对于能够预训练多模态并预测单模态的无监督方法的需求日益增加。为了满足这些需求，我们提出了第一个利用图像和表格数据训练单模态编码器的自监督对比学习框架。我们的解决方案结合了SimCLR和SCARF两种领先的对比学习策略，简单且有效。在实验中，我们通过使用40,000名英国生物库受试者的心脏MR图像和120个临床特征来预测心肌梗死和冠脉动脉疾病（CAD）的风险，展示了我们框架的强大性能。此外，我们还展示了我们方法在自然图像领域的泛化能力，使用DVM汽车广告数据集。我们利用表格数据的高可解释性，并通过归因和消融实验发现，在对比学习过程中，描述大小和形状的形态表格特征具有重要作用，并提高了学习嵌入的质量。最后，我们引入了一种新颖的监督对比学习形式，称为特征标签（LaaF），通过在多模态预训练过程中添加地面真实标签作为表格特征，超越了所有监督对比基线。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Hager_Best_of_Both_Worlds_Multimodal_Contrastive_Learning_With_Tabular_and_CVPR_2023_paper.pdf) |
| 2023 | OReX: Object Reconstruction From Planar Cross-Sections Using Neural Fields | Haim Sawdayee, Amir Vaxman, Amit H. Bermano | Reconstructing 3D shapes from planar cross-sections is a challenge inspired by downstream applications like medical imaging and geographic informatics. The input is an in/out indicator function fully defined on a sparse collection of planes in space, and the output is an interpolation of the indicator function to the entire volume. Previous works addressing this sparse and ill-posed problem either produce low quality results, or rely on additional priors such as target topology, appearance information, or input normal directions. In this paper, we present OReX, a method for 3D shape reconstruction from slices alone, featuring a Neural Field as the interpolation prior. A modest neural network is trained on the input planes to return an inside/outside estimate for a given 3D coordinate, yielding a powerful prior that induces smoothness and self-similarities. The main challenge for this approach is high-frequency details, as the neural prior is overly smoothing. To alleviate this, we offer an iterative estimation architecture and a hierarchical input sampling scheme that encourage coarse-to-fine training, allowing the training process to focus on high frequencies at later stages. In addition, we identify and analyze a ripple-like effect stemming from the mesh extraction step. We mitigate it by regularizing the spatial gradients of the indicator function around input in/out boundaries during network training, tackling the problem at the root. Through extensive qualitative and quantitative experimentation, we demonstrate our method is robust, accurate, and scales well with the size of the input. We report state-of-the-art results compared to previous approaches and recent potential solutions, and demonstrate the benefit of our individual contributions through analysis and ablation studies. | 从平面剖面中重建3D形状是一个挑战，灵感来自于医学成像和地理信息学等下游应用。输入是在空间中稀疏收集的平面上完全定义的内/外指示器函数，输出是将指示器函数插值到整个体积。先前处理这个稀疏和不适定问题的工作要么产生质量低下的结果，要么依赖于额外的先验，如目标拓扑、外观信息或输入法线方向。在本文中，我们提出了一种名为OReX的方法，只从切片中进行3D形状重建，其中包括一个神经场作为插值先验。一个适度的神经网络在输入平面上进行训练，为给定的3D坐标返回一个内/外估计，从而产生一个强大的先验，引入平滑性和自相似性。这种方法的主要挑战是高频细节，因为神经先验过于平滑。为了缓解这一问题，我们提供了一个迭代估计架构和一个分层输入采样方案，鼓励从粗到细的训练，使训练过程能够在后期集中于高频率。此外，我们识别并分析了源自网格提取步骤的波纹效应。在网络训练期间，我们通过正规化输入内/外边界周围的指示器函数的空间梯度来缓解这一问题，从根本上解决了问题。通过广泛的定性和定量实验，我们证明我们的方法具有鲁棒性、准确性，并且能够很好地随着输入大小的增加而扩展。与先前方法和最近的潜在解决方案相比，我们报告了最新的结果，并通过分析和消融研究展示了我们个别贡献的好处。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Sawdayee_OReX_Object_Reconstruction_From_Planar_Cross-Sections_Using_Neural_Fields_CVPR_2023_paper.pdf) |
| 2023 | DeGPR: Deep Guided Posterior Regularization for Multi-Class Cell Detection and Counting | Aayush Kumar Tyagi, Chirag Mohapatra, Prasenjit Das, Govind Makharia, Lalita Mehra, Prathosh AP, Mausam | Multi-class cell detection and counting is an essential task for many pathological diagnoses. Manual counting is tedious and often leads to inter-observer variations among pathologists. While there exist multiple, general-purpose, deep learning-based object detection and counting methods, they may not readily transfer to detecting and counting cells in medical images, due to the limited data, presence of tiny overlapping objects, multiple cell types, severe class-imbalance, minute differences in size/shape of cells, etc. In response, we propose guided posterior regularization DeGPR, which assists an object detector by guiding it to exploit discriminative features among cells. The features may be pathologist-provided or inferred directly from visual data. We validate our model on two publicly available datasets (CoNSeP and MoNuSAC), and on MuCeD, a novel dataset that we contribute. MuCeD consists of 55 biopsy images of the human duodenum for predicting celiac disease. We perform extensive experimentation with three object detection baselines on three datasets to show that DeGPR is model-agnostic, and consistently improves baselines obtaining up to 9% (absolute) mAP gains. | 多类细胞检测和计数是许多病理诊断中的重要任务。手动计数繁琐且常常导致病理学家之间的观察者之间的变化。虽然存在多个通用的基于深度学习的目标检测和计数方法，但由于数据有限，存在微小重叠对象，多种细胞类型，严重的类别不平衡，细胞尺寸/形状的微小差异等，它们可能不容易转移到医学图像中检测和计数细胞。为此，我们提出了引导后验正则化DeGPR，通过引导物体检测器利用细胞之间的区分特征来辅助它。这些特征可以由病理学家提供，也可以直接从视觉数据中推断。我们在两个公开可用的数据集（CoNSeP和MoNuSAC）以及我们贡献的新数据集MuCeD上验证了我们的模型。MuCeD包括55张人类十二指肠活检图像，用于预测乳糜泻。我们对三个数据集上的三个目标检测基线进行了广泛实验，以表明DeGPR是模型不可知的，并且始终改善基线，获得高达9%（绝对）mAP增益。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Tyagi_DeGPR_Deep_Guided_Posterior_Regularization_for_Multi-Class_Cell_Detection_and_CVPR_2023_paper.pdf) |
| 2023 | Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning | Kangning Liu, Weicheng Zhu, Yiqiu Shen, Sheng Liu, Narges Razavian, Krzysztof J. Geras, Carlos Fernandez-Granda | Learning representations for individual instances when only bag-level labels are available is a fundamental challenge in multiple instance learning (MIL). Recent works have shown promising results using contrastive self-supervised learning (CSSL), which learns to push apart representations corresponding to two different randomly-selected instances. Unfortunately, in real-world applications such as medical image classification, there is often class imbalance, so randomly-selected instances mostly belong to the same majority class, which precludes CSSL from learning inter-class differences. To address this issue, we propose a novel framework, Iterative Self-paced Supervised Contrastive Learning for MIL Representations (ItS2CLR), which improves the learned representation by exploiting instance-level pseudo labels derived from the bag-level labels. The framework employs a novel self-paced sampling strategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three medical datasets, showing that it improves the quality of instance-level pseudo labels and representations, and outperforms existing MIL methods in terms of both bag and instance level accuracy. Code is available at https://github.com/Kangningthu/ItS2CLR | 在仅有包级别标签的情况下学习个体实例的表示是多实例学习（MIL）中的一个基本挑战。最近的研究表明，使用对比自监督学习（CSSL）可以取得有希望的结果，该方法学习将对应于两个不同随机选择实例的表示推开。不幸的是，在像医学图像分类这样的现实应用中，通常存在类别不平衡，因此随机选择的实例大部分属于相同的多数类，这使得CSSL无法学习跨类别的差异。为了解决这个问题，我们提出了一个新颖的框架，即迭代自主监督对比学习（ItS2CLR），通过利用从包级别标签导出的实例级伪标签改善学习到的表示。该框架采用了一种新颖的自主学习采样策略，以确保伪标签的准确性。我们在三个医学数据集上评估了ItS2CLR，结果显示它改善了实例级伪标签和表示的质量，并在包级别和实例级别准确性方面优于现有的MIL方法。代码可在https://github.com/Kangningthu/ItS2CLR 上找到。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Multiple_Instance_Learning_via_Iterative_Self-Paced_Supervised_Contrastive_Learning_CVPR_2023_paper.pdf) |
| 2023 | Label-Free Liver Tumor Segmentation | Qixin Hu, Yixiong Chen, Junfei Xiao, Shuwen Sun, Jieneng Chen, Alan L. Yuille, Zongwei Zhou | We demonstrate that AI models can accurately segment liver tumors without the need for manual annotation by using synthetic tumors in CT scans. Our synthetic tumors have two intriguing advantages: (I) realistic in shape and texture, which even medical professionals can confuse with real tumors; (II) effective for training AI models, which can perform liver tumor segmentation similarly to the model trained on real tumors--this result is exciting because no existing work, using synthetic tumors only, has thus far reached a similar or even close performance to real tumors. This result also implies that manual efforts for annotating tumors voxel by voxel (which took years to create) can be significantly reduced in the future. Moreover, our synthetic tumors can automatically generate many examples of small (or even tiny) synthetic tumors and have the potential to improve the success rate of detecting small liver tumors, which is critical for detecting the early stages of cancer. In addition to enriching the training data, our synthesizing strategy also enables us to rigorously assess the AI robustness. | 我们证明了人工智能模型可以在CT扫描中使用合成肝肿瘤准确分割肝肿瘤，而无需手动标注。我们的合成肿瘤具有两个引人注目的优势：（一）形状和质地逼真，甚至连医疗专业人员都可能将其与真实肿瘤混淆；（二）对于训练人工智能模型非常有效，可以像在真实肿瘤上训练的模型一样执行肝肿瘤分割-这一结果令人振奋，因为迄今为止，仅使用合成肿瘤的现有工作尚未达到类似或甚至接近真实肿瘤的性能。这一结果还意味着未来手动逐体素标注肿瘤的工作量（花费多年时间创建）可以显著减少。此外，我们的合成肿瘤可以自动生成许多小（甚至微小）合成肿瘤的例子，并有潜力提高检测小肝肿瘤的成功率，这对于早期癌症的检测至关重要。除了丰富训练数据外，我们的合成策略还使我们能够严格评估人工智能的鲁棒性。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Label-Free_Liver_Tumor_Segmentation_CVPR_2023_paper.pdf) |
| 2023 | CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion | Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Shuang Xu, Zudi Lin, Radu Timofte, Luc Van Gool | Multi-modality (MM) image fusion aims to render fused images that maintain the merits of different modalities, e.g., functional highlight and detailed textures. To tackle the challenge in modeling cross-modality features and decomposing desirable modality-specific and modality-shared features, we propose a novel Correlation-Driven feature Decomposition Fusion (CDDFuse) network. Firstly, CDDFuse uses Restormer blocks to extract cross-modality shallow features. We then introduce a dual-branch Transformer-CNN feature extractor with Lite Transformer (LT) blocks leveraging long-range attention to handle low-frequency global features and Invertible Neural Networks (INN) blocks focusing on extracting high-frequency local information. A correlation-driven loss is further proposed to make the low-frequency features correlated while the high-frequency features uncorrelated based on the embedded information. Then, the LT-based global fusion and INN-based local fusion layers output the fused image. Extensive experiments demonstrate that our CDDFuse achieves promising results in multiple fusion tasks, including infrared-visible image fusion and medical image fusion. We also show that CDDFuse can boost the performance in downstream infrared-visible semantic segmentation and object detection in a unified benchmark. The code is available at https://github.com/Zhaozixiang1228/MMIF-CDDFuse. | 多模态（MM）图像融合旨在生成融合图像，保持不同模态的优点，例如功能突出和详细纹理。为了解决建模跨模态特征和分解理想模态特定和模态共享特征的挑战，我们提出了一种新颖的基于相关性驱动特征分解融合（CDDFuse）网络。首先，CDDFuse使用Restormer块提取跨模态浅层特征。然后，我们引入了一个双分支Transformer-CNN特征提取器，其中包括利用长程注意力处理低频全局特征的Lite Transformer（LT）块和专注于提取高频本地信息的Invertible Neural Networks（INN）块。进一步提出了一种基于相关性驱动的损失，根据嵌入信息使低频特征相关，而高频特征不相关。然后，基于LT的全局融合和基于INN的局部融合层输出融合图像。大量实验证明，我们的CDDFuse在多个融合任务中取得了令人满意的结果，包括红外-可见图像融合和医学图像融合。我们还展示了CDDFuse可以提高统一基准测试中下游红外-可见语义分割和目标检测的性能。该代码可在https://github.com/Zhaozixiang1228/MMIF-CDDFuse找到。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.pdf) |
| 2023 | Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation | Lihe Yang, Lei Qi, Litong Feng, Wayne Zhang, Yinghuan Shi | In this work, we revisit the weak-to-strong consistency framework, popularized by FixMatch from semi-supervised classification, where the prediction of a weakly perturbed image serves as supervision for its strongly perturbed version. Intriguingly, we observe that such a simple pipeline already achieves competitive results against recent advanced works, when transferred to our segmentation scenario. Its success heavily relies on the manual design of strong data augmentations, however, which may be limited and inadequate to explore a broader perturbation space. Motivated by this, we propose an auxiliary feature perturbation stream as a supplement, leading to an expanded perturbation space. On the other, to sufficiently probe original image-level augmentations, we present a dual-stream perturbation technique, enabling two strong views to be simultaneously guided by a common weak view. Consequently, our overall Unified Dual-Stream Perturbations approach (UniMatch) surpasses all existing methods significantly across all evaluation protocols on the Pascal, Cityscapes, and COCO benchmarks. Its superiority is also demonstrated in remote sensing interpretation and medical image analysis. We hope our reproduced FixMatch and our results can inspire more future works. Code and logs are available at https://github.com/LiheYoung/UniMatch. | 在这项工作中，我们重新审视了弱一致性到强一致性框架，这个框架由半监督分类中的FixMatch广泛推广，其中对弱扰动图像的预测作为对其强扰动版本的监督。有趣的是，我们观察到，即使在转移到我们的分割场景时，这样一个简单的流程已经取得了与最近的高级作品竞争的成绩。然而，它的成功在很大程度上依赖于强数据增强的手动设计，这可能是有限的且不足以探索更广泛的扰动空间。出于这个动机，我们提出了一个辅助特征扰动流作为补充，扩展了扰动空间。另一方面，为了充分探测原始图像级增强，我们提出了一种双流扰动技术，使两个强视图同时由一个共同的弱视图指导。因此，我们的整体统一双流扰动方法(UniMatch)在Pascal、Cityscapes和COCO基准测试上显著超越了所有现有方法的所有评估协议。它的优越性也在遥感解释和医学图像分析中得到了证明。我们希望我们重新制作的FixMatch和我们的结果能够激发更多未来的研究工作。代码和日志可在https://github.com/LiheYoung/UniMatch找到。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Revisiting_Weak-to-Strong_Consistency_in_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf) |
| 2023 | A Loopback Network for Explainable Microvascular Invasion Classification | Shengxuming Zhang, Tianqi Shi, Yang Jiang, Xiuming Zhang, Jie Lei, Zunlei Feng, Mingli Song | Microvascular invasion (MVI) is a critical factor for prognosis evaluation and cancer treatment. The current diagnosis of MVI relies on pathologists to manually find out cancerous cells from hundreds of blood vessels, which is time-consuming, tedious, and subjective. Recently, deep learning has achieved promising results in medical image analysis tasks. However, the unexplainability of black box models and the requirement of massive annotated samples limit the clinical application of deep learning based diagnostic methods. In this paper, aiming to develop an accurate, objective, and explainable diagnosis tool for MVI, we propose a Loopback Network (LoopNet) for classifying MVI efficiently. With the image-level category annotations of the collected Pathologic Vessel Image Dataset (PVID), LoopNet is devised to be composed binary classification branch and cell locating branch. The latter is devised to locate the area of cancerous cells, regular non-cancerous cells, and background. For healthy samples, the pseudo masks of cells supervise the cell locating branch to distinguish the area of regular non-cancerous cells and background. For each MVI sample, the cell locating branch predicts the mask of cancerous cells. Then the masked cancerous and non-cancerous areas of the same sample are inputted back to the binary classification branch separately. The loopback between two branches enables the category label to supervise the cell locating branch to learn the locating ability for cancerous areas. Experiment results show that the proposed LoopNet achieves 97.5% accuracy on MVI classification. Surprisingly, the proposed loopback mechanism not only enables LoopNet to predict the cancerous area but also facilitates the classification backbone to achieve better classification performance. | 微血管侵袭(MVI)是评估预后和癌症治疗的关键因素。目前的MVI诊断依赖于病理学家手动从数百个血管中找出癌细胞，这是耗时、繁琐且主观的。最近，深度学习在医学图像分析任务中取得了令人期待的结果。然而，黑匣子模型的不可解释性和大量标注样本的要求限制了基于深度学习的诊断方法的临床应用。为了开发一种准确、客观和可解释的MVI诊断工具，本文提出了一种环回网络(LoopNet)用于高效分类MVI。通过收集的病理血管图像数据集(PVID)的图像级别类别注释，LoopNet被设计为由二进制分类分支和细胞定位分支组成。后者被设计为定位癌细胞、常规非癌细胞和背景区域。对于健康样本，细胞的伪掩模监督细胞定位分支以区分常规非癌细胞和背景区域。对于每个MVI样本，细胞定位分支预测癌细胞的掩模。然后，同一样本的带掩模的癌细胞和非癌细胞区域分别输入回二进制分类分支。两个分支之间的环回使得类别标签能够监督细胞定位分支学习对癌细胞区域的定位能力。实验结果表明，所提出的LoopNet在MVI分类上达到了97.5%的准确率。令人惊讶的是，所提出的环回机制不仅使LoopNet能够预测癌细胞区域，还促进了分类主干实现更好的分类性能。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_A_Loopback_Network_for_Explainable_Microvascular_Invasion_Classification_CVPR_2023_paper.pdf) |
| 2023 | Rethinking Few-Shot Medical Segmentation: A Vector Quantization View | Shiqi Huang, Tingfa Xu, Ning Shen, Feng Mu, Jianan Li | The existing few-shot medical segmentation networks share the same practice that the more prototypes, the better performance. This phenomenon can be theoretically interpreted in Vector Quantization (VQ) view: the more prototypes, the more clusters are separated from pixel-wise feature points distributed over the full space. However, as we further think about few-shot segmentation with this perspective, it is found that the clusterization of feature points and the adaptation to unseen tasks have not received enough attention. Motivated by the observation, we propose a learning VQ mechanism consisting of grid-format VQ (GFVQ), self-organized VQ (SOVQ) and residual oriented VQ (ROVQ). To be specific, GFVQ generates the prototype matrix by averaging square grids over the spatial extent, which uniformly quantizes the local details; SOVQ adaptively assigns the feature points to different local classes and creates a new representation space where the learnable local prototypes are updated with a global view; ROVQ introduces residual information to fine-tune the aforementioned learned local prototypes without re-training, which benefits the generalization performance for the irrelevance to the training task. We empirically show that our VQ framework yields the state-of-the-art performance over abdomen, cardiac and prostate MRI datasets and expect this work will provoke a rethink of the current few-shot medical segmentation model design. Our code will soon be publicly available. | 现有的少样本医学分割网络都遵循相同的做法，即原型越多，性能越好。这一现象可以在矢量量化（VQ）视图中从理论上解释：原型越多，从分布在整个空间上的像素级特征点中分离的聚类就越多。然而，当我们进一步从这个角度思考少样本分割时，发现特征点的聚类和对未知任务的适应性并没有得到足够关注。受到这一观察的启发，我们提出了一个学习VQ机制，包括网格格式VQ（GFVQ）、自组织VQ（SOVQ）和残差导向VQ（ROVQ）。具体而言，GFVQ通过对空间范围内的平均方格进行量化生成原型矩阵，均匀量化局部细节；SOVQ自适应地将特征点分配给不同的局部类别，并创建一个新的表示空间，其中可学习的局部原型通过全局视图进行更新；ROVQ引入残差信息来微调前述学习的局部原型，无需重新训练，这有利于泛化性能，因为它与训练任务无关。我们在腹部、心脏和前列腺MRI数据集上实验证明，我们的VQ框架实现了最先进的性能，希望这项工作能引发对当前少样本医学分割模型设计的重新思考。我们的代码将很快公开。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Rethinking_Few-Shot_Medical_Segmentation_A_Vector_Quantization_View_CVPR_2023_paper.pdf) |
| 2023 | Orthogonal Annotation Benefits Barely-Supervised Medical Image Segmentation | Heng Cai, Shumeng Li, Lei Qi, Qian Yu, Yinghuan Shi, Yang Gao | Recent trends in semi-supervised learning have significantly boosted the performance of 3D semi-supervised medical image segmentation. Compared with 2D images, 3D medical volumes involve information from different directions, e.g., transverse, sagittal, and coronal planes, so as to naturally provide complementary views. These complementary views and the intrinsic similarity among adjacent 3D slices inspire us to develop a novel annotation way and its corresponding semi-supervised model for effective segmentation. Specifically, we firstly propose the orthogonal annotation by only labeling two orthogonal slices in a labeled volume, which significantly relieves the burden of annotation. Then, we perform registration to obtain the initial pseudo labels for sparsely labeled volumes. Subsequently, by introducing unlabeled volumes, we propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that exploits dense pseudo labels in early stage and sparse labels in later stage and meanwhile forces consistent output of two networks. Experimental results on three benchmark datasets validated our effectiveness in performance and efficiency in annotation. For example, with only 10 annotated slices, our method reaches a Dice up to 86.93% on KiTS19 dataset. | 近年来，半监督学习在3D半监督医学图像分割领域显著提升了性能。与2D图像相比，3D医学容积涉及来自不同方向的信息，例如横断面、矢状面和冠状面，因此自然地提供了互补视图。这些互补视图和相邻3D切片之间的内在相似性激发了我们开发一种新的标注方式及其相应的半监督模型以实现有效的分割。具体而言，我们首先提出了正交标注，只标记标记体积中的两个正交切片，显著减轻了标注的负担。然后，我们进行配准以获得稀疏标记体积的初始伪标签。随后，通过引入未标记的体积，我们提出了一种名为Dense-Sparse Co-training (DeSCO)的双网络范式，该范式在早期阶段利用密集伪标签，在后期阶段利用稀疏标签，并同时强制两个网络的输出保持一致。在三个基准数据集上的实验结果验证了我们在性能和标注效率方面的有效性。例如，仅仅标注了10个切片，我们的方法在KiTS19数据集上达到了高达86.93%的Dice系数。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Orthogonal_Annotation_Benefits_Barely-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf) |
| 2023 | SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation | Hyungseob Shin, Hyeongyu Kim, Sewon Kim, Yohan Jun, Taejoon Eo, Dosik Hwang | Recent advances in deep learning-based medical image segmentation studies achieve nearly human-level performance in fully supervised manner. However, acquiring pixel-level expert annotations is extremely expensive and laborious in medical imaging fields. Unsupervised domain adaptation (UDA) can alleviate this problem, which makes it possible to use annotated data in one imaging modality to train a network that can successfully perform segmentation on target imaging modality with no labels. In this work, we propose SDC-UDA, a simple yet effective volumetric UDA framework for Slice-Direction Continuous cross-modality medical image segmentation which combines intra- and inter-slice self-attentive image translation, uncertainty-constrained pseudo-label refinement, and volumetric self-training. Our method is distinguished from previous methods on UDA for medical image segmentation in that it can obtain continuous segmentation in the slice direction, thereby ensuring higher accuracy and potential in clinical practice. We validate SDC-UDA with multiple publicly available cross-modality medical image segmentation datasets and achieve state-of-the-art segmentation performance, not to mention the superior slice-direction continuity of prediction compared to previous studies. | 最近深度学习在医学图像分割研究中取得了近乎人类水平的表现，但在医学影像领域，获取像素级别的专家标注是非常昂贵和费时的。无监督领域适应（UDA）可以缓解这一问题，使得可以利用一个成像模态中的已标注数据来训练一个网络，该网络可以在目标成像模态上成功执行分割而无需标签。在这项工作中，我们提出了SDC-UDA，这是一个简单但有效的体积级UDA框架，用于切片方向连续跨模态医学图像分割，它结合了切片内和切片间的自注意图像转换、不确定性约束伪标签细化和体积级自我训练。我们的方法与先前用于医学图像分割的UDA方法有所不同，它可以在切片方向上获得连续分割，从而确保更高的准确性和在临床实践中的潜力。我们使用多个公开可用的跨模态医学图像分割数据集验证了SDC-UDA，并取得了最先进的分割性能，更不用说与先前研究相比，预测的切片方向连续性更优越。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_SDC-UDA_Volumetric_Unsupervised_Domain_Adaptation_Framework_for_Slice-Direction_Continuous_Cross-Modality_CVPR_2023_paper.pdf) |
| 2023 | Dynamic Graph Enhanced Contrastive Learning for Chest X-Ray Report Generation | Mingjie Li, Bingqian Lin, Zicong Chen, Haokun Lin, Xiaodan Liang, Xiaojun Chang | Automatic radiology reporting has great clinical potential to relieve radiologists from heavy workloads and improve diagnosis interpretation. Recently, researchers have enhanced data-driven neural networks with medical knowledge graphs to eliminate the severe visual and textual bias in this task. The structures of such graphs are exploited by using the clinical dependencies formed by the disease topic tags via general knowledge and usually do not update during the training process. Consequently, the fixed graphs can not guarantee the most appropriate scope of knowledge and limit the effectiveness. To address the limitation, we propose a knowledge graph with Dynamic structure and nodes to facilitate chest X-ray report generation with Contrastive Learning, named DCL. In detail, the fundamental structure of our graph is pre-constructed from general knowledge. Then we explore specific knowledge extracted from the retrieved reports to add additional nodes or redefine their relations in a bottom-up manner. Each image feature is integrated with its very own updated graph before being fed into the decoder module for report generation. Finally, this paper introduces Image-Report Contrastive and Image-Report Matching losses to better represent visual features and textual information. Evaluated on IU-Xray and MIMIC-CXR datasets, our DCL outperforms previous state-of-the-art models on these two benchmarks. | 自动放射学报告具有巨大的临床潜力，可以减轻放射科医生的工作量，并改善诊断解释。最近，研究人员利用医学知识图增强了基于数据驱动的神经网络，以消除该任务中严重的视觉和文本偏见。这些图的结构通过使用由疾病主题标签形成的临床依赖关系来利用，通常在训练过程中不进行更新。因此，固定的图不能保证最适当的知识范围并限制效果。为了解决这一局限性，我们提出了一种具有动态结构和节点的知识图，以促进胸部X射线报告的生成，命名为DCL。具体而言，我们的图的基本结构是从通用知识预先构建的。然后，我们探索从检索报告中提取的特定知识，以自下而上的方式添加额外节点或重新定义它们的关系。每个图像特征在被馈送到解码器模块进行报告生成之前，会与其自身更新的图进行集成。最后，本文引入了图像-报告对比和图像-报告匹配损失，以更好地表示视觉特征和文本信息。在IU-Xray和MIMIC-CXR数据集上评估，我们的DCL模型在这两个基准测试中优于先前的最先进模型。 | [link](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Dynamic_Graph_Enhanced_Contrastive_Learning_for_Chest_X-Ray_Report_Generation_CVPR_2023_paper.pdf) |
